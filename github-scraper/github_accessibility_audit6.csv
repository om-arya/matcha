"repo","path","content","MISSING_TITLE","MISSING_XLABEL","MISSING_YLABEL","MISSING_LEGEND","INSUFFICIENT_COLOR_CONTRAST","FONTSIZE_TOO_SMALL","FIGSIZE_TOO_SMALL","ANIMATIONS"
"oldclesleycode/deep-learning-python","m.py","
import matplotlib.pyplot as plt
import numpy as np
import sklearn
import sklearn.datasets
import sklearn.linear_model
import matplotlib


matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)

np.random.seed(0)
X, y = sklearn.datasets.make_moons(200, noise=0.20)

num_examples = len(X) # training set size
nn_input_dim = 2 # input layer dimensionality
nn_output_dim = 2 # o

epsilon = 0.01 # learning rate for gradient descent
reg_lambda = 0.01 # regularization strength

def plot_decision_boundary(pred_func):
    # Set min and max values and give it some padding
    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # Predict the function value for the whole gid
    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # Plot the contour and training examples
    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)

def calculate_loss(model):
    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']
    # Forward propagation to calculate our predictions
    z1 = X.dot(W1) + b1
    a1 = np.tanh(z1)
    z2 = a1.dot(W2) + b2
    exp_scores = np.exp(z2)
    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
    # Calculating the loss
    corect_logprobs = -np.log(probs[range(num_examples), y])
    data_loss = np.sum(corect_logprobs)
    # Add regulatization term to loss 
    data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))
    return (1./num_examples * data_loss)



def predict(model, x):
    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']
    # Forward propagation
    z1 = x.dot(W1) + b1
    a1 = np.tanh(z1)
    z2 = a1.dot(W2) + b2
    exp_scores = np.exp(z2)
    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
    return (np.argmax(probs, axis=1))



def build_model(nn_hdim, num_passes=20000, print_loss=False):

    # Initialize the parameters to random values
    np.random.seed(0)
    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)
    b1 = np.zeros((1, nn_hdim))
    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)
    b2 = np.zeros((1, nn_output_dim))

    # This is what we return at the end
    model = {}

    # Gradient descent for each batch
    for i in range(0, num_passes):

        # Forward propagation
        z1 = X.dot(W1) + b1
        a1 = np.tanh(z1)
        z2 = a1.dot(W2) + b2
        exp_scores = np.exp(z2)
        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

        # Backpropagation
        delta3 = probs
        delta3[range(num_examples), y] -= 1
        dW2 = (a1.T).dot(delta3)
        db2 = np.sum(delta3, axis=0, keepdims=True)
        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))
        dW1 = np.dot(X.T, delta2)
        db1 = np.sum(delta2, axis=0)

        # Add regularization terms 
        dW2 += reg_lambda * W2
        dW1 += reg_lambda * W1

        # Gradient descent parameter update
        W1 += -epsilon * dW1
        b1 += -epsilon * db1
        W2 += -epsilon * dW2
        b2 += -epsilon * db2

        # Assign new parameters to the model
        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}

        if print_loss and i % 1000 == 0:
          print (""Loss after iteration %i: %f"" %(i, calculate_loss(model)))

    return (model)
model = build_model(3, print_loss=True)


plot_decision_boundary(lambda x: predict(model, x))
plt.title(""Decision Boundary for hidden layer size 3"")
plt.show()

plt.figure(figsize=(16, 32))
hidden_layer_dimensions = [1, 2, 3, 4, 5, 20, 50]
for i, nn_hdim in enumerate(hidden_layer_dimensions):
    plt.subplot(5, 2, i+1)
    plt.title('Hidden Layer size %d' % nn_hdim)
    model = build_model(nn_hdim)
    plot_decision_boundary(lambda x: predict(model, x))
plt.show()","False","True","True","True","False","True","False","False"
"nishio/reinforcement_learning","t.py","""""""
Quarto

0: vacant
1-16: occupied
""""""
import numpy as np
from collections import Counter

def board_to_int(board):
    s = 0L
    for i in range(16):
        s += long(board[i]) * (17 ** i)
    return s

def board_to_possible_hands(board):
    return [i for i in range(16) if board[i] == 0]

def init_board():
    return np.zeros(16, dtype=np.int)

def init_Q():
    from scipy.sparse import dok_matrix
    return dok_matrix((17 ** 16, 16 * 16))

LINES = [
    [0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15],
    [0, 4, 8, 12], [1, 5, 9, 13], [2, 6, 10, 14], [3, 7, 11, 15],
    [0, 5, 10, 15], [3, 6, 9, 12]
]
def is_win(board):
    for line in LINES:
        xs = board[line]
        if any(x == 0 for x in xs): continue
        a, b, c, d = xs - 1
        if a & b & c & d != 0:
            return 1
        if a | b | c | d != 15:
            return 1
    return 0

def print_board(board):
    """"""
    >>> print_board(range(16))
    . o x o | . o o x | . o o o | . o o o
    x o x o | x o o x | o x x x | o o o o
    x o x o | x o o x | x o o o | o x x x
    x o x o | x o o x | o x x x | x x x x
    """"""
    m = np.zeros((16, 4), dtype=np.int)
    for i in range(16):
        if board[i] == 0:
            m[i, :] = 0
        else:
            v = board[i] - 1
            for bit in range(4):  #  nth bit
                m[i, bit] = ((v >> bit) & 1) + 1

    for y in range(4):
        print ' | '.join(
            ' '.join(
                ['.ox'[v] for v in m[y * 4 : (y + 1) * 4, bit]]
            )
        for bit in range(4))
    print


def policy_random(env):
    from random import choice
    position = choice(board_to_possible_hands(env.board))
    piece = choice(env.available_pieces)
    return (position, piece)


class Environment(object):
    def __init__(self, policy=policy_random):
        self.op_policy = policy
        self.result_log =[]
        self.init_env()

    def init_env(self):
        self.board = init_board()
        self.available_pieces= range(2, 17)
        self.selected_piece = 1

    def _update(self, action, k=1, to_print=False):
        position, piece = action

        if self.board[position] != 0:
            # illegal move
            print 'illegal pos'
            self.init_env()
            self.result_log.append(-1 * k)
            return (self.board, -1 * k)

        if piece not in self.available_pieces:
            # illegal move
            print 'illegal piece'
            self.init_env()
            self.result_log.append(-1 * k)
            return (self.board, -1 * k)

        self.board[position] = self.selected_piece
        self.available_pieces.remove(piece)
        self.selected_piece = piece

        if to_print:
            print k, action
            print_board(self.board)

        b = is_win(self.board)
        if b:
            self.init_env()
            self.result_log.append(+1 * k)
            return (self.board, +1 * k)

        if not self.available_pieces:
            # put selected piece
            self.board[self.board==0] = self.selected_piece
            b = is_win(self.board)
            if to_print:
                print 'last move'
                print_board(self.board)

            self.init_env()
            if b:
                # opponent win
                self.result_log.append(-1 * k)
                return (self.board, -1 * k)
            else:
                # tie
                self.result_log.append(0)
                return (self.board, -1)

        return None

    def __call__(self, action, to_print=False):
        ret = self._update(action, k=1, to_print=to_print)
        if ret: return ret
        op_action = self.op_policy(self)
        ret = self._update(op_action, k=-1, to_print=to_print)
        if ret: return ret
        return (self.board, 0)


def play(policy1, policy2=policy_random, to_print=False):
    env = Environment()
    result = 0

    for i in range(9):
        a = policy1(env)
        s, r = env(a, to_print=to_print)
        if r != 0: break
    if to_print:
        print env.result_log[-1]
    return env.result_log[-1]

#play(policy_random, to_print=True)


class Greedy(object):
    def __init__(self):
        self.Qtable = init_Q()

    def __call__(self, env):
        from random import choice
        s = board_to_int(env.board)
        actions = (action_to_int((pos, piece))
            for pos in board_to_possible_hands(env.board)
            for piece in env.available_pieces
        )
        qa = [(self.Qtable[s, a], a) for a in actions]
        bestQ, bestA = max(qa)
        bextQ, bestA = choice([(q, a) for (q, a) in qa if q == bestQ])
        return int_to_action(bestA)


class EpsilonGreedy(object):
    def __init__(self, eps=0.1):
        self.Qtable = init_Q()
        self.eps = eps

    def __call__(self, env):
        from random import choice, random
        s = board_to_int(env.board)
        if random() < self.eps:
            pos = choice(board_to_possible_hands(env.board))
            piece = choice(env.available_pieces)
            return (pos, piece)

        actions = (action_to_int((pos, piece))
            for pos in board_to_possible_hands(env.board)
            for piece in env.available_pieces
        )
        qa = [(self.Qtable[s, a], a) for a in actions]
        bestQ, bestA = max(qa)
        bextQ, bestA = choice([(q, a) for (q, a) in qa if q == bestQ])
        return int_to_action(bestA)


def board_to_state(board):
    return board_to_int(board)

def action_to_int(action):
    pos, piece = action
    return pos * 16 + (piece - 1)

def int_to_action(i):
    assert 0 <= i < 16 * 16
    return (i / 16, i % 16 + 1)


from kagura.utils import Digest
digest = Digest(1)
battle_per_seconds = []

def sarsa(alpha, policyClass=Greedy):
    global environment, policy
    gamma = 0.9
    num_result = batch_width * num_batch
    environment = Environment()
    policy = policyClass()

    action = policy(environment)
    state = board_to_state(environment.board)
    while True:
        next_board, reward = environment(action)
        next_state = board_to_state(next_board)

        # determine a'
        next_action = policy(environment)
        nextQ = policy.Qtable[next_state, action_to_int(next_action)]

        # update Q(s, a)
        s_a = (state, action_to_int(action))
        Qsa = policy.Qtable[s_a]
        estimated_reward = reward + gamma * nextQ
        diff = estimated_reward - Qsa
        policy.Qtable[s_a] += alpha * diff

        state = next_state
        action = next_action
        if len(environment.result_log) == num_result:
            break
        t = digest.digest(len(environment.result_log))
        if t:
            battle_per_seconds.append(t)

    vs = []
    for i in range(num_batch):
        c = Counter(environment.result_log[batch_width * i : batch_width * (i + 1)])
        print c
        vs.append(float(c[1]) / batch_width)
    return vs


def qlearn(alpha, policyClass=Greedy):
    global environment, policy
    gamma = 0.9
    num_result = batch_width * num_batch
    environment = Environment()
    policy = policyClass()

    state = board_to_state(environment.board)
    while True:
        action = policy(environment)
        next_board, reward = environment(action)
        next_state = board_to_state(next_board)

        # update Q(s, a)
        maxQ = max(policy.Qtable[next_state, a] for a in board_to_possible_hands(next_board))
        s_a = (state, action_to_int(action))

        Qsa = policy.Qtable[s_a]
        estimated_reward = reward + gamma * maxQ
        diff = estimated_reward - Qsa
        policy.Qtable[s_a] += alpha * diff

        state = next_state

        if len(environment.result_log) == num_result:
            break
        t = digest.digest(len(environment.result_log))
        if t:
            battle_per_seconds.append(t)

    vs = []
    for i in range(num_batch):
        c = Counter(environment.result_log[batch_width * i : batch_width * (i + 1)])
        print c
        vs.append(float(c[1]) / batch_width)
    return vs



def plot_log():
    from kagura import load
    result_log = load(""sarsa_0.05_result_log"")
    batch_width = 1000
    num_batch = 1000
    vs = []
    for i in range(num_batch):
        c = Counter(result_log[batch_width * i : batch_width * (i + 1)])
        print c
        vs.append(float(c[1]) / batch_width)

    label = 'Sarsa(0.05)'
    imgname = 'sarsa_0.05.png'
    plot()

def plot():
    import matplotlib.pyplot as plt
    plt.clf()
    plt.plot([0.475] * len(vs), label = ""baseline"")
    plt.plot(vs, label=label)
    plt.xlabel(""iteration"")
    plt.ylabel(""Prob. of win"")
    plt.legend(loc = 4)
    plt.savefig(imgname)


def f(n, m):
    if m == 1: return n + 1
    return n * f(n - 1, m - 1) + f(n, m - 1)


if not'ex1':
    from collections import Counter
    print Counter(
        play(policy_random) for i in range(10000))
elif not'ex2':
    batch_width = 1000
    num_batch = 100
    vs = sarsa(0.5)
elif not'ex3':
    batch_width = 1000
    num_batch = 1000
    vs = sarsa(0.5)

if 0:
    batch_width = 1000
    num_batch = 1000
    vs = qlearn(0.5)
    label = 'Qlearn(0.5)'
    imgname = 'qlearn.png'
elif 0:
    batch_width = 1000
    num_batch = 1000
    vs = qlearn(0.05)
    label = 'Qlearn(0.05)'
    imgname = 'qlearn_0.05.png'


from kagura import dump
if 0:
    batch_width = 1000
    num_batch = 1000
    vs = sarsa(0.5, policyClass=EpsilonGreedy)
    label = 'Sarsa(0.5, eps=0.1)'
    imgname = 'sarsa_0.5_eps0.1.png'
    dump(environment.result_log, imgname.replace('.png', '_result_log'))
elif 0:
    batch_width = 1000
    num_batch = 1000
    vs = sarsa(0.05, policyClass=EpsilonGreedy)
    label = 'Sarsa(0.05, eps=0.1)'
    imgname = 'sarsa_0.05_eps0.1.png'
    dump(environment.result_log, imgname.replace('.png', '_result_log'))

if 0:
    batch_width = 100
    num_batch = 1000
    vs = sarsa(0.05, policyClass=Greedy)
    label = 'Sarsa(0.05)'
    imgname = 'sarsa_0.05_2.png'
    dump(environment.result_log, imgname.replace('.png', '_result_log'))


batch_width = 1000
num_batch = 100
vs = sarsa(0.5)
label = 'Sarsa(0.5)'
imgname = 'sarsa_0.5_2.png'

plot()

","True","False","False","False","False","True","True","False"
"glangsto/analyze","a.py","#Python Script to plot raw NSF record data.
#import matplotlib.pyplot as plt
#plot the raw data from the observation
#HISTORY
#16AUG29 GIL make more efficient
#16AUG16 GIL use new radiospectrum class

#15AUG30 add option to plot range fo values
#15JUL01 GIL Initial version
#
import matplotlib.pyplot as plt
import sys
import datetime
import statistics
import radioastronomy

avetimesec = 1800.
dy = -1.

nargs = len( sys.argv)

linestyles = ['-','-','--','-.','-','--','-.','-','--','-.','-','--','-.','-','--','-.','-','--','-','-','--','-.','-','--','-.','-','--','-.','-','--','-.','-','--','-.','-','--','-.']
colors = ['-b','-r','-g','-b','-r','-g','-b','-r','-g','-b','-r','-g','-b','-r','-g','-b','-r','-g','-b','-r','-g','-b','-r','-g','-b','-r','-g','-b','-r','-g','-b','-r','-g','-b','-r','-g']
nmax = len(colors)
scalefactor = 1e8
xallmax = -9.e9
xallmin =  9.e9
yallmax = -9.e9
yallmin =  9.e9

#for symbol, value in locals().items():
#    print symbol, value

nplot = 0
nhot = 0
ncold = 0

# first read through all data and find hot load
for iii in range(1, nargs):

    filename = sys.argv[iii]

    rs = radioastronomy.Spectrum()
    rs.read_spec_ast( filename)
    rs.azel2radec()    # compute ra,dec from az,el

    if rs.telel < 0:
        if nhot == 0:
            hot = rs
            nhot = 1
        else:
            hot.ydataA = hot.ydataA + rs.ydataA
            hot.count = hot.count + rs.count
            nhot = nhot + 1

if nhot > 0:
    hot.ydataA = scalefactor * hot.ydataA / float(nhot)
    print ""Found %3d hot load obs"" % nhot
else:
    print ""No hot load data, can not calibrate""
    exit()

xv = hot.xdata * 1.e-6 # convert to MHz
yv = hot.ydataA
yallmin = min(yv)
yallmax = max(yv)

fig,ax1 = plt.subplots(figsize=(10,6))
plt.hold(True)
fig.canvas.set_window_title(filename)
az = hot.telaz
el = hot.telel
label = 'Hot Load Average'
ymin = min(yv)
ymax = max(yv)
ymed = statistics.median(yv)
count = hot.count
ncold = 0
print(' Max: %9.1f  Median: %9.1f SNR: %6.2f ; %s %s' % (ymax, ymed, ymax/ymed, count, label))
plt.plot(xv, yv, colors[0], linestyle=linestyles[0],label=label)

avetime = datetime.timedelta(seconds=avetimesec)

nread = 0        
# now read through all data and average cold sky obs
for iii in range(1, nargs):

    filename = str(sys.argv[iii])

    rs = radioastronomy.Spectrum()
#    print filename
    rs.read_spec_ast( filename)
    rs.azel2radec()    # compute ra,dec from az,el

    if rs.telel > 0:
        if ncold == 0:
            cold = rs
            ncold = 1
            # sums are weighted by durations
            cold.ydataA = cold.ydataA * cold.durationSec
            # keep track of observing time for weighted sum
            timesum = cold.durationSec
        else:
            # time difference is between mid-points of integrations
            dt = rs.utc - cold.utc 
            # add the time since midpoint of latests
            dt = dt + datetime.timedelta(seconds=rs.durationSec/2.)
            # plus time before start of the first
            dt = dt + datetime.timedelta(seconds=cold.durationSec/2.)
            # if time to average
            if dt > avetime:
                cold.ydataA = cold.ydataA/float(timesum)

                gallon = cold.gallon
                gallat = cold.gallat
                az = cold.telaz
                el = cold.telel
                parts = filename.split('/')
                nparts = len(parts)
                aname = parts[nparts-1]
                parts = aname.split('.')
                aname = parts[0]
                parts = filename.split('/')
                nparts = len(parts)
                aname = parts[nparts-1]
                parts = aname.split('.')
                aname = parts[0]
                parts = aname.split('T')
                date  = parts[0]
                time  = parts[1]
                time  = time.replace('_',':')

                label = '%s, AZ,EL: %5s,%5s, Lon,Lat=%5.1f,%5.1f' % (time, az,el,gallon,gallat)
                xv = cold.xdata * 1.e-6
                yv = cold.ydataA * scalefactor

                xmin = min(xv)
                xmax = max(xv)
                xallmin = min(xmin,xallmin)
                xallmax = max(xmax,xallmax)
                ymin = min(yv)
                ymax = max(yv)
                ymed = statistics.median(yv)
                count = cold.count
                ncold = 0
                print(' Max: %9.1f  Median: %9.1f SNR: %6.2f ; %s %s' % (ymax, ymed, ymax/ymed, count, label))
                nplot = nplot + 1
                note = cold.noteA
                    #print('%s' % note)
                yallmin = min(ymin,yallmin)
                yallmax = max(ymax,yallmax)
                ncolor = min(nmax-1, nplot) 
                plt.plot(xv, yv, colors[ncolor], linestyle=linestyles[ncolor],label=label)
            else: # else ont enough time yet, average cold data
                cold.count = cold.count + rs.count 
                cold.ydataA = cold.ydataA + (rs.ydataA * cold.durationSec)
            # keep track of observing time for weighted sum
                timesum = timesum + cold.durationSec
            # end if not a enough time
        # end if a cold file
    #end for all files to sum

plt.xlim(xallmin,xallmax)
plt.ylim(0,1.5*yallmax)
plt.title(date)
fig.canvas.set_window_title(date)
for tick in ax1.xaxis.get_major_ticks():
    tick.label.set_fontsize(14) 
for tick in ax1.yaxis.get_major_ticks():
    tick.label.set_fontsize(14) 
plt.xlabel('Frequency (MHz)', fontsize=14)
plt.ylabel('Intensity (Counts)', fontsize=14)
plt.legend(loc='upper right')
plt.show()
","False","False","False","False","True","False","False","False"
"guxiaowei1/DIYresnet-NST","1.py","from __future__ import print_function

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from PIL import Image
import matplotlib.pyplot as plt

import torchvision.transforms as transforms
import torchvision.models as models

import copy

#device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
device = torch.device( ""cpu"")
# desired size of the output image
imsize = 512 if torch.cuda.is_available() else 128  # use small size if no gpu

loader = transforms.Compose([
    transforms.Resize(imsize),  # scale imported image
    transforms.ToTensor()])  # transform it into a torch tensor


def image_loader(image_name):
    image = Image.open(image_name)
    # fake batch dimension required to fit network's input dimensions
    image = loader(image).unsqueeze(0)
    return image.to(device, torch.float)


style_img = image_loader(""./data/picasso.jpg"")
content_img = image_loader(""./data/dancing.jpg"")

assert style_img.size() == content_img.size(), \
    ""we need to import style and content images of the same size""

unloader = transforms.ToPILImage()  # reconvert into PIL image

plt.ion()

def imshow(tensor, title=None):
    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it
    image = image.squeeze(0)      # remove the fake batch dimension
    image = unloader(image)
    plt.imshow(image)
    if title is not None:
        plt.title(title)
    plt.pause(0.01) # pause a bit so that plots are updated


# plt.figure()
# imshow(style_img, title='Style Image')
#
# plt.figure()
# imshow(content_img, title='Content Image')

def gram_matrix(input):
    a, b, c, d = input.size()  # a=batch size(=1)
    # b=number of feature maps
    # (c,d)=dimensions of a f. map (N=c*d)

    features = input.view(a * b, c * d)  # resise F_XL into \hat F_XL

    G = torch.mm(features, features.t())  # compute the gram product

    # we 'normalize' the values of the gram matrix
    # by dividing by the number of element in each feature maps.
    return G.div(a * b * c * d)

class StyleLoss(nn.Module):

    def __init__(self, target_feature):
        super(StyleLoss, self).__init__()
        self.target = gram_matrix(target_feature).detach()

    def forward(self, input):
        G = gram_matrix(input)
        self.loss = F.mse_loss(G, self.target)
        return input

class ContentLoss(nn.Module):

    def __init__(self, target,):
        super(ContentLoss, self).__init__()
        # we 'detach' the target content from the tree used
        # to dynamically compute the gradient: this is a stated value,
        # not a variable. Otherwise the forward method of the criterion
        # will throw an error.
        self.target = target.detach()

    def forward(self, input):
        self.loss = F.mse_loss(input, self.target)
        return input

cnn = models.vgg19(pretrained=False).features.to(device).eval()
#cnn = models.vgg19(pretrained=False)
print(cnn)
cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)
cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)

# create a module to normalize input image so we can easily put it in a
# nn.Sequential
class Normalization(nn.Module):
    def __init__(self, mean, std):
        super(Normalization, self).__init__()
        # .view the mean and std to make them [C x 1 x 1] so that they can
        # directly work with image Tensor of shape [B x C x H x W].
        # B is batch size. C is number of channels. H is height and W is width.
        self.mean = torch.tensor(mean).view(-1, 1, 1)
        self.std = torch.tensor(std).view(-1, 1, 1)

    def forward(self, img):
        # normalize img
        return (img - self.mean) / self.std

# desired depth layers to compute style/content losses :
content_layers_default = ['conv_4']
style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']

def get_style_model_and_losses(cnn, normalization_mean, normalization_std,
                               style_img, content_img,
                               content_layers=content_layers_default,
                               style_layers=style_layers_default):
    cnn = copy.deepcopy(cnn)

    # normalization module
    normalization = Normalization(normalization_mean, normalization_std).to(device)

    # just in order to have an iterable access to or list of content/syle
    # losses
    content_losses = []
    style_losses = []

    # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential
    # to put in modules that are supposed to be activated sequentially
    model = nn.Sequential(normalization)

    i = 0  # increment every time we see a conv
    for layer in cnn.children():
        if isinstance(layer, nn.Conv2d):
            i += 1
            name = 'conv_{}'.format(i)
        elif isinstance(layer, nn.ReLU):
            name = 'relu_{}'.format(i)
            # The in-place version doesn't play very nicely with the ContentLoss
            # and StyleLoss we insert below. So we replace with out-of-place
            # ones here.
            layer = nn.ReLU(inplace=False)
        elif isinstance(layer, nn.MaxPool2d):
            name = 'pool_{}'.format(i)
        elif isinstance(layer, nn.BatchNorm2d):
            name = 'bn_{}'.format(i)
        else:
            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))

        model.add_module(name, layer)

        if name in content_layers:
            # add content loss:
            target = model(content_img).detach()
            content_loss = ContentLoss(target)
            model.add_module(""content_loss_{}"".format(i), content_loss)
            content_losses.append(content_loss)

        if name in style_layers:
            # add style loss:
            target_feature = model(style_img).detach()
            style_loss = StyleLoss(target_feature)
            model.add_module(""style_loss_{}"".format(i), style_loss)
            style_losses.append(style_loss)

    # now we trim off the layers after the last content and style losses
    for i in range(len(model) - 1, -1, -1):
        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):
            print(model[i])
            break

    model = model[:(i + 1)]

    return model, style_losses, content_losses

input_img = content_img.clone()
# if you want to use white noise instead uncomment the below line:
# input_img = torch.randn(content_img.data.size(), device=device)

# add the original input image to the figure:
plt.figure()
imshow(input_img, title='Input Image')

def get_input_optimizer(input_img):
    # this line to show that input is a parameter that requires a gradient
    optimizer = optim.LBFGS([input_img.requires_grad_()])
    return optimizer

def run_style_transfer(cnn, normalization_mean, normalization_std,
                       content_img, style_img, input_img, num_steps=300,
                       style_weight=1000000, content_weight=1):
    """"""Run the style transfer.""""""
    print('Building the style transfer model..')
    model, style_losses, content_losses = get_style_model_and_losses(cnn,
        normalization_mean, normalization_std, style_img, content_img)
    optimizer = get_input_optimizer(input_img)

    print('Optimizing..')
    run = [0]
    while run[0] <= num_steps:

        def closure():
            # correct the values of updated input image
            input_img.data.clamp_(0, 1)

            optimizer.zero_grad()
            model(input_img)
            style_score = 0
            content_score = 0

            for sl in style_losses:
                style_score += sl.loss
            for cl in content_losses:
                content_score += cl.loss

            style_score *= style_weight
            content_score *= content_weight

            loss = style_score + content_score
            loss.backward()

            run[0] += 1
            if run[0] % 50 == 0:
                print(""run {}:"".format(run))
                print('Style Loss : {:4f} Content Loss: {:4f}'.format(
                    style_score.item(), content_score.item()))
                print()

            return style_score + content_score

        optimizer.step(closure)

    # a last correction...
    input_img.data.clamp_(0, 1)

    return input_img

model, style_losses, content_losses = get_style_model_and_losses(cnn,
         cnn_normalization_mean, cnn_normalization_std, style_img, content_img)
# output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,
#                             content_img, style_img, input_img)
#
# plt.figure()
# imshow(output, title='Output Image')
#
# # sphinx_gallery_thumbnail_number = 4
# plt.ioff()
# plt.show()
print(model)","False","True","True","True","False","True","True","False"
"Naizil-Francis/ML-LAB","8.py","import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

data = pd.read_csv('opinion.csv')
print(""The first 5 values of data are:\n"", data.head())

X = data.iloc[:, :-1]
y = data.iloc[:, -1]

label_encoders = {}
for column in X.columns:
    le = LabelEncoder()
    X[column] = le.fit_transform(X[column])
    label_encoders[column] = le

print(""\nNow the train data is:\n"", X.head())

le_target = LabelEncoder()
y = le_target.fit_transform(y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

classifier = GaussianNB()
param_grid = {'var_smoothing': [1e-09, 1e-08, 1e-07, 1e-06, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01, 1.0]}
grid_search = GridSearchCV(classifier, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print(""\nBest parameters found:\n"", grid_search.best_params_)
print(""\nBest cross-validation accuracy:\n"", grid_search.best_score_)

best_classifier = grid_search.best_estimator_
best_classifier.fit(X_train, y_train)
y_pred = best_classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f""Accuracy: {accuracy * 100:.2f}%"")
","True","True","True","True","False","True","True","False"
"MogilipalemHemanthKumar/Framework-for-Sentiment-Analysis-and-Text-Data-Analytics","2.py","import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import nltk
import base64
from wordcloud import WordCloud, STOPWORDS
from nltk.util import ngrams
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
from textblob import TextBlob
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
nltk.download('omw-1.4')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
from nltk.sentiment import SentimentIntensityAnalyzer

nltk.download('vader_lexicon')  # Download the pre-trained sentiment analyzer

sid = SentimentIntensityAnalyzer()
def get_sentiment_scores(df):
    sid = SentimentIntensityAnalyzer()

    # Apply polarity_scores method to 'Reviews' column
    df['sentiment'] = df['Reviews'].apply(lambda x: sid.polarity_scores(x)['compound'])

    # Define threshold for positive and negative sentiment
    threshold = 0.05

    # Count number of positive and negative reviews
    num_pos = sum(df['sentiment'] > threshold)
    num_neg = sum(df['sentiment'] < -threshold)

    # Print overall sentiment of dataframe
    if num_pos > num_neg:
        st.success('The Product review has a positive sentiment.')
    elif num_pos < num_neg:
        st.success('The Product review  has a negative sentiment.')
    else:
        st.success('The Product review  has a neutral sentiment.')
def preprocess_cleaning(text, pos_tags=None, lower=False):
    import re, string
    if isinstance(text, str):
        stop_words = set(stopwords.words('english')) - {'no', 'not', 'nor'} | set()
        text = text.lower() if lower else text
        text_without_punc = text.translate(str.maketrans('', '', string.punctuation))
        text_tokens = nltk.RegexpTokenizer(r'\w+|\$[\d\.]+|\S+').tokenize(text_without_punc)
        tokens_without_sw = [word for word in text_tokens if word not in stop_words]
        tokens_lemmatized = [nltk.stem.WordNetLemmatizer().lemmatize(word) for word in tokens_without_sw]
        if pos_tags is not None:
            tokens_pos_tagged = nltk.pos_tag(tokens_lemmatized)
            tokens_lemmatized = [word for word, pos_tag in tokens_pos_tagged if
                                 pos_tag in pos_tags and word not in ['top']]  # ['JJ','JJR','JJS']
        return "" "".join(tokens_lemmatized)
    else:
        return "" ""



# List of product review URLs
st.markdown(
    f""""""
    <div style='text-align: center;'>
        <h1>Real Time Amazon Product Review Sentiment Analysis</h1>
    </div>
    """""",
    unsafe_allow_html=True
)
container = st.container()
url=""https://logos-world.net/wp-content/uploads/2020/04/Amazon-Logo.png""
container.markdown(f'<div style=""text-align: center;"">'
                       f'<img src=""{url}""width=""150"">'
                       f'</div>',
                       unsafe_allow_html=True)
Categories=['Electronics','Clothing and Fashion','Home and Kitchen','Books']
selected_category= st.selectbox(""Select a product review URL"", Categories)
if (selected_category=='Electronics'):
  product = [""Apple iPhone 12 Mini (128GB) - Blue"",""EchoDot (3rdGen) - Smartsp"", ""Sonya 7 III Full - FrameM irrorl"", ""Bose Quiet comfort 35Ii Noise"", ""boAt Wave Call Smart Watch"", ""ZEBRONICS Zeb - Dash Plus2"", ""Spigen Tough Armor BackCove""]
  Product_id=[""B08L5VN68Y"",""B07PFFMP9P"",""B07B43WPVK"",""B0756CYWWD"",""B0B5B6PQCT"",""B08YDFX7Y1"",""B096HG9474""]

  #urls = [ ""https://www.amazon.in/New-Apple-iPhone-Mini-128GB/product-reviews/B08L5VN68Y/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",    ""https://www.amazon.in/product-reviews/B07PFFMP9P/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",    ""https://www.amazon.in/product-reviews/B07B43WPVK/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B0756CYWWD/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B0BPPWX8F2 /ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B0B5B6PQCT/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B08YDFX7Y1/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B096HG9474/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B0BRQV14YD /ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews""]
  selected_product = st.selectbox(""Select a product review URL"", product)
  x = product.index(selected_product)
  y = Product_id[x]
  selected_url = f""https://www.amazon.in/product-reviews/{y}/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews""
  link = f""[For more details ..... \n You can refer this link ](https://www.amazon.in/dp/{y}/?_encoding=UTF8&pd_rd_w=21WMM&content-id=amzn1.sym.82b4a24f-081c-4d15-959c-ef13a1d3fa4e&pf_rd_p=82b4a24f-081c-4d15-959c-ef13a1d3fa4e&pf_rd_r=2FEEVAEMP0E15J8PD1ZE&pd_rd_wg=LduuX&pd_rd_r=1e90e15d-cb65-4e84-b14a-2e6216025c7e&ref_=pd_gw_ci_mcx_mr_hp_atf_m&th=1)""
  st.markdown(link, unsafe_allow_html=True)
# Create a dropdown menu to select the URL

if (selected_category=='Clothing and Fashion'):
  product=[""Amazon Brand - Symbol Men"",""Scott International Polo T-Shir"",""Allen Solly Men Jet Black Re"",""Zeel Clothing Women's Organ"",""Vardha Women's Kanchipuram Raw Silk Saree with Unstitched Blouse Piece - Zari Woven Work Sarees for Wedding"",""KARAGIRI Women's Woven Silk Blend Saree With Blouse Piece (KARAGIRI_Purple)""]
  Product_id =[""B07BDY51R6"",""B01F0XDZ80"",""B06Y2FG6R7"",""B09MBY3SNR"",""B08WCF58GP"",""B0B5HL5WZR""]
  #urls = [ ""https://www.amazon.in/product-reviews/B07BDY51R6/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",    ""https://www.amazon.in/product-reviews/B01F0XDZ80/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",    ""https://www.amazon.in/product-reviews/B06Y2FG6R7/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B0756CYWWD/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B09MBY3SNR/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B0B5B6PQCT/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B0BQDS2M4X/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B096HG9474/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B0BWRWQ3XK/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews""]
  selected_product = st.selectbox(""Select a product Name"", product)
  x = product.index(selected_product)
  y = Product_id[x]
  selected_url = f""https://www.amazon.in/product-reviews/{y}/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews""
# Create a dropdown menu to select the URL
if (selected_category=='Home and Kitchen'):
  product=[""Havells Ambrose 1200mm Ceiling Fan"",""Havells OFR - 11Fin 2900-Watt PTC Fan Heater (Black)"",""Havells Capture 500 Watt Mixer Grinder with 3 Bigger size Stainless Steel Jar (Grey & Green) with 5 year Motor warranty"",""Havells Air Fryer Prolife Digi with 4L Capacity | Digital Touch Panel | Auto On/Off | 60 Min Timer | Basket Release Button | Air Filtration System | 2 Yr Warranty, Black"",""Priti - Golden Coffee Table Golden with Black Laminated Marble Table top Round Metal Table and Outdoor or Indoor"",""Accent for Living Room Bedroom Balcony and Office - Set of Two"",""Pigeon-Amaze-Plus-1-5-Ltr"",""beatXP-Multipurpose-Portable-Electronic-Weighing"",""Lifelong-LLMG23-500-Watt-Liquidizing-Stainless"",""Philips-GC1905-1440-Watt-Steam-Spray"",""Kuber-Industries-Laundry-Organiser-LMESH02"",""NutriPro-Bullet-Juicer-Grinder-Blades"",""Durable-Quality-Bicycle-Ultra-Loud-Trending"",""Lista-Lista056-Rechargeable-Light-Bright"",""Quality-Bicycle-Silicone-Cycling-Cushion"",""Boldfit-Typhoon-Shaker-Leakproof-Preworkout"",""ElectroSky-Building-Exercise-Workout-Training"",""Bajaj-Torque-New-Honeycomb-Technology"",""Lifelong-LLMG23-500-Watt-Liquidizing-Stainless"",""Pigeon-Kessel-1-2-Litre-Multi-purpose-Kettle""]
  #urls = [ ""https://www.amazon.in/New-Apple-iPhone-Mini-128GB/product-reviews/B08L5VN68Y/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",    ""https://www.amazon.in/product-reviews/B07PFFMP9P/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",    ""https://www.amazon.in/product-reviews/B07B43WPVK/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B0756CYWWD/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B0BPPWX8F2 /ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B0B5B6PQCT/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B08YDFX7Y1/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B096HG9474/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B0BRQV14YD /ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews""]
  Product_id=[""B01LYU3BZF"",""B00PQCXVQM"",""B0872HNF94"",""B01LWUEDJI"",""B0B4DPGGG9"",""B07WMS7TWB"",""B0B61DSF17"",""B09X5C9VLK"",""B008QTK47Q"",""B077K1HFZ3"",""B09J2SCVQT"",""B09RPRB634"",""B075GYJSPT"",""B075GYJSPT"",""B00RV3CLVU"",""B08JTMZRJW"",""B09YDLBS5D"",""B09R3QNGW5"",""B09X5C9VLK"",""B01I1LDZGA""]
# Create a dropdown menu to select the URL
  selected_product = st.selectbox(""Select a product Name"", product)
  x=product.index(selected_product)
  y=Product_id[x]
  selected_url=f""https://www.amazon.in/product-reviews/{y}/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews""
if (selected_category=='Books'):
  product=[""I Wish My Kids Had Cancer: A Family Surviving the Autism Epidemic "",""Dopamine-Detox-Remove-Distractions-Productivity-ebook"",""Sanskrit-Hindi-Kosh-Vaman-Shivram-Apte"",""Psychology-Money-Morgan-Housel"",""Atomic-Habits-James-Clear"",""Ikigai-H%C3%A9ctor-Garc%C3%ADa"",""Habits-Highly-Effective-People-Powerful-ebook"",""Rich-Dad-Poor-Middle-Anniversary"",""Lifes-Amazing-Secrets-Balance-Purpose"",""Dont-Believe-Everything-You-Think"",""Mans-Search-Meaning-Viktor-Frankl"",""As-Man-Thinketh-James-Allen"",""How-Win-Friends-Influence-People"",""BlackBook-English-Vocabulary-March-Nikhil"",""NTA-Paper-Teaching-Research-Aptitude"",""Services-Prelims-Topic-wise-Solved-Papers"",""Indian-English-Revised-Services-Administrative""]
  Product_id=[""1606720708"",""B098MHBF23"",""8120820975"",""9390166268"",""1847941834"",""178633089X"",""B01069X4H0"",""1612681131"",""143442295"",""935543135X"",""1846041244"",""9386538172"",""8194790891"",""8195645712"",""935606590X"",""B09XMHRQ63"",""9354600352""]
  #urls = [ ""https://www.amazon.in/New-Apple-iPhone-Mini-128GB/product-reviews/B08L5VN68Y/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",    ""https://www.amazon.in/product-reviews/B07PFFMP9P/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",    ""https://www.amazon.in/product-reviews/B07B43WPVK/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B0756CYWWD/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B0BPPWX8F2 /ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B0B5B6PQCT/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B08YDFX7Y1/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B096HG9474/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"",""https://www.amazon.in/product-reviews/B0BRQV14YD /ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews""]
  selected_product = st.selectbox(""Select a product Name"", product)
  x = product.index(selected_product)
  y = Product_id[x]
  selected_url = f""https://www.amazon.in/product-reviews/{y}/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews""

code = requests.get(selected_url)
if code.status_code == 200:
    soup = BeautifulSoup(code.content, 'html.parser')
    names = soup.select('span.a-profile-name')[2:]
    image_tags = soup.select('img')
    image_urls = [img['src'] for img in image_tags]
    container = st.container()
    container.markdown(f'<div style=""text-align: center;"">'
                       f'<img src=""{image_urls[3]}""width=""250"">'
                       f'</div>',
                       unsafe_allow_html=True)
    st.write(""\n\n"")
    if(len(names)>0):
        titles = soup.select('a.review-title span')
        stars = soup.select('i.review-rating span.a-icon-alt')[2:]
        reviews = soup.select('span.review-text-content span')

        cust_name = []
        ratings = []
        rev_title = []
        rev_content = []

        st.write(""\n\n\n"")
        minx = min(len(names), len(stars), len(titles), len(reviews))
        for i in range(minx):
            cust_name.append(names[i].get_text())
            ratings.append(stars[i].get_text())
            rev_title.append(titles[i].get_text())
            rev_content.append(reviews[i].get_text().strip(""\n ""))
        df = pd.DataFrame()
        df['Customer Name'] = cust_name
        df['Ratings'] = ratings
        df['Review Title'] = rev_title
        df['Reviews'] = rev_content
        df = df.dropna()
        st.write(df)

        df.to_csv(f'Amazon_Product_Reviews/{y}.csv')
        if st.button(""Download CSV File""):
            csv = df.to_csv(index=False)
            print(csv)
            if csv is not None:
                b64 = base64.b64encode(csv.encode()).decode()
                href = f'<a href=""data:file/csv;base64,{b64}"" download=""{y}.csv"">Download CSV File</a>'
                st.markdown(href, unsafe_allow_html=True)

        df = pd.read_csv(f""Amazon_Product_Reviews\{y}.csv"")
        df['text_clean'] = df['Reviews'].apply(preprocess_cleaning)
        comment_words = ''
        stopwords = set(STOPWORDS)
        for val in df['text_clean']:
            val = str(val)

            tokens = val.split()
            for i in range(len(tokens)):
                tokens[i] = tokens[i].lower()

            comment_words += "" "".join(tokens) + "" ""

        wordcloud = WordCloud(width=1000, height=800,
                              background_color='Black',
                              stopwords=stopwords,
                              min_font_size=10).generate(comment_words)

        # Display the image
        st.image(wordcloud.to_array())

        get_sentiment_scores(df)

        # Classify the document as positive, negative, or neutral based on the average sentiment score
    else:
        st.write(""\n\n"")
        st.success(""The Product Review has Negative Sentiment"")


else:
    st.write(""Error: The selected URL returned a status code of"", code.status_code)
","True","True","True","True","True","True","True","False"
"kynexi/NumericalAnalysis","9.py","import numpy as np
import matplotlib.pyplot as plt

def EscVel(z, c, N):
    for n in range(N):
        if (abs(z) > 2): #check if it escaped 
            return n #return the iter number when it escaped
        z = z**2 + c #mandelbrot formula
    return N #if it never escape, return max iter

def JuliaSet(zMax, c, N):
    size = 500 #grid resolution
    x = np.linspace(-zMax, zMax, size) # real part grid
    y = np.linspace(-zMax, zMax, size) # imaginary part grid
    mset = np.zeros((size, size)) #escape time matrix
    
    X, Y = np.meshgrid(x, y) #full 2d grid
    Z0 = X + 1j * Y  # convert into complex numbers

    for i in range(size):
        for j in range(size):
            z = Z0[i, j]  # Get the correct complex number
            mset[i, j] = EscVel(z, c, N)


    return mset

N = 100
c_values = [
    complex(-.297491, 0.641051),
    complex(-.79, .15),
    complex(-.162, 1.04),
    complex(.3, -.01),
    complex(-1.476, 0),
    complex(-.12, -.77), #intriticate set, requires higher N
    complex(0.28, .008),
    complex(0, 0.8) #bonus plot
]
#dictionary for dynamic zMax values
zMaxD = {
    complex(-.79, .15): 0.9,
    complex(-.162, 1.04): 0.3,
    complex(-1.476, 0) : 0.65,
    complex(-.12, -.77) : 0.8,
    complex(0, 0.8) : 1.6
}
#dynamic N
N_dict = {
    complex(-.12, -.77) : 300
}


# Create a figure with multiple subplots (3 rows, 3 columns)
fig, axes = plt.subplots(2, 4, figsize=(15, 8))  # Adjust rows/columns as needed
axes = axes.ravel()  # Flatten axes for easy iteration

# Generate and plot each Julia set
for i, c in enumerate(c_values):
    zMax = zMaxD.get(c, 1.2)  
    N = N_dict.get(c, 100)

    M = JuliaSet(zMax, c, N)  # Compute the Julia Set
    axes[i].imshow(np.arctan(0.1 * M), extent=[-zMax, zMax, -zMax, zMax], cmap='inferno', origin='lower')
    axes[i].set_title(f""c = {c}"")
    axes[i].set_xlabel(""Real Axis"")
    axes[i].set_ylabel(""Imaginary Axis"")

# Adjust layout to fit all subplots nicely
plt.tight_layout()
plt.show()","True","True","True","True","False","True","False","False"
"watsjustice/matheq","6.py","import sympy as sp
import numpy as np
import matplotlib.pyplot as mp
import os

class Math():

	derivative_x , derivative_y , newequation_x , newequation_y , flag = 0 , 0 ,str(), 0 ,False 
	x ,y= sp.symbols('x,y')


	def __init__(self , *kwargs):
		globals().update({item : object for item , object in zip(['value' + str(x) for x in range(100)] , kwargs)})
		self.theequation_x = value0
		try:
			self.theequation_y = value1
			Math.flag = True
		except:
			pass
		self.diff()

	def diff(self):
		Math.derivative_x = self.theequation_x.diff(Math.x)
		try:
			Math.derivative_x = Math.derivative_x / self.theequation_x.diff(Math.y)
		except:
			pass
		if Math.flag:
			Math.derivative_y = self.theequation_y.diff(Math.x)



	@property
	def xf(self):
		fig = mp.subplots()
		y = np.linspace(-10 , 10,1000)
		x = np.linspace(-10 , 10,1000)
		mp.plot(y , Math.newequation_x(x,y))

		if Math.flag:
			mp.plot(y , Math.newequation_y(y))
			
		mp.show()


	@xf.setter
	def xf(self , *kwargs):
		globals().update({item : object for item , object in zip(['val' + str(x) for x in range(10)] , kwargs)})
		Math.newequation_x = val0

		if Math.flag:
			Math.newequation_y = val1

	def __str__(self):
		if Math.flag:
			Math.flag = False

			return f'The derivative of the {self.theequation_x} and {self.theequation_y} is :' + '\n'*3 + f'{Math.derivative_x/Math.derivative_y}' + '\n'*3 
		return f'The derivative of the {self.theequation_x} is :' + '\n'*3 + f'{Math.derivative_x}' + '\n'*3

x,y = sp.symbols('x,y')
ss = Math(y*sp.sin(x)-sp.cos(x-y))
print(ss)


# ИНСТРУКЦИЯ ПО ПОСТРОЙКЕ ГРАФИКА.

# ЕСЛИ В ПРОИВЗОДНОЙ ОТСУТСТВУЮТ SIN , COS , TAN , CTG , ASIN (ARCSINUS) , ACOS (ARCCOS) , ТО
# ПРОСТО КОПИРУЕМ ПОЯВИВШИЕСЯ В КОНСОЛИ УРАВНЕНИЕ - ПРОИЗВОДНУЮ ВВЕДЕННОГО РАНЕЕ УРАВНЕНИЯ
# ВО ВСЕХ СЛУЧАЯХ БЕЗ ИСКЛЮЧЕНИЯ ТРЕБУЕТСЯ ЗАМЕНА SQRT НА ( ВЫРАЖЕНИЕ , КОТОРОЕ БЫЛО В МЕТОДЕ SQRT) ** 0.5

# В СЛУЧАЕ ЕСЛИ ЕСТЬ SIN , COS , TAN , CTG , ASIN (ARCSINUS) , ACOS (ARCCOS) , ТО
# ПЕРЕД ТРИГОНОМЕТРИЧЕСКИМ МЕТОДОВ СТАВИМ NP.
# ПРИМЕР : SIN(X) ДОЛЖНО БЫТЬ ПЕРЕПИСАНО В NP.SIN(X)
#



ss.xf = lambda x,y: (y*np.cos(x) + np.sin(x - y))/(np.sin(x) - np.sin(x - y))   #ПОЛЕ ДЛЯ ВВОДА УРАВНЕНИЯ ДЛЯ ПОСТРОЙКИ ГРАФИКА
ss.xf


","True","True","True","True","False","True","True","False"
"Shiva69-prasad/ml","p3.py","import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

iris = load_iris()
data = iris.data
labels = iris.target
label_names = iris.target_names

iris_df = pd.DataFrame(data, columns=iris.feature_names)

pca = PCA(n_components=2)
data_reduced = pca.fit_transform(data)

reduced_df = pd.DataFrame(data_reduced, columns=['Principal Component 1', 'Principal Component 2'])
reduced_df['Label'] = labels

plt.figure(figsize=(8, 6))
colors = ['r', 'g', 'b']
for i, label in enumerate(np.unique(labels)):
    plt.scatter(
        reduced_df[reduced_df['Label'] == label]['Principal Component 1'],
        reduced_df[reduced_df['Label'] == label]['Principal Component 2'],
        label=label_names[label],
        color=colors[i]
    )

plt.title('PCA on Iris Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.grid()
plt.show()
","False","False","False","False","True","True","False","False"
"Vedansh5545/GraphNeuralNetwork","3.py","# Visualize the Graph
""""""
# This script demonstrates how to create a graph using PyTorch Geometric and visualize it using NetworkX and Matplotlib.
# It includes:
# 1. Creating a simple graph with 3 nodes and 2 edges.
# 2. Loading a real-world dataset (Cora).
# 3. Visualizing the graph using NetworkX and Matplotlib.
# The graph is represented using the `Data` class from PyTorch Geometric, which contains node features and edge indices.
# The Cora dataset is a citation network of scientific papers, and it is loaded using the `Planetoid` class from the 
# `torch_geometric.datasets` module.
# The graph is visualized using NetworkX and Matplotlib, with nodes colored in sky blue and labeled.
""""""
import networkx as nx
import matplotlib.pyplot as plt
from torch_geometric.utils import to_networkx
from torch_geometric.datasets import Planetoid

dataset = Planetoid(root='data/Planetoid', name='Cora')
data = dataset[0]

G = to_networkx(data, to_undirected=True)
nx.draw(G, with_labels=True, node_color='skyblue', node_size=500)
plt.show()
","True","True","True","True","True","True","True","False"
"dabhuvi/QUERY-PROCESSING","4.py","import yfinance as yf
import matplotlib.pyplot as plt
import pandas as pd
ticker = ""GOOGL""
start_date = ""2023-01-01""
end_date = ""2023-12-31""
data = yf.download(ticker, start=start_date, end=end_date)
if not data.empty:
    plt.figure(figsize=(12, 6))
    plt.plot(data.index, data[""Close""], label=""Close Price"", color=""blue"")
    plt.title(f""Alphabet Inc. Stock Prices ({start_date} to {end_date})"", fontsize=16)
    plt.xlabel(""Date"", fontsize=12)
    plt.ylabel(""Close Price (USD)"", fontsize=12)
    plt.legend()
    plt.grid()
    plt.show()
else:
    print(""No data found for the specified date range."")
","False","False","False","False","True","False","False","False"
"vish22ise/prog2","prog2.py","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing


california_data = fetch_california_housing(as_frame=True)
data = california_data.frame

correlation_matrix = data.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix of California Housing Features')
plt.show()


sns.pairplot(data, diag_kind='kde', plot_kws={'alpha': 0.5})
plt.suptitle('Pair Plot of California Housing Features', y=1.02)
plt.show()
","False","True","True","True","False","True","False","False"
"muxxaffar/AI_2023","CNN_Code.py","#importing the required libraries
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense

#loading data
(X_train, y_train), (X_test, y_test) = mnist.load_data()

#reshaping data and normalizing the pixel values
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)) / 255.0
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)) / 255.0

#defining model
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(MaxPool2D(2, 2))
model.add(Flatten())
model.add(Dense(100, activation='relu'))
model.add(Dense(10, activation='softmax'))
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

#fitting the model
history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(""Test Accuracy:"", test_accuracy)

# Plot some sample images from train data
plt.figure(figsize=(10, 10))
for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')
    plt.title(str(y_train[i]))
    plt.axis('off')
plt.suptitle(""Sample Images from Train Data"")
plt.show()

# Plot some sample images from test data
plt.figure(figsize=(10, 10))
for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')
    plt.title(str(y_test[i]))
    plt.axis('off')
plt.suptitle(""Sample Images from Test Data"")
plt.show()

# Plot the training and validation accuracy over epochs
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training and Validation Accuracy')
plt.show()
","False","False","False","False","False","True","False","False"
"SofiaCher22/get","7.py","import RPi.GPIO as GPIO
import time
import matplotlib.pyplot as plt
GPIO.setmode(GPIO.BCM)
dac = [8, 11, 7, 1, 0, 5, 12, 6]
leds = [2, 3, 4, 17, 27, 22, 10, 9]

GPIO.setup(leds, GPIO.OUT)
GPIO.setup(dac, GPIO.OUT)
comp = 14
troyka = 13
GPIO.setup(troyka, GPIO.OUT, initial = GPIO.LOW)
GPIO.setup(comp, GPIO.IN)

def dec_bin(N):
    return [int(X) for X in bin(N)[2:].zfill(8)]   


def update_leds(x):
    b = dec_bin(int(x/3.3 * 255))
    GPIO.output(leds, b)


def adc():
    value = 0
    for i in range(7, -1, -1):
        value += 2**i
        GPIO.output(dac, dec_bin(value))
        time.sleep(0.001)
        if GPIO.input(comp) == 1:
            value -= 2**i
    return value

try:
    measured = []
    start = time.time()
    GPIO.output(troyka, 1)
    print('zariadka')

    while True:
        value = adc()
        
        print(value)
        # v = value * 3.3/256
        measured.append(value)
        # update_leds(v)
        if value >= 206:
            break

    GPIO.output(troyka, 0)
    print('rasriadka')

    while True:
        value = adc()
        #v = value * 3.3/256
        print(value)
        measured.append(value)
        #update_leds(v)
        if value <= 77:
            break
    

    final = time.time()
    Time = final - start

    plt.plot(measured)
    plt.grid()
    plt.show()

    with open('data.txt', 'w') as outfile:
        outfile.write('\n'.join(map(str, measured)))    
    
    
    Quant = 3.3/256
    Vd = 1/(Time/len(measured))
    a = [str(Quant), str(Vd)]
    with open('setting.txt', 'w') as outfile1:
        outfile1.write('\n'.join(a))  
    
    
    print('Общая продолжительность эксперимента: ', Time)
    print('Период: ', Time/len(measured))
    print('Частота дискретизации ', Vd)
    print('Шаг квантования ', Quant)

    
     
except KeyboardInterrupt:
            print('Stopped')


finally:
    GPIO.output(dac, 0)
    GPIO.cleanup()","True","True","True","True","False","True","True","False"
"NunoMBatista/TI-TP1","TP1.py","import pandas as pd 
import matplotlib.pyplot as plt
import numpy as np
import math
import huffmancodec as huffc

def pearson(MPG, target):
    return np.corrcoef(target, MPG)[0][1]

def infoMut(MPG, target, alfa): 

    MPGEntropy = entropy(MPG, alfa)
    targetEntropy = entropy(target, alfa)
    tamanho = len(target)
    
    alfaLenMPG = np.max(MPG) - np.min(MPG)
    alfaLenTarget = np.max(target) - np.min(target)
    probMPGTarget = np.histogram2d(MPG, target, bins=(alfaLenMPG, alfaLenTarget+10))[0] / tamanho
    
    nonZeroIdx = probMPGTarget != 0
    probMPGTarget = probMPGTarget[nonZeroIdx]
    
    MPGTargetEntropy = -np.sum(probMPGTarget * np.log2(probMPGTarget))
        
    return MPGEntropy + targetEntropy - MPGTargetEntropy

def entropyHuff (target, alfa):
    codec = huffc.HuffmanCodec.from_data(target) 
    symbols, lengths = codec.get_code_len()

    ocorr = ocorrencias(target, alfa)   
    tamanho = len(target)

    probabilidades = np.array([(ocorr[symbols[x]]/tamanho)*lengths[x] for x in range(len(symbols))])
    entropy = np.sum(probabilidades)
    
    variancia = var(target, symbols, lengths, entropy, ocorr)   
    print(""Variância de Comprimentos:"", variancia)
    
    return entropy

def var(target, symbols, lengths, entropy, ocorr):
    tamanho = len(target)
    variance = 0
    for idx in range(len(symbols)):
        prob = ocorr[symbols[idx]] / tamanho
        variance += prob * (lengths[idx] - entropy) ** 2
    return variance

def entropy(target, alfa):
    # H(X) = -ΣP(i)*log2(P(i))
    contador = ocorrencias(target, alfa)
    menor = min(target)
    maior = max(target)
    tamanho = len(target)
    
    probabilidades = np.array([(contador[x]/tamanho)*math.log2(contador[x]/tamanho) for x in range(menor, maior+1) if (contador[x] > 0)])
    
    return -np.sum(probabilidades)  
    
def mediaBits (target, alfa):
    contador = ocorrencias(target, alfa)
    nAlfa = len([x for x in contador.values() if x > 0])
    return math.log2(nAlfa)

def compareMPG(dataMatrix, varNames):
    #MPG é o index 6 da lista
    MPG = dataMatrix[:,6]

    for i in range (6):
        plt.figure(1)
        plt.subplot(321+i)
        plt.scatter(dataMatrix[:,i], MPG, color = 'purple')
        plt.xlabel(varNames[i])
        plt.ylabel(""MPG"")
        plt.title(""MPG vs. "" + varNames[i])
    plt.tight_layout()

def ocorrencias (target, alfa):
    contador = alfa.copy()
    for i in target:
        contador[i] += 1
    return contador

def ocorrenciasPlot (target, alfa, name, tickInterval, figura):
    contador = ocorrencias(target, alfa)
    xAxis = [x for x in contador.keys() if contador[x] > 0]
    yAxis = []
    for i in xAxis:
        yAxis.append(contador[i]) 
        
    # O x_values é uma linha de valores para o eixo X sem espaços vazios
    x_values = np.arange(len(xAxis))
    plt.figure(figura)
    plt.bar(x_values, yAxis, color = ""red"")
    plt.xlabel(name)
    plt.ylabel(""Count"")
    
    # xticks é usado para   trocar as labels do x_values pelas do xAxis, tendo assim 
    # uma linha não interrompida de valores em xmas com as labels corretas do xAxis    
    
    tickPos = np.arange(0, len(xAxis), tickInterval)
    tickLabels = [xAxis[i] for i in tickPos]
    
    plt.xticks(tickPos, tickLabels)
    plt.axis(""tight"")
    plt.tight_layout()
    
def binning (target, n, firstAlfa):
    lastAlfa = np.max(target)
    targetAlfa = {key: 0 for key in range(0, lastAlfa + 1)}
    ocorr = ocorrencias(target, targetAlfa)
    binningN = math.ceil(len(list(ocorr.keys())) / n)
    for i in range(binningN):
        binn = list(ocorr.keys())[i * n : ((i+1) * n)]
        # A função lambda retorna as ocorrências de k se k estiver no intervalo de binn, caso k não esteja nesse intervalo, retorna -1
        replacement = max(ocorr, key = lambda k: ocorr[k] if k in binn else -1)
        mask = (target >= np.min(binn)) & (target <= np.max(binn))
        target[mask] = replacement
    return target

def MAE (MPG, target):
    target = np.array(target)
    MPG = np.array(MPG)
    return np.mean(np.abs(MPG-target))

def predictMPG (dataM, remove):
    rowSize = len(dataM[0])
    for i in remove: 
        dataM[i] = [0 for i in range(rowSize)]
    MPGpred = [-5.5241 - 0.146 * dataM[0][i] - 0.4909 * dataM[1][i] + 0.0026 * dataM[2][i] - 0.0045 * dataM[3][i] + 0.6725 * dataM[4][i] - 0.0059 * dataM[5][i] for i in range(rowSize)]
        
    return MPGpred
        

data = pd.read_excel('CarDataset.xlsx')
varNames = data.columns.values.tolist()
dataMatrix = data.to_numpy()
dataMatrix = dataMatrix.astype(""uint16"")

# Definir alfabeto
alfa = {key: 0 for key in range(0, 65535)} #todos os Uint16

acceleration = np.copy(dataMatrix[:,0])
cylinders = np.copy(dataMatrix[:,1])
displacement = np.copy(dataMatrix[:,2])
horsepower = np.copy(dataMatrix[:,3])
model = np.copy(dataMatrix[:,4])
weight = np.copy(dataMatrix[:,5])


compareMPG(dataMatrix, varNames)
ocorrenciasPlot(acceleration, alfa, ""Acceleration"", 1, 2)

weight = binning(weight, 40, np.min(weight))
ocorrenciasPlot(weight, alfa, ""Weight"", 7, 3)
displacement = binning(displacement, 5, np.min(displacement))
ocorrenciasPlot(displacement, alfa, ""displacement"", 5, 4)
horsepower = binning(horsepower, 5, np.min(horsepower))
ocorrenciasPlot(horsepower, alfa, ""Horse Power"", 3, 5)

MPG = np.copy(dataMatrix[:,6])
dataMatrixBinn = [acceleration, cylinders, displacement, horsepower, model, weight, MPG]

IMarray = [infoMut(MPG, dataMatrixBinn[i], alfa) for i in range(7)]

for i in range(7):
    print(varNames[i])
    print(""Nº Médio de bits com símbolos equiprováveis:"", mediaBits(dataMatrixBinn[i], alfa))
    #print(""Entropia normal antes do binning:"", entropy(dataMatrix[:,i], alfa))
    print(""Entropia normal após binning:"", entropy(dataMatrixBinn[i], alfa))
    #print(""Entropia de Huffman antes do binning:"", entropyHuff(dataMatrix[:,i], alfa))
    print(""Entropia de Huffman após binning:"", entropyHuff(dataMatrixBinn[i], alfa)) 
    print(""Relação de "" + varNames[i] + "" com MPG:"", pearson(MPG, dataMatrixBinn[i]))
    print(""Informação mútua com MPG: "", IMarray[i])
    print(""\n\n"")

print(""Entropia da matriz inteira antes do binning: "", entropy(np.reshape(dataMatrix, -1), alfa), ""\n"")
print(""Entropia da matriz inteira depois do binning: "", entropy(np.reshape(dataMatrixBinn, -1), alfa), ""\n"")

MPGpred = predictMPG(dataMatrixBinn.copy(), [])
print(""Erro de MPGpred com todas as variáveis: "", MAE(MPG, MPGpred), ""MPG previsto: "", np.average(MPGpred))

MPGpred = predictMPG(dataMatrixBinn.copy(), [5])
print(""Erro de MPGpred sem variável de maior MI: "", MAE(MPG, MPGpred), ""MPG previsto: "", np.average(MPGpred))

MPGpred = predictMPG(dataMatrixBinn.copy(), [0])
print(""Erro de MPGpred sem variável de menor MI: "", MAE(MPG, MPGpred), ""MPG previsto: "", np.average(MPGpred))

#plt.show()
","False","False","False","True","True","True","True","False"
"KianESoftware/Satellite-image-processing",".py","import sentinelhub
from oauthlib.oauth2 import BackendApplicationClient
from requests_oauthlib import OAuth2Session
from sentinelhub import SHConfig, SentinelHubRequest, DataCollection, BBox, bbox_to_dimensions, constants, MimeType
import matplotlib


config = SHConfig()
config.sh_client_id = ""******************""
config.sh_client_secret = ""********************""
config.sh_token_url = ""https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token""
config.sh_base_url = ""https://sh.dataspace.copernicus.eu""

resolution = 10

bbox = BBox(bbox=[
  28.90825,
  40.066512,
  29.066584,
  40.14319
], crs=sentinelhub.CRS.WGS84)
betsiboka_size = bbox_to_dimensions(bbox, resolution=resolution)
print(f""Image shape at {resolution} m resolution: {betsiboka_size} pixels"")


evalscript_true_color = """"""
    //VERSION=3

    function setup() {
        return {
            input: [{
                bands: [""B04"", ""B03"", ""B02""]
            }],
            output: {
                bands: 3
            }
        };
    }

    function evaluatePixel(sample) {
        return [3*sample.B04, 3*sample.B03, 3*sample.B02];
    }
""""""

request_true_color = SentinelHubRequest(
    data_folder= r""path/to/your/directory"",
    evalscript=evalscript_true_color,
    input_data=[
        SentinelHubRequest.input_data(
            data_collection=DataCollection.SENTINEL2_L2A.define_from(
                ""s2l2a"", service_url=config.sh_base_url
            ),
            time_interval=(""2023-08-12"", ""2023-12-29""),
        )
    ],
    responses=[SentinelHubRequest.output_response(""default"", MimeType.PNG)],
    bbox=bbox,
    size=betsiboka_size,
    config=config,
)

true_color_imgs = request_true_color.get_data(save_data=True)

print(
    f""Returned data is of type = {type(true_color_imgs)} and length {len(true_color_imgs)}.""
)
print(
    f""Single element in the list is of type {type(true_color_imgs[-1])} and has shape {true_color_imgs[-1].shape}""
)

image = true_color_imgs[0]
print(f""Image type: {image.dtype}"")
","True","True","True","True","True","True","True","False"
"ILARIO-KAGOO/MLA0203-FOM","Programs/logistic_regression.py","# Step 1: Import the required modules
from sklearn.datasets import make_classification
from matplotlib import pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import pandas as pd

# Step 2: Generate the dataset
x, y = make_classification(
    n_samples=100,
    n_features=1,
    n_classes=2,
    n_clusters_per_class=1,
    flip_y=0.03,
    n_informative=1,
    n_redundant=0,
    n_repeated=0
)
print(y)

# Step 3: visualize the data
plt.scatter(x, y, c=y, cmap='rainbow')
plt.title('Scatter Plot of Logistic Regression')
plt.show()

# Step 4: Split the dataset
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1)

x_train.shape

# Step 4: Perform Logistic Regression
log_reg = LogisticRegression()
log_reg.fit(x_train, y_train)

# Step 5: Make prediction using the model
y_pred = log_reg.predict(x_test)

# Step 6: Display the Confusion Matrix
confusion_matrix(y_test, y_pred)
","False","True","True","True","False","True","True","False"
"unchhohang/XAUUSDTradeBot","p.py","import pandas as pd;
import matplotlib.pyplot as plt

df = pd.DataFrame(
    {
        ""Name"": [
            ""Braund, Mr. Owen Harris"",
            ""Allen, Mr. William Henry"",
            ""Bonnell, Miss. Elizabeth"",
        ],
        ""Age"": [22, 35, 58],
        ""Sex"": [""male"", ""male"", ""female""],
    }
)

df[""PAge""] = df[""Age""] + 2

print(df.head())

","True","True","True","True","False","True","True","False"
"spidy1080/Big-Data","10/dsbdapr10.py","import seaborn as sns
dataset = sns.load_dataset('iris')
dataset.head()

import matplotlib.pyplot as plt
#fig, axes = plt.subplots(2,2, figsize = (16,9))
sns.histplot(dataset['sepal_length'])
sns.histplot(dataset['sepal_width'])
sns.histplot(dataset['petal_length'])
sns.histplot(dataset['petal_width'])

import matplotlib.pyplot as plt
#fig, axes = plt.subplots(2,2, figsize = (16,9))
sns.boxplot(y='petal_length',x='species', data = dataset)
sns.boxplot(y='petal_width',x='species', data = dataset)
sns.boxplot(y='sepal_length',x='species', data = dataset)
sns.boxplot(y='sepal_width',x='species', data = dataset)
","True","True","True","True","False","True","False","False"
"mengzi667/QML-asignment2","I.py","import gurobipy as gp
from gurobipy import GRB
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.patches import FancyArrowPatch
import math
import time
import json

# ---------------------- Data Parsing ----------------------
# Reading data file with variable time windows
data_file = ""data_large_multiTW_diff.txt""
nodes = []
with open(data_file, 'r') as file:
    for line in file:
        parts = line.strip().split('\t')
        node_id = int(parts[0])
        x_coord = float(parts[1])
        y_coord = float(parts[2])
        demand = float(parts[3])
        service_time = float(parts[4])
        num_tw = int(parts[5])
        time_windows = [(float(parts[6 + 2 * i]), float(parts[7 + 2 * i])) for i in range(num_tw)]
        nodes.append({
            'id': node_id,
            'x': x_coord,
            'y': y_coord,
            'demand': demand,
            'service_time': service_time,
            'time_windows': time_windows
        })

# Duplicate depot as virtual depot (n+1)
virtual_depot = nodes[0].copy()
virtual_depot['id'] = len(nodes)  # Assign virtual depot ID
nodes.append(virtual_depot)

n = len(nodes)   # Number of nodes
N0 = range(1, n-1)  # Customer nodes (excluding depot and virtual depot)
N = range(n)        # All nodes (including depot and virtual depot)
V = range(3)        # Vehicles (3 vehicles)
b_v = 200            # Vehicle capacity
M = 1e6             # Large constant for time constraints
coordinates = [(node['x'], node['y']) for node in nodes]

# Calculate distances between nodes
distance = {(i, j): math.sqrt((nodes[i]['x'] - nodes[j]['x'])**2 + (nodes[i]['y'] - nodes[j]['y'])**2)
            for i in N for j in N if i != j}

# ---------------------- Model Creation ----------------------
model = gp.Model(""Split_Pickup_MultiTW"")

# Decision Variables
x = model.addVars(N, N, V, vtype=GRB.BINARY, name=""x"")  # Vehicle routing
y = model.addVars(N0, V, vtype=GRB.CONTINUOUS, name=""y"")  # Quantity picked up
z = model.addVars(N0, V, len(nodes[1]['time_windows']), vtype=GRB.BINARY, name=""z"")  # Time window selection
w = model.addVars(N, V, vtype=GRB.CONTINUOUS, name=""w"")  # Service start time

# ---------------------- Objective Function ----------------------
model.setObjective(gp.quicksum(distance[i, j] * x[i, j, v] for i in N for j in N for v in V if i != j), GRB.MINIMIZE)

# ---------------------- Constraints ----------------------
# (27) Vehicle capacity constraint
model.addConstrs((gp.quicksum(y[i, v] for i in N0) <= b_v for v in V), name=""capacity"")

# (28) Total demand satisfaction
model.addConstrs((gp.quicksum(y[i, v] for v in V) == nodes[i]['demand'] for i in N0), name=""demand"")

# (29) Linking y with x
model.addConstrs((y[i, v] <= nodes[i]['demand'] * gp.quicksum(x[i, j, v] for j in N if j != i)
                  for v in V for i in N0), name=""link_y_x"")

# (30) Each node visited at least once in a time window
model.addConstrs((gp.quicksum(z[i, v, t] for v in V for t in range(len(nodes[i]['time_windows']))) >= 1
                  for i in N0), name=""visit"")

# (31) Flow conservation
model.addConstrs((gp.quicksum(x[i, j, v] for i in N if i != j) == gp.quicksum(x[j, k, v] for k in N if k != j)
                  for v in V for j in N0), name=""flow"")

# (32) Vehicles start from the depot
model.addConstrs((gp.quicksum(x[0, j, v] for j in N) == 1 for v in V), name=""start"")

# (33) Vehicles return to the virtual depot
model.addConstrs((gp.quicksum(x[i, n-1, v] for i in N) == 1 for v in V), name=""end"")

# (34) Time window constraints
model.addConstrs((nodes[i]['time_windows'][t][0] * z[i, v, t] <= w[i, v] for i in N0 for v in V 
                  for t in range(len(nodes[i]['time_windows'])))
                 , name=""time_window_lb"")

model.addConstrs((w[i, v] <= nodes[i]['time_windows'][t][1] * z[i, v, t] + M * (1 - z[i, v, t])
                  for i in N0 for v in V for t in range(len(nodes[i]['time_windows'])))
                 , name=""time_window_ub"")

# (35) Service time constraints
model.addConstrs((w[i, v] + nodes[i]['service_time'] + distance[i, j] <= w[j, v] + M * (1 - x[i, j, v])
                  for v in V for i in N for j in N if i != j), name=""service_time"")

# (36) Linking z and x
model.addConstrs((gp.quicksum(z[i, v, t] for t in range(len(nodes[i]['time_windows']))) ==
                  gp.quicksum(x[i, j, v] for j in N if j != i)
                  for v in V for i in N0), name=""link_z_x"")

# ---------------------- Solve the Model ----------------------
start_time = time.time()  
model.optimize()
end_time = time.time() 

# ---------------------- Print Computation Time ----------------------
print(""\nComputation Time: {:.2f} seconds"".format(end_time - start_time))

# ---------------------- Results and Plotting ----------------------
if model.status == GRB.OPTIMAL:
    print(""Optimal solution found!"")

    # Extract x (vehicle routes), y (quantity picked), z (time window selection), and w (service start times)
    x_data = []
    y_data = []
    z_data = []
    w_data = []

    for v in V:  # Vehicles
        # Extract x and y
        for i in N:  # Origin nodes
            for j in N:  # Destination nodes
                if i != j and x[i, j, v].X > 0.5:  # Vehicle routes
                    x_data.append({
                        ""Vehicle"": v + 1,
                        ""Origin"": i,
                        ""Destination"": j,
                        ""Value"": x[i, j, v].X
                    })
            if i in N0:  # Quantity picked at customer nodes
                y_data.append({
                    ""Vehicle"": v + 1,
                    ""Node"": i,
                    ""Quantity"": round(y[i, v].X, 3)
                })

        # Extract z (time window selection)
        for i in N0:  # Customer nodes
            for t in range(len(nodes[i]['time_windows'])):
                if z[i, v, t].X > 0.5:
                    z_data.append({
                        ""Vehicle"": v + 1,
                        ""Node"": i,
                        ""Time Window Index"": t,
                        ""Value"": z[i, v, t].X
                    })

        # Extract w (service start times)
        for i in N:  # All nodes
            w_data.append({
                ""Vehicle"": v + 1,
                ""Node"": i,
                ""Start Time"": round(w[i, v].X, 3)
            })

    # Convert to DataFrames
    x_df = pd.DataFrame(x_data)
    y_df = pd.DataFrame(y_data)
    z_df = pd.DataFrame(z_data)
    w_df = pd.DataFrame(w_data)

    # Print data
    print(""\n--- Decision Variable: x (Routes) ---"")
    print(x_df)

    print(""\n--- Decision Variable: y (Quantities Picked) ---"")
    print(y_df)

    print(""\n--- Decision Variable: z (Time Window Selection) ---"")
    print(z_df)

    print(""\n--- Decision Variable: w (Service Start Times) ---"")
    print(w_df)

    # Save data to CSV files
    x_df.to_csv(""routes_variables.csv"", index=False)
    y_df.to_csv(""quantities_variables.csv"", index=False)
    z_df.to_csv(""time_window_variables.csv"", index=False)
    w_df.to_csv(""start_times_variables.csv"", index=False)

    print(""\nDecision variable data saved to:"")
    print("" - 'routes_variables.csv' for x"")
    print("" - 'quantities_variables.csv' for y"")
    print("" - 'time_window_variables.csv' for z"")
    print("" - 'start_times_variables.csv' for w"")

# ---------------------- Plot Vehicle Routes ----------------------
'''
!!! Please note! The following code is for plotting. To run it, create a new `.py` file to 
save this code, then run it after running the above code to generate the charts and 
results table. Since only one code file can be uploaded, I have combined them in this manner.
Thank you for your understanding.
'''
# import pandas as pd
# import matplotlib.pyplot as plt
# from matplotlib.patches import FancyArrowPatch

# # ---------------------- Data Loading ----------------------
# # File paths
# routes_file = ""routes_variables.csv""
# start_times_file = ""start_times_variables.csv""
# time_window_file = ""time_window_variables.csv""
# quantities_file = ""quantities_variables.csv""
# diff_file = ""data_large_multiTW_diff.txt""

# # Load CSV files
# routes_df = pd.read_csv(routes_file)
# start_times_df = pd.read_csv(start_times_file)
# time_window_df = pd.read_csv(time_window_file)
# quantities_df = pd.read_csv(quantities_file)

# # Load node coordinates, demand, and time windows
# nodes = []
# with open(diff_file, 'r') as file:
#     for line in file:
#         parts = line.strip().split( )
#         node_id = int(parts[0])
#         x_coord = float(parts[1])
#         y_coord = float(parts[2])
#         demand = float(parts[3])
#         servicetime = float(parts[4])
#         num_tw = int(parts[5])
#         time_windows = [(float(parts[6 + 2 * i]), float(parts[7 + 2 * i])) for i in range(num_tw)]
#         nodes.append({
#             'id': node_id, 'x': x_coord, 'y': y_coord, 
#             'demand': demand, 'time_windows': time_windows
#         })

# # Coordinates and demand dictionary
# coordinates = {node['id']: (node['x'], node['y']) for node in nodes}
# demands = {node['id']: node['demand'] for node in nodes}
# time_window_dict = {node['id']: node['time_windows'] for node in nodes}

# # Replace node 19 with 0 (Depot)
# routes_df['Origin'] = routes_df['Origin'].replace(9, 0).astype(int)
# routes_df['Destination'] = routes_df['Destination'].replace(9, 0).astype(int)

# # ---------------------- Path Reconstruction Function ----------------------
# def reconstruct_path(vehicle_routes):
#     """"""
#     Reconstruct the vehicle route in the correct order.
#     """"""
#     path = []
#     current_node = 0  # Start from the depot
#     while True:
#         next_nodes = vehicle_routes[vehicle_routes['Origin'] == current_node]
#         if next_nodes.empty:
#             break
#         next_node = next_nodes.iloc[0]['Destination']
#         path.append((current_node, next_node))
#         current_node = next_node
#         if current_node == 0:  # Return to depot
#             break
#     return path

# # ---------------------- Generate Result Summary ----------------------
# def create_vehicle_summary(routes, start_times, quantities, time_windows):
#     vehicle_summaries = {}
#     for v in routes['Vehicle'].unique():
#         vehicle_routes = routes[routes['Vehicle'] == v]
#         vehicle_times = start_times[start_times['Vehicle'] == v]
#         vehicle_quantities = quantities[quantities['Vehicle'] == v]
#         vehicle_time_windows = time_windows[time_windows['Vehicle'] == v]

#         # Reconstruct the route
#         path = reconstruct_path(vehicle_routes)

#         # Initialize load and create result table
#         load = 0  
#         vehicle_table = []

#         for idx, (origin, destination) in enumerate(path):
#             if idx == 0:  # Force Time O = 0 at the depot
#                 time_o = 0.0
#             else:
#                 time_o = vehicle_times.loc[vehicle_times['Node'] == origin, 'Start Time'].values[0] + servicetime
            
#             time_d = vehicle_times.loc[vehicle_times['Node'] == destination, 'Start Time'].values[0]
#             y_iv = vehicle_quantities.loc[vehicle_quantities['Node'] == destination, 'Quantity'].values[0] if destination != 0 else 0
#             tw_index = vehicle_time_windows.loc[vehicle_time_windows['Node'] == destination, 'Time Window Index'].values[0] if destination != 0 else -1

#             load_o = load
#             load_d = load + y_iv
#             load = load_d

#             tw_used = time_window_dict[destination][int(tw_index)] 

#             vehicle_table.append({
#                 ""Origin"": int(origin),
#                 ""Destination"": int(destination),
#                 ""Time O (Arrival)"": round(time_o, 1),
#                 ""Time D (Departure)"": round(time_d, 1),
#                 ""Pick-up Quantity by V at D"": y_iv,  # Output the y_iv column
#                 ""Load O"": load_o,
#                 ""Load D"": load_d,
#                 ""Time Window Used of D"": tw_used
#             })

#         vehicle_summaries[v] = pd.DataFrame(vehicle_table)
#     return vehicle_summaries

# # Generate summaries
# vehicle_summaries = create_vehicle_summary(routes_df, start_times_df, quantities_df, time_window_df)

# # Display and save results
# for v, table in vehicle_summaries.items():
#     print(f""\n--- Vehicle {v} Summary ---"")
#     print(table)
#     table.to_csv(f""vehicle_{v}_summary.csv"", index=False)  # Save with y_iv included
#     print(f""Saved to vehicle_{v}_summary.csv"")

# # ---------------------- Plot Vehicle Routes ----------------------
# plt.figure(figsize=(12, 10))
# colors = ['r', 'b', 'g', 'c', 'm', 'y', 'k']

# # Plot all nodes
# for node_id, (x, y) in coordinates.items():
#     plt.scatter(x, y, color='black', zorder=2)
#     plt.text(x + 0.5, y + 0.5, str(node_id), fontsize=10, color='black', zorder=3)

# # Plot routes
# for v, vehicle_routes in routes_df.groupby('Vehicle'):
#     color = colors[v % len(colors)]
#     path = reconstruct_path(vehicle_routes)
#     for origin, destination in path:
#         x1, y1 = coordinates[origin]
#         x2, y2 = coordinates[destination]
#         arrow = FancyArrowPatch((x1, y1), (x2, y2), color=color, arrowstyle='->', mutation_scale=15, zorder=1)
#         plt.gca().add_patch(arrow)
#     plt.plot([], [], color=color, label=f""Vehicle {v}"")

# plt.legend(loc='best')
# plt.title(""Routes for VRP with Split Pickup and Multiple TW"")
# plt.xlabel(""X Coordinate"")
# plt.ylabel(""Y Coordinate"")
# plt.grid(True)
# plt.show()
","False","False","False","False","True","False","False","False"
"LiZe-21/Ex","a.py","import streamlit as st
import matplotlib.pyplot as plt

st.title(""반도체 공정 시뮬레이터"")

st.sidebar.header(""공정 조건 선택"")
process= st.sidebar.selectbox(""공정 선택"", [""산화"", ""식각"", ""증착""])
temp = st.sidebar.slider(""온도(°C)"", 200, 1000, 600)
time = st.sidebar.slider(""공정 시간(분)"", 1, 120, 30)

st.write(f""선택한 공정:**{process}**, 온도: {tempy}°C, 시간: {time}분"")

# 산화 시뮬레이션
if process == ""산화"":
  thickness = 0.1 * (temp / 100) * (time ** 0.5)
  st.write(f""예상 산화막 두께: **{round(thickness,2)} nm**"")

  fig, ax = plt.subplots()
  ax.plot([O, time], [O, thickness])
  ax.set_xlabel (""Time (min)"")
  ax.set_ylabel (""Oxide Thickness (nm)"")
  ax.set_title(""Change in Oxide Thickness"") 
  st.pyplot(fig)

# 식각 시뮬레이션
elif process == ""식각"":
    etch_rate = 0.05 * (temp / 100)  # 단위: nm/min
    etched_thickness = etch_rate * time
    st.write(f""예상 제거된 두께: **{round(etched_thickness, 2)} nm**"")

    fig, ax = plt.subplots()
    ax.plot([0, time], [0, etched_thickness])
    ax.set_xlabel(""Time (min)"")
    ax.set_ylabel(""Etched Thickness (nm)"")
    ax.set_title(""식각 두께 변화"")
    st.pyplot(fig)

# 증착 시뮬레이션
elif process ==""증착"":
  deposition_rate = 0.08 * (temp / 100) # 단위: nm/min
  deposited_thickness = deposition_rate * time
  st.write(f""예상 증착막 두께:**{round(deposited_thickness,2)} nm**"")
  
  fig, ax = plt.subplots()
  ax.plot([O, time], [O, deposited_thickness])
  ax.set_xlabel(""Time (min)"")
  ax.set_ylabel (""Deposited Thickness (nm)"") 
  ax.set_title(""Change in Deposited Thickness"") 
  st.pyplot(fig)

# 이론 설명
st.markdown(""---"")
with st.expander(""공정 이론 보기""):
  if process ==""산화"":
    st.markdown(""""""
    -**산화 공정**은 실리콘 표면에 산화막(Si02)을 형성하는 과정입니다.
    - 고온에서 02 또는 H20 를 반응시켜 이루어지며, ** Dry** 또는 **Wet 산화** 방식이 있습니다.
    """""")
elif process ==""식각"":
  st.markdown (""""""
    -**식각 공정**은 불필요한 박막을 제거하는 과정입니다.
    -**습식 식각(Wet Etching)**과 **건식 식각(Dry Etching)** 방식이 있으며, 정밀도가 중요합니다.
    """""")
elif process ==""증착"":
  st.markdown (""""""
    -**증착 공정**은 표면에 원하는 물질을 얇게 쌓아 올리는 과정입니다.
    - 대표적으로 **CVD (화학 기상 증착)**, **PVD (물리적 기상 증착)** 방식이 사용됩니다.
    """""")
","False","True","True","True","False","True","True","False"
"janerli/ekzamen","5.py","""""""найти значение функции в диапазоне (a, b).
Построить график функции и таблицу значений занести в файл .xlsx""""""
import tkinter as tk
from tkinter import ttk, messagebox, filedialog
import matplotlib.pyplot as plt
import numpy as np
from openpyxl import Workbook
from openpyxl.utils import get_column_letter
from openpyxl.drawing.image import Image
import io

def evaluate_function(x):
    """"""Функция, значения которой мы будем вычислять. (Пример: sin(x))""""""
    return np.sin(x)

def create_plot(min_x, max_x):
    """"""Создает график функции в заданном диапазоне и возвращает его как объект plt.Figure.""""""
    x = np.linspace(min_x, max_x, 400)
    y = [evaluate_function(val) for val in x]

    fig, ax = plt.subplots(figsize=(8, 6))
    ax.plot(x, y)
    ax.set_title(""График функции f(x) = sin(x)"")
    ax.set_xlabel(""x"")
    ax.set_ylabel(""f(x)"")
    ax.grid(True)
    return fig

def save_to_excel(min_x, max_x):
    """"""Сохраняет таблицу значений функции и график в Excel файл.""""""
    x_values = np.linspace(min_x, max_x, 400)
    y_values = [evaluate_function(val) for val in x_values]

    workbook = Workbook()
    sheet = workbook.active
    sheet.title = ""Значения функции""

    # Заголовки
    sheet['A1'] = 'x'
    sheet['B1'] = 'f(x)'

    # Запись значений
    for row, (x, y) in enumerate(zip(x_values, y_values), start=2):
        sheet.cell(row=row, column=1, value=x)
        sheet.cell(row=row, column=2, value=y)

    # Выравнивание по ширине столбца
    for column in range(1, 3):
        max_length = 0
        column_letter = get_column_letter(column)
        for cell in sheet[column_letter]:
            try:
                if len(str(cell.value)) > max_length:
                    max_length = len(str(cell.value))
            except:
                pass
        adjusted_width = max_length + 2
        sheet.column_dimensions[column_letter].width = adjusted_width

    # Добавление графика
    fig = create_plot(min_x, max_x)
    img_buf = io.BytesIO()
    fig.savefig(img_buf, format='png')
    img = Image(img_buf)
    sheet.add_image(img, 'D1')

    # Сохранение файла
    file_path = filedialog.asksaveasfilename(
        defaultextension="".xlsx"", filetypes=[(""Excel Files"", ""*.xlsx"")]
    )
    if file_path:
        try:
            workbook.save(file_path)
            messagebox.showinfo(""Успех"", ""Данные и график успешно сохранены в Excel файл."")
        except Exception as e:
             messagebox.showerror(""Ошибка"", f""Не удалось сохранить файл: {e}"")
    plt.close(fig) # Закрываем фигуру matplotlib


def process_function():
    """"""Получает диапазон и сохраняет в Excel.""""""
    try:
        min_x = float(min_entry.get())
        max_x = float(max_entry.get())

        if min_x >= max_x:
            messagebox.showerror(""Ошибка"", ""Минимальное значение должно быть меньше максимального"")
            return

        save_to_excel(min_x, max_x)
    except ValueError:
        messagebox.showerror(""Ошибка"", ""Пожалуйста, введите числа в поля"")

# Создание главного окна
window = tk.Tk()
window.title(""Вычисление функции"")
window.geometry(""500x300"")

# Фрейм для ввода диапазона
range_frame = ttk.Frame(window)
range_frame.pack(pady=10)

ttk.Label(range_frame, text=""Минимальное значение (a):"").grid(row=0, column=0, padx=5, pady=5)
min_entry = ttk.Entry(range_frame)
min_entry.grid(row=0, column=1, padx=5, pady=5)
min_entry.insert(0, ""0"") # Пример значения

ttk.Label(range_frame, text=""Максимальное значение (b):"").grid(row=1, column=0, padx=5, pady=5)
max_entry = ttk.Entry(range_frame)
max_entry.grid(row=1, column=1, padx=5, pady=5)
max_entry.insert(0, ""10"") # Пример значения

# Кнопка обработки
process_button = ttk.Button(window, text=""Вычислить и сохранить в Excel"", command=process_function)
process_button.pack(pady=10)


window.mainloop()
","False","True","True","True","False","True","False","False"
"DIvkov575/FND","c.py","import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
import re
import string

df_fake = pd.read_csv(""./Fake.csv"")
df_true = pd.read_csv(""./True.csv"")

df_fake[""class""] = 0
df_true[""class""] = 1

# Removing last 10 rows for manual testing
df_fake_manual_testing = df_fake.tail(10)
for i in range(23480,23470,-1):
    df_fake.drop([i], axis = 0, inplace = True)


df_true_manual_testing = df_true.tail(10)
for i in range(21416,21406,-1):
    df_true.drop([i], axis = 0, inplace = True)

df_fake_manual_testing[""class""] = 0
df_true_manual_testing[""class""] = 1

df_manual_testing = pd.concat([df_fake_manual_testing,df_true_manual_testing], axis = 0)
df_manual_testing.to_csv(""manual_testing.csv"")

df_merge = pd.concat([df_fake, df_true], axis =0 )
df_merge.head(10)

df = df_merge.drop([""title"", ""subject"",""date""], axis = 1)

df.isnull().sum()

df = df.sample(frac = 1)

df.reset_index(inplace = True)
df.drop([""index""], axis = 1, inplace = True)

def wordopt(text):
    text = text.lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub(""\\W"","" "",text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    return text

df[""text""] = df[""text""].apply(wordopt)

x = df[""text""]
y = df[""class""]

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)


vectorization = TfidfVectorizer()
xv_train = vectorization.fit_transform(x_train)
xv_test = vectorization.transform(x_test)

# DTC
# from sklearn.tree import DecisionTreeClassifier
#
# DT = DecisionTreeClassifier()
# DT.fit(xv_train, y_train)
# pred_dt = DT.predict(xv_test)
# DT.score(xv_test, y_test)
# print(""print1"")
# print(classification_report(y_test, pred_dt))




# Random Forest

RFC = RandomForestClassifier(random_state=0)
RFC.fit(xv_train, y_train)

pred_rfc = RFC.predict(xv_test)
RFC.score(xv_test, y_test)
print(""print2"")
print(classification_report(y_test, pred_rfc))

# # GBTREE
# from sklearn.ensemble import GradientBoostingClassifier
#
# GBC = GradientBoostingClassifier(random_state=0)
# GBC.fit(xv_train, y_train)
# pred_gbc = GBC.predict(xv_test)
# GBC.score(xv_test, y_test)
# print(""print3"")
# print(classification_report(y_test, pred_gbc))
#



# # testing
# def output_lable(n):
#     if n == 0:
#         return ""Fake News""
#     elif n == 1:
#         return ""Not A Fake News""
#
#
# def manual_testing(news):
#     testing_news = {""text"":[news]}
#     new_def_test = pd.DataFrame(testing_news)
#     new_def_test[""text""] = new_def_test[""text""].apply(wordopt)
#     new_x_test = new_def_test[""text""]
#     new_xv_test = vectorization.transform(new_x_test)
#     pred_LR = LR.predict(new_xv_test)
#     pred_DT = DT.predict(new_xv_test)
#     pred_GBC = GBC.predict(new_xv_test)
#     pred_RFC = RFC.predict(new_xv_test)
#
#     return print(""\n\nLR Prediction: {} \nDT Prediction: {} \nGBC Prediction: {} \nRFC Prediction: {}"".format(output_lable(pred_LR[0]),                                                                                                       output_lable(pred_DT[0]),
#                                                                                                               output_lable(pred_GBC[0]),
#                                                                                                               output_lable(pred_RFC[0])))
# news = str(input())
# manual_testing(news)

import joblib

# Save the trained models
print(""aasd"")
joblib.dump(RFC, './out/random_forest_model.pkl')
# joblib.dump(LR, './out/logistic_regression_model.pkl')
# joblib.dump(GBC, './out/gradient_boosting_model.pkl')
print(""asdf3"")

# Save the vectorizer
joblib.dump(vectorization, './out/tfidf_vectorizer.pkl')","True","True","True","True","False","True","True","False"
"Saiprasannakumar987/CSA1748-Artifical-intelligence-for-neuronal-application-192210609-","decision.py","# Importing necessary libraries
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

# Sample Outlook dataset
data = {
    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy', 'Rainy', 'Overcast', 'Sunny', 'Sunny', 'Rainy', 'Sunny', 'Overcast', 'Overcast', 'Rainy'],
    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],
    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],
    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],
    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
}

# Create a pandas DataFrame
df = pd.DataFrame(data)

# Convert categorical features to numerical values
df['Outlook'] = df['Outlook'].map({'Sunny': 0, 'Overcast': 1, 'Rainy': 2})
df['Temperature'] = df['Temperature'].map({'Hot': 0, 'Mild': 1, 'Cool': 2})
df['Humidity'] = df['Humidity'].map({'High': 0, 'Normal': 1})
df['Wind'] = df['Wind'].map({'Weak': 0, 'Strong': 1})
df['Play'] = df['Play'].map({'No': 0, 'Yes': 1})

# Features (X) and target (y)
X = df[['Outlook', 'Temperature', 'Humidity', 'Wind']]
y = df['Play']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier()

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f""Accuracy: {accuracy * 100:.2f}%"")

# Print the classification report
print(""\nClassification Report:"")
print(classification_report(y_test, y_pred, target_names=['No', 'Yes']))

# Visualizing the decision tree
plt.figure(figsize=(12, 8))
plot_tree(clf, feature_names=['Outlook', 'Temperature', 'Humidity', 'Wind'], class_names=['No', 'Yes'], filled=True)
plt.show()
","True","True","True","True","False","True","False","False"
"Hore01/Carbon-Emission-Track","main.py","
import streamlit as st
import matplotlib.pyplot as plt
from fpdf import FPDF

# Initialize session state
if ""history"" not in st.session_state:
    st.session_state.history = []

# Title
st.title(""Carbon Emissions Tracker"")
st.subheader(""Estimate your carbon footprint from daily activities"")

# Activity input
activity = st.selectbox(""Choose an activity:"", [""Driving"", ""Flying"", ""Electricity Use""])
value = st.number_input(""Enter distance (km) or energy used (kWh):"", min_value=0.0)

# Emission factors
factors = {
    ""Driving"": 0.21,
    ""Flying"": 0.25,
    ""Electricity Use"": 0.233
}

# Calculate emissions
if st.button(""Add to Report""):
    emissions = value * factors[activity]
    st.session_state.history.append((activity, value, emissions))
    st.success(f""Added: {emissions:.2f} kg CO₂ for {value} units of {activity}"")

# Show history and total
if st.session_state.history:
    st.subheader(""Emission Breakdown"")
    total = 0
    breakdown = {}
    for act, val, emis in st.session_state.history:
        total += emis
        breakdown[act] = breakdown.get(act, 0) + emis

    # Show pie chart
    fig, ax = plt.subplots()
    ax.pie(breakdown.values(), labels=breakdown.keys(), autopct='%1.1f%%')
    st.pyplot(fig)
    st.write(f""**Total emissions:** {total:.2f} kg CO₂"")

    # Export to PDF
    if st.button(""Download PDF Report""):
        pdf = FPDF()
        pdf.add_page()
        pdf.set_font(""Arial"", size=12)
        pdf.cell(200, 10, txt=""Carbon Emissions Report"", ln=True, align='C')
        pdf.ln(10)

        for act, val, emis in st.session_state.history:
            pdf.cell(200, 10, txt=f""{act}: {val} units → {emis:.2f} kg CO₂"", ln=True)

        pdf.cell(200, 10, txt=f""Total Emissions: {total:.2f} kg CO₂"", ln=True)

        # Save and download
        pdf.output(""carbon_emissions_report.pdf"")
        with open(""carbon_emissions_report.pdf"", ""rb"") as file:
            st.download_button(
                label=""Click to Download Report"",
                data=file,
                file_name=""carbon_emissions_report.pdf"",
                mime=""application/pdf""
            )
","False","True","True","True","False","True","True","False"
"mc6666/MyNeuralNetwork","0.py","# 導入函式庫
import numpy as np  
from keras.models import Sequential
from keras.datasets import mnist
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.utils import np_utils  # 用來後續將 label 標籤轉為 one-hot-encoding  
from matplotlib import pyplot as plt

# 載入 MNIST 資料庫的訓練資料，並自動分為『訓練組』及『測試組』
(X_train, y_train), (X_test, y_test) = mnist.load_data()


# 建立簡單的線性執行的模型
model = Sequential()
# Add Input layer, 隱藏層(hidden layer) 有 256個輸出變數
model.add(Dense(units=256, input_dim=784, kernel_initializer='normal', activation='relu')) 
# Add output layer
model.add(Dense(units=10, kernel_initializer='normal', activation='softmax'))

# 編譯: 選擇損失函數、優化方法及成效衡量方式
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) 

# 將 training 的 label 進行 one-hot encoding，例如數字 7 經過 One-hot encoding 轉換後是 0000001000，即第7個值為 1
y_TrainOneHot = np_utils.to_categorical(y_train) 
y_TestOneHot = np_utils.to_categorical(y_test) 

# 將 training 的 input 資料轉為2維
X_train_2D = X_train.reshape(60000, 28*28).astype('float32')  
X_test_2D = X_test.reshape(10000, 28*28).astype('float32')  

x_Train_norm = X_train_2D/255
x_Test_norm = X_test_2D/255

# 進行訓練, 訓練過程會存在 train_history 變數中
train_history = model.fit(x=x_Train_norm, y=y_TrainOneHot, validation_split=0.2, epochs=10, batch_size=800, verbose=2)  

# 顯示訓練成果(分數)
scores = model.evaluate(x_Test_norm, y_TestOneHot)  
print()  
print(""\t[Info] Accuracy of testing data = {:2.1f}%"".format(scores[1]*100.0))  

# 預測(prediction)
X = x_Test_norm[0:10,:]
predictions = model.predict_classes(X)
# get prediction result
print(predictions)

# 模型結構存檔
from keras.models import model_from_json
json_string = model.to_json()
with open(""model.config"", ""w"") as text_file:
    text_file.write(json_string)

    
# 模型訓練結果存檔
model.save_weights(""model.weight"")

","True","True","True","True","False","True","True","False"
"PyPatel/Machine-Learning-and-AI-in-Trading","BTC Trading: Regression on Price/lognormal_reg.py","# -*- coding: utf-8 -*-
from __future__ import absolute_import

''' Generating Log Normal Regression curve for Bitcoin price projection. '''
__author__ = 'PyPatel'
__version__ = '2.0'

from datetime import date

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression as LR

def noDays(day):
    delta = day - date(2009,1,3)
    return int(delta.days)

# Get data
data = pd.read_csv('./market-price.csv')
data.columns = ['Date', 'Price']
data['Date'] =pd.to_datetime(data.Date).dt.date

# Add column for No. Days since Bitcoin inception
data['Days'] = [noDays(i) for i in data['Date'].values]

# Remove 0 price days, since there is no data available for these days
data.drop(data[data['Price'] == 0].index, inplace = True)

# Take Logarithm based 10 of Days and Price, these are the variable to be optimise 
log_days = np.array(np.log10(data['Days'].values)).reshape(-1,1)
log_price = np.array(np.log10(data['Price'].values)).reshape(-1,1)

# Train Linear Regression model
reg = LR().fit(log_days, log_price)
print('Fitness Score of the model: ', reg.score(log_days, log_price))
print('Coefficient of the line: ', reg.coef_[0])
print('Intercept of the line: ', reg.intercept_) 

# Predictions
print('\n----------------------------------------')
print(""Today's BTC Price based on regression: US ${}"" .format(float(10**(reg.predict(np.log10(noDays(date.today())))))))
print('----------------------------------------')

# Visualization
plt.plot(log_price, label='BTC Price')
plt.plot(reg.coef_[0]*log_days + reg.intercept_ , label='Regression Line')
plt.legend()
plt.title('Lognormal regression for BTC daily price data')
plt.xlabel('No. of Days since BTC inception')
plt.ylabel(r""$\log_{10}(Price)$"")
plt.show()
","False","False","False","False","False","True","True","False"
"BogiHsu/Moire-Pattern-Simulator","A.py","import matplotlib.pyplot as plt
from matplotlib.lines import Line2D
import numpy as np
import math

window_length = 8
def line(angle, plt):
	wave_length = 0.5
	x_distance_mm = wave_length/math.cos(angle*math.pi/180)
	per_cm_x_mm = 10/x_distance_mm
	wave_length = x_distance_mm*math.cos(angle*math.pi/180)
	delta_y = wave_length/2/math.sin(angle*math.pi/180)
	wave_num = int(window_length*10/x_distance_mm)
	line_length = 10

	k = math.sin(angle*math.pi/180)*line_length/2
	for i in range(wave_num):
		line1 = [(i/per_cm_x_mm -k, window_length/2-line_length/2), (i/per_cm_x_mm+k, window_length/2+line_length/2)]
		(line1_xs, line1_ys) = zip(*line1)
		plt.plot(line1_xs, line1_ys, 'k-', linewidth=1)

	for i in range(wave_num):
		line1 = [(i/per_cm_x_mm+k, window_length/2-line_length/2), (i/per_cm_x_mm-k, window_length/2+line_length/2)]
		(line1_xs, line1_ys) = zip(*line1)
		plt.plot(line1_xs, line1_ys, 'k-', linewidth=1)

	return wave_length, delta_y
	

plt.figure(figsize=(6, 6))
plt.ion()

step = 44
index = -1*step
while True:
	angle = step-abs(index)+1
	plt.cla()
	plt.title('A')
	plt.grid(True)

	plt.xlabel('X')
	plt.xlim(0, window_length)
	plt.xticks([])
	plt.ylabel('Y')
	plt.ylim(0, window_length)
	plt.yticks([])

	wave_length, delta_y = line(angle, plt)
	print('angle:%02d,'%angle, 'lambda:%4.1f,'%wave_length, 'delta y:%6.3f'%delta_y)

	plt.pause(1e-20)
	index = -1*step if index >= step-1 else index+2

plt.ioff()
plt.show()
","False","False","False","True","False","True","False","False"
"guxiaowei1/DIYresnet-NST","1.py","from __future__ import print_function

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from PIL import Image
import matplotlib.pyplot as plt

import torchvision.transforms as transforms
import torchvision.models as models

import copy

#device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
device = torch.device( ""cpu"")
# desired size of the output image
imsize = 512 if torch.cuda.is_available() else 128  # use small size if no gpu

loader = transforms.Compose([
    transforms.Resize(imsize),  # scale imported image
    transforms.ToTensor()])  # transform it into a torch tensor


def image_loader(image_name):
    image = Image.open(image_name)
    # fake batch dimension required to fit network's input dimensions
    image = loader(image).unsqueeze(0)
    return image.to(device, torch.float)


style_img = image_loader(""./data/picasso.jpg"")
content_img = image_loader(""./data/dancing.jpg"")

assert style_img.size() == content_img.size(), \
    ""we need to import style and content images of the same size""

unloader = transforms.ToPILImage()  # reconvert into PIL image

plt.ion()

def imshow(tensor, title=None):
    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it
    image = image.squeeze(0)      # remove the fake batch dimension
    image = unloader(image)
    plt.imshow(image)
    if title is not None:
        plt.title(title)
    plt.pause(0.01) # pause a bit so that plots are updated


# plt.figure()
# imshow(style_img, title='Style Image')
#
# plt.figure()
# imshow(content_img, title='Content Image')

def gram_matrix(input):
    a, b, c, d = input.size()  # a=batch size(=1)
    # b=number of feature maps
    # (c,d)=dimensions of a f. map (N=c*d)

    features = input.view(a * b, c * d)  # resise F_XL into \hat F_XL

    G = torch.mm(features, features.t())  # compute the gram product

    # we 'normalize' the values of the gram matrix
    # by dividing by the number of element in each feature maps.
    return G.div(a * b * c * d)

class StyleLoss(nn.Module):

    def __init__(self, target_feature):
        super(StyleLoss, self).__init__()
        self.target = gram_matrix(target_feature).detach()

    def forward(self, input):
        G = gram_matrix(input)
        self.loss = F.mse_loss(G, self.target)
        return input

class ContentLoss(nn.Module):

    def __init__(self, target,):
        super(ContentLoss, self).__init__()
        # we 'detach' the target content from the tree used
        # to dynamically compute the gradient: this is a stated value,
        # not a variable. Otherwise the forward method of the criterion
        # will throw an error.
        self.target = target.detach()

    def forward(self, input):
        self.loss = F.mse_loss(input, self.target)
        return input

cnn = models.vgg19(pretrained=False).features.to(device).eval()
#cnn = models.vgg19(pretrained=False)
print(cnn)
cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)
cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)

# create a module to normalize input image so we can easily put it in a
# nn.Sequential
class Normalization(nn.Module):
    def __init__(self, mean, std):
        super(Normalization, self).__init__()
        # .view the mean and std to make them [C x 1 x 1] so that they can
        # directly work with image Tensor of shape [B x C x H x W].
        # B is batch size. C is number of channels. H is height and W is width.
        self.mean = torch.tensor(mean).view(-1, 1, 1)
        self.std = torch.tensor(std).view(-1, 1, 1)

    def forward(self, img):
        # normalize img
        return (img - self.mean) / self.std

# desired depth layers to compute style/content losses :
content_layers_default = ['conv_4']
style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']

def get_style_model_and_losses(cnn, normalization_mean, normalization_std,
                               style_img, content_img,
                               content_layers=content_layers_default,
                               style_layers=style_layers_default):
    cnn = copy.deepcopy(cnn)

    # normalization module
    normalization = Normalization(normalization_mean, normalization_std).to(device)

    # just in order to have an iterable access to or list of content/syle
    # losses
    content_losses = []
    style_losses = []

    # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential
    # to put in modules that are supposed to be activated sequentially
    model = nn.Sequential(normalization)

    i = 0  # increment every time we see a conv
    for layer in cnn.children():
        if isinstance(layer, nn.Conv2d):
            i += 1
            name = 'conv_{}'.format(i)
        elif isinstance(layer, nn.ReLU):
            name = 'relu_{}'.format(i)
            # The in-place version doesn't play very nicely with the ContentLoss
            # and StyleLoss we insert below. So we replace with out-of-place
            # ones here.
            layer = nn.ReLU(inplace=False)
        elif isinstance(layer, nn.MaxPool2d):
            name = 'pool_{}'.format(i)
        elif isinstance(layer, nn.BatchNorm2d):
            name = 'bn_{}'.format(i)
        else:
            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))

        model.add_module(name, layer)

        if name in content_layers:
            # add content loss:
            target = model(content_img).detach()
            content_loss = ContentLoss(target)
            model.add_module(""content_loss_{}"".format(i), content_loss)
            content_losses.append(content_loss)

        if name in style_layers:
            # add style loss:
            target_feature = model(style_img).detach()
            style_loss = StyleLoss(target_feature)
            model.add_module(""style_loss_{}"".format(i), style_loss)
            style_losses.append(style_loss)

    # now we trim off the layers after the last content and style losses
    for i in range(len(model) - 1, -1, -1):
        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):
            print(model[i])
            break

    model = model[:(i + 1)]

    return model, style_losses, content_losses

input_img = content_img.clone()
# if you want to use white noise instead uncomment the below line:
# input_img = torch.randn(content_img.data.size(), device=device)

# add the original input image to the figure:
plt.figure()
imshow(input_img, title='Input Image')

def get_input_optimizer(input_img):
    # this line to show that input is a parameter that requires a gradient
    optimizer = optim.LBFGS([input_img.requires_grad_()])
    return optimizer

def run_style_transfer(cnn, normalization_mean, normalization_std,
                       content_img, style_img, input_img, num_steps=300,
                       style_weight=1000000, content_weight=1):
    """"""Run the style transfer.""""""
    print('Building the style transfer model..')
    model, style_losses, content_losses = get_style_model_and_losses(cnn,
        normalization_mean, normalization_std, style_img, content_img)
    optimizer = get_input_optimizer(input_img)

    print('Optimizing..')
    run = [0]
    while run[0] <= num_steps:

        def closure():
            # correct the values of updated input image
            input_img.data.clamp_(0, 1)

            optimizer.zero_grad()
            model(input_img)
            style_score = 0
            content_score = 0

            for sl in style_losses:
                style_score += sl.loss
            for cl in content_losses:
                content_score += cl.loss

            style_score *= style_weight
            content_score *= content_weight

            loss = style_score + content_score
            loss.backward()

            run[0] += 1
            if run[0] % 50 == 0:
                print(""run {}:"".format(run))
                print('Style Loss : {:4f} Content Loss: {:4f}'.format(
                    style_score.item(), content_score.item()))
                print()

            return style_score + content_score

        optimizer.step(closure)

    # a last correction...
    input_img.data.clamp_(0, 1)

    return input_img

model, style_losses, content_losses = get_style_model_and_losses(cnn,
         cnn_normalization_mean, cnn_normalization_std, style_img, content_img)
# output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,
#                             content_img, style_img, input_img)
#
# plt.figure()
# imshow(output, title='Output Image')
#
# # sphinx_gallery_thumbnail_number = 4
# plt.ioff()
# plt.show()
print(model)","False","True","True","True","False","True","True","False"
"glangsto/analyze","a.py","#Python Script to plot raw NSF record data.
#import matplotlib.pyplot as plt
#plot the raw data from the observation
#HISTORY
#16AUG29 GIL make more efficient
#16AUG16 GIL use new radiospectrum class

#15AUG30 add option to plot range fo values
#15JUL01 GIL Initial version
#
import matplotlib.pyplot as plt
import sys
import datetime
import statistics
import radioastronomy

avetimesec = 1800.
dy = -1.

nargs = len( sys.argv)

linestyles = ['-','-','--','-.','-','--','-.','-','--','-.','-','--','-.','-','--','-.','-','--','-','-','--','-.','-','--','-.','-','--','-.','-','--','-.','-','--','-.','-','--','-.']
colors = ['-b','-r','-g','-b','-r','-g','-b','-r','-g','-b','-r','-g','-b','-r','-g','-b','-r','-g','-b','-r','-g','-b','-r','-g','-b','-r','-g','-b','-r','-g','-b','-r','-g','-b','-r','-g']
nmax = len(colors)
scalefactor = 1e8
xallmax = -9.e9
xallmin =  9.e9
yallmax = -9.e9
yallmin =  9.e9

#for symbol, value in locals().items():
#    print symbol, value

nplot = 0
nhot = 0
ncold = 0

# first read through all data and find hot load
for iii in range(1, nargs):

    filename = sys.argv[iii]

    rs = radioastronomy.Spectrum()
    rs.read_spec_ast( filename)
    rs.azel2radec()    # compute ra,dec from az,el

    if rs.telel < 0:
        if nhot == 0:
            hot = rs
            nhot = 1
        else:
            hot.ydataA = hot.ydataA + rs.ydataA
            hot.count = hot.count + rs.count
            nhot = nhot + 1

if nhot > 0:
    hot.ydataA = scalefactor * hot.ydataA / float(nhot)
    print ""Found %3d hot load obs"" % nhot
else:
    print ""No hot load data, can not calibrate""
    exit()

xv = hot.xdata * 1.e-6 # convert to MHz
yv = hot.ydataA
yallmin = min(yv)
yallmax = max(yv)

fig,ax1 = plt.subplots(figsize=(10,6))
plt.hold(True)
fig.canvas.set_window_title(filename)
az = hot.telaz
el = hot.telel
label = 'Hot Load Average'
ymin = min(yv)
ymax = max(yv)
ymed = statistics.median(yv)
count = hot.count
ncold = 0
print(' Max: %9.1f  Median: %9.1f SNR: %6.2f ; %s %s' % (ymax, ymed, ymax/ymed, count, label))
plt.plot(xv, yv, colors[0], linestyle=linestyles[0],label=label)

avetime = datetime.timedelta(seconds=avetimesec)

nread = 0        
# now read through all data and average cold sky obs
for iii in range(1, nargs):

    filename = str(sys.argv[iii])

    rs = radioastronomy.Spectrum()
#    print filename
    rs.read_spec_ast( filename)
    rs.azel2radec()    # compute ra,dec from az,el

    if rs.telel > 0:
        if ncold == 0:
            cold = rs
            ncold = 1
            # sums are weighted by durations
            cold.ydataA = cold.ydataA * cold.durationSec
            # keep track of observing time for weighted sum
            timesum = cold.durationSec
        else:
            # time difference is between mid-points of integrations
            dt = rs.utc - cold.utc 
            # add the time since midpoint of latests
            dt = dt + datetime.timedelta(seconds=rs.durationSec/2.)
            # plus time before start of the first
            dt = dt + datetime.timedelta(seconds=cold.durationSec/2.)
            # if time to average
            if dt > avetime:
                cold.ydataA = cold.ydataA/float(timesum)

                gallon = cold.gallon
                gallat = cold.gallat
                az = cold.telaz
                el = cold.telel
                parts = filename.split('/')
                nparts = len(parts)
                aname = parts[nparts-1]
                parts = aname.split('.')
                aname = parts[0]
                parts = filename.split('/')
                nparts = len(parts)
                aname = parts[nparts-1]
                parts = aname.split('.')
                aname = parts[0]
                parts = aname.split('T')
                date  = parts[0]
                time  = parts[1]
                time  = time.replace('_',':')

                label = '%s, AZ,EL: %5s,%5s, Lon,Lat=%5.1f,%5.1f' % (time, az,el,gallon,gallat)
                xv = cold.xdata * 1.e-6
                yv = cold.ydataA * scalefactor

                xmin = min(xv)
                xmax = max(xv)
                xallmin = min(xmin,xallmin)
                xallmax = max(xmax,xallmax)
                ymin = min(yv)
                ymax = max(yv)
                ymed = statistics.median(yv)
                count = cold.count
                ncold = 0
                print(' Max: %9.1f  Median: %9.1f SNR: %6.2f ; %s %s' % (ymax, ymed, ymax/ymed, count, label))
                nplot = nplot + 1
                note = cold.noteA
                    #print('%s' % note)
                yallmin = min(ymin,yallmin)
                yallmax = max(ymax,yallmax)
                ncolor = min(nmax-1, nplot) 
                plt.plot(xv, yv, colors[ncolor], linestyle=linestyles[ncolor],label=label)
            else: # else ont enough time yet, average cold data
                cold.count = cold.count + rs.count 
                cold.ydataA = cold.ydataA + (rs.ydataA * cold.durationSec)
            # keep track of observing time for weighted sum
                timesum = timesum + cold.durationSec
            # end if not a enough time
        # end if a cold file
    #end for all files to sum

plt.xlim(xallmin,xallmax)
plt.ylim(0,1.5*yallmax)
plt.title(date)
fig.canvas.set_window_title(date)
for tick in ax1.xaxis.get_major_ticks():
    tick.label.set_fontsize(14) 
for tick in ax1.yaxis.get_major_ticks():
    tick.label.set_fontsize(14) 
plt.xlabel('Frequency (MHz)', fontsize=14)
plt.ylabel('Intensity (Counts)', fontsize=14)
plt.legend(loc='upper right')
plt.show()
","False","False","False","False","True","False","False","False"
"nishio/reinforcement_learning","t.py","""""""
Quarto

0: vacant
1-16: occupied
""""""
import numpy as np
from collections import Counter

def board_to_int(board):
    s = 0L
    for i in range(16):
        s += long(board[i]) * (17 ** i)
    return s

def board_to_possible_hands(board):
    return [i for i in range(16) if board[i] == 0]

def init_board():
    return np.zeros(16, dtype=np.int)

def init_Q():
    from scipy.sparse import dok_matrix
    return dok_matrix((17 ** 16, 16 * 16))

LINES = [
    [0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15],
    [0, 4, 8, 12], [1, 5, 9, 13], [2, 6, 10, 14], [3, 7, 11, 15],
    [0, 5, 10, 15], [3, 6, 9, 12]
]
def is_win(board):
    for line in LINES:
        xs = board[line]
        if any(x == 0 for x in xs): continue
        a, b, c, d = xs - 1
        if a & b & c & d != 0:
            return 1
        if a | b | c | d != 15:
            return 1
    return 0

def print_board(board):
    """"""
    >>> print_board(range(16))
    . o x o | . o o x | . o o o | . o o o
    x o x o | x o o x | o x x x | o o o o
    x o x o | x o o x | x o o o | o x x x
    x o x o | x o o x | o x x x | x x x x
    """"""
    m = np.zeros((16, 4), dtype=np.int)
    for i in range(16):
        if board[i] == 0:
            m[i, :] = 0
        else:
            v = board[i] - 1
            for bit in range(4):  #  nth bit
                m[i, bit] = ((v >> bit) & 1) + 1

    for y in range(4):
        print ' | '.join(
            ' '.join(
                ['.ox'[v] for v in m[y * 4 : (y + 1) * 4, bit]]
            )
        for bit in range(4))
    print


def policy_random(env):
    from random import choice
    position = choice(board_to_possible_hands(env.board))
    piece = choice(env.available_pieces)
    return (position, piece)


class Environment(object):
    def __init__(self, policy=policy_random):
        self.op_policy = policy
        self.result_log =[]
        self.init_env()

    def init_env(self):
        self.board = init_board()
        self.available_pieces= range(2, 17)
        self.selected_piece = 1

    def _update(self, action, k=1, to_print=False):
        position, piece = action

        if self.board[position] != 0:
            # illegal move
            print 'illegal pos'
            self.init_env()
            self.result_log.append(-1 * k)
            return (self.board, -1 * k)

        if piece not in self.available_pieces:
            # illegal move
            print 'illegal piece'
            self.init_env()
            self.result_log.append(-1 * k)
            return (self.board, -1 * k)

        self.board[position] = self.selected_piece
        self.available_pieces.remove(piece)
        self.selected_piece = piece

        if to_print:
            print k, action
            print_board(self.board)

        b = is_win(self.board)
        if b:
            self.init_env()
            self.result_log.append(+1 * k)
            return (self.board, +1 * k)

        if not self.available_pieces:
            # put selected piece
            self.board[self.board==0] = self.selected_piece
            b = is_win(self.board)
            if to_print:
                print 'last move'
                print_board(self.board)

            self.init_env()
            if b:
                # opponent win
                self.result_log.append(-1 * k)
                return (self.board, -1 * k)
            else:
                # tie
                self.result_log.append(0)
                return (self.board, -1)

        return None

    def __call__(self, action, to_print=False):
        ret = self._update(action, k=1, to_print=to_print)
        if ret: return ret
        op_action = self.op_policy(self)
        ret = self._update(op_action, k=-1, to_print=to_print)
        if ret: return ret
        return (self.board, 0)


def play(policy1, policy2=policy_random, to_print=False):
    env = Environment()
    result = 0

    for i in range(9):
        a = policy1(env)
        s, r = env(a, to_print=to_print)
        if r != 0: break
    if to_print:
        print env.result_log[-1]
    return env.result_log[-1]

#play(policy_random, to_print=True)


class Greedy(object):
    def __init__(self):
        self.Qtable = init_Q()

    def __call__(self, env):
        from random import choice
        s = board_to_int(env.board)
        actions = (action_to_int((pos, piece))
            for pos in board_to_possible_hands(env.board)
            for piece in env.available_pieces
        )
        qa = [(self.Qtable[s, a], a) for a in actions]
        bestQ, bestA = max(qa)
        bextQ, bestA = choice([(q, a) for (q, a) in qa if q == bestQ])
        return int_to_action(bestA)


class EpsilonGreedy(object):
    def __init__(self, eps=0.1):
        self.Qtable = init_Q()
        self.eps = eps

    def __call__(self, env):
        from random import choice, random
        s = board_to_int(env.board)
        if random() < self.eps:
            pos = choice(board_to_possible_hands(env.board))
            piece = choice(env.available_pieces)
            return (pos, piece)

        actions = (action_to_int((pos, piece))
            for pos in board_to_possible_hands(env.board)
            for piece in env.available_pieces
        )
        qa = [(self.Qtable[s, a], a) for a in actions]
        bestQ, bestA = max(qa)
        bextQ, bestA = choice([(q, a) for (q, a) in qa if q == bestQ])
        return int_to_action(bestA)


def board_to_state(board):
    return board_to_int(board)

def action_to_int(action):
    pos, piece = action
    return pos * 16 + (piece - 1)

def int_to_action(i):
    assert 0 <= i < 16 * 16
    return (i / 16, i % 16 + 1)


from kagura.utils import Digest
digest = Digest(1)
battle_per_seconds = []

def sarsa(alpha, policyClass=Greedy):
    global environment, policy
    gamma = 0.9
    num_result = batch_width * num_batch
    environment = Environment()
    policy = policyClass()

    action = policy(environment)
    state = board_to_state(environment.board)
    while True:
        next_board, reward = environment(action)
        next_state = board_to_state(next_board)

        # determine a'
        next_action = policy(environment)
        nextQ = policy.Qtable[next_state, action_to_int(next_action)]

        # update Q(s, a)
        s_a = (state, action_to_int(action))
        Qsa = policy.Qtable[s_a]
        estimated_reward = reward + gamma * nextQ
        diff = estimated_reward - Qsa
        policy.Qtable[s_a] += alpha * diff

        state = next_state
        action = next_action
        if len(environment.result_log) == num_result:
            break
        t = digest.digest(len(environment.result_log))
        if t:
            battle_per_seconds.append(t)

    vs = []
    for i in range(num_batch):
        c = Counter(environment.result_log[batch_width * i : batch_width * (i + 1)])
        print c
        vs.append(float(c[1]) / batch_width)
    return vs


def qlearn(alpha, policyClass=Greedy):
    global environment, policy
    gamma = 0.9
    num_result = batch_width * num_batch
    environment = Environment()
    policy = policyClass()

    state = board_to_state(environment.board)
    while True:
        action = policy(environment)
        next_board, reward = environment(action)
        next_state = board_to_state(next_board)

        # update Q(s, a)
        maxQ = max(policy.Qtable[next_state, a] for a in board_to_possible_hands(next_board))
        s_a = (state, action_to_int(action))

        Qsa = policy.Qtable[s_a]
        estimated_reward = reward + gamma * maxQ
        diff = estimated_reward - Qsa
        policy.Qtable[s_a] += alpha * diff

        state = next_state

        if len(environment.result_log) == num_result:
            break
        t = digest.digest(len(environment.result_log))
        if t:
            battle_per_seconds.append(t)

    vs = []
    for i in range(num_batch):
        c = Counter(environment.result_log[batch_width * i : batch_width * (i + 1)])
        print c
        vs.append(float(c[1]) / batch_width)
    return vs



def plot_log():
    from kagura import load
    result_log = load(""sarsa_0.05_result_log"")
    batch_width = 1000
    num_batch = 1000
    vs = []
    for i in range(num_batch):
        c = Counter(result_log[batch_width * i : batch_width * (i + 1)])
        print c
        vs.append(float(c[1]) / batch_width)

    label = 'Sarsa(0.05)'
    imgname = 'sarsa_0.05.png'
    plot()

def plot():
    import matplotlib.pyplot as plt
    plt.clf()
    plt.plot([0.475] * len(vs), label = ""baseline"")
    plt.plot(vs, label=label)
    plt.xlabel(""iteration"")
    plt.ylabel(""Prob. of win"")
    plt.legend(loc = 4)
    plt.savefig(imgname)


def f(n, m):
    if m == 1: return n + 1
    return n * f(n - 1, m - 1) + f(n, m - 1)


if not'ex1':
    from collections import Counter
    print Counter(
        play(policy_random) for i in range(10000))
elif not'ex2':
    batch_width = 1000
    num_batch = 100
    vs = sarsa(0.5)
elif not'ex3':
    batch_width = 1000
    num_batch = 1000
    vs = sarsa(0.5)

if 0:
    batch_width = 1000
    num_batch = 1000
    vs = qlearn(0.5)
    label = 'Qlearn(0.5)'
    imgname = 'qlearn.png'
elif 0:
    batch_width = 1000
    num_batch = 1000
    vs = qlearn(0.05)
    label = 'Qlearn(0.05)'
    imgname = 'qlearn_0.05.png'


from kagura import dump
if 0:
    batch_width = 1000
    num_batch = 1000
    vs = sarsa(0.5, policyClass=EpsilonGreedy)
    label = 'Sarsa(0.5, eps=0.1)'
    imgname = 'sarsa_0.5_eps0.1.png'
    dump(environment.result_log, imgname.replace('.png', '_result_log'))
elif 0:
    batch_width = 1000
    num_batch = 1000
    vs = sarsa(0.05, policyClass=EpsilonGreedy)
    label = 'Sarsa(0.05, eps=0.1)'
    imgname = 'sarsa_0.05_eps0.1.png'
    dump(environment.result_log, imgname.replace('.png', '_result_log'))

if 0:
    batch_width = 100
    num_batch = 1000
    vs = sarsa(0.05, policyClass=Greedy)
    label = 'Sarsa(0.05)'
    imgname = 'sarsa_0.05_2.png'
    dump(environment.result_log, imgname.replace('.png', '_result_log'))


batch_width = 1000
num_batch = 100
vs = sarsa(0.5)
label = 'Sarsa(0.5)'
imgname = 'sarsa_0.5_2.png'

plot()

","True","False","False","False","False","True","True","False"
"LeanneNortje/Multilingual_speech-image_ME_dataset","8.py","#_________________________________________________________________________________________________
#
# Author: Leanne Nortje
# Year: 2023
# Email: nortjeleanne@gmail.com
#_________________________________________________________________________________________________

from pathlib import Path
from tqdm import tqdm
import numpy as np
import shutil
import torchaudio
import json
from PIL import Image
from matplotlib import image
from matplotlib import pyplot
import gzip
import librosa
from pydub import AudioSegment
import IPython.display as ipd


import argparse

parser = argparse.ArgumentParser()
parser.add_argument(
    ""mls_french_data_path"",
    metavar=""mls-french-data-path"",
    type=Path,
    help=""path to MLS French dataset."",
)
parser.add_argument(
    ""cv_french_data_path"",
    metavar=""cv-french-data-path"",
    type=Path,
    help=""path to Common Voice French dataset."",
)
args = parser.parse_args()


dutch_vocab = set()
french_vocab = set()
translations = {}
with open(Path('data/concepts.txt'), 'r') as f:
    for line in f:
        english, dutch, french = line.split()
        dutch_vocab.add(dutch)
        french_vocab.add(french)
        if english not in translations:
            translations[english] = {
                'dutch': dutch,
                'french': french
            }

dataset = np.load(Path(""data/english_me_dataset.npz""), allow_pickle=True)['dataset'].item()

mls_fn = Path(args.mls_french_data_path) / Path(""words.txt"")

print(mls_fn.is_file())

read_in = set()
french_me_dataset = {}
aud_dir = mls_fn.parent

names = {}

for wav in tqdm(list(aud_dir.rglob(f'**/*.opus'))):
    name = wav.stem
    if name not in names:
        names[name] = wav
    else:
        print(wav, names[name])

with open(mls_fn, 'r') as f:
    for line in tqdm(f):
        name = line.split()[0]
        start = float(line.split()[1])
        end = float(line.split()[2])
        word = line.split()[-1]
        
        if word in french_vocab:
            wav = names[name]
            read_in.add(word)
            aud, sr = torchaudio.load(wav)
            offset = int(start * sr)
            dur = int((end - start) * sr)
            aud, sr = torchaudio.load(wav, frame_offset=offset, num_frames=dur)
            
            new_fn = Path('data/french_words')
            new_fn.mkdir(parents=True, exist_ok=True)
            new_fn = new_fn / Path(f'{word}_{name}.wav')
            if sr != 16000:
                aud = torchaudio.functional.resample(aud, orig_freq=sr, new_freq=16000)
                sr = 16000
#             print(aud.size())
            torchaudio.save(new_fn, aud, sr)
#             print(word)
#             aud = aud.squeeze().numpy()
#             ipd.display(ipd.Audio(aud, rate=sr))

            if word not in french_me_dataset: french_me_dataset[word] = []
            french_me_dataset[word].append(new_fn)


cv_fn = Path(args.cv_french_data_path) / Path(""fr/words.txt"")
print(cv_fn.is_file())

temp_fn = 'temp.wav'

aud_dir = cv_fn.parent / 'clips'
with open(cv_fn, 'r') as f:
    for line in tqdm(f):
        wav = aud_dir / Path(line.split()[0] + '.mp3')
        word = line.split()[-1]
        if word in french_vocab:
            read_in.add(word)
            
            sound = AudioSegment.from_mp3(wav)
            sound.export(temp_fn, format=""wav"")
            aud, sr = torchaudio.load(temp_fn)

            offset = int(float(line.split()[1]) * sr)
            dur = int((float(line.split()[2]) - float(line.split()[1])) * sr)
            
            aud, sr = torchaudio.load(temp_fn, frame_offset=offset, num_frames=dur)

            name = wav.stem.split('_')[-1]
            new_fn = Path('data/french_words')
            new_fn.mkdir(parents=True, exist_ok=True)
            new_fn = new_fn / Path(f'{word}_{name}.wav')
            if sr != 16000:
                aud = torchaudio.functional.resample(aud, orig_freq=sr, new_freq=16000)
                sr = 16000
#             print(aud.size())
            torchaudio.save(new_fn, aud, sr)
#             print(word)
#             aud = aud.squeeze().numpy()
#             ipd.display(ipd.Audio(aud, rate=sr))

            if word not in french_me_dataset: french_me_dataset[word] = []
            french_me_dataset[word].append(new_fn)

print(french_vocab - read_in)

french = {}
for e_w in dataset:
    if e_w not in translations: continue
    f_w = translations[e_w]['french']
    if f_w not in french: french[f_w] = {'audio': []}
    french[f_w]['audio'] = french_me_dataset[f_w]
#     french[f_w]['images'] = dataset[e_w]['images']

for c in french:
    if len(french[c]['audio']) == 0:
#         or len(french[c]['images']) == 0: 
        print(c)

np.savez_compressed(
    Path(""data/french_me_dataset""), 
    dataset=french
    )","True","True","True","True","False","True","True","False"
"leier/z_py","z.py","'''
  Finds location of maximum probability in a 2D space defined by three seperate PDFs.
  
  | z.py: teaser at https://tech.zalando.com/jobs/65946/?gh_jid=65946
  
  | author: Dominik Leier
  
  | Requirements: python libs: numpy, scipy, matplotlib, mpl_toolkits.basemap
  
  | Usage: python z.py spree.dat 100
  |   For the program to run an ascii file containing coordinates (column1=latitude, column2=longitude)
  |   of the course of the river Spree must be provided!
  |   The number is the number of grid cells in one dimension, i.e. 100 creates a 100x100 grid
  
  | Output: a png file (zmap.png) and on-screen log
  
  | Instructions for the quest are as follows:

  | The candidate is likely to be close to the river Spree.
  | The probability at any point is given by a Gaussian function of its shortest distance to the river.
  | The function peaks at zero and has 95 per cent of its total integral within +/-2730m.
  
  | A probability distribution centered around the Brandenburg Gate also informs us
  | of the candidate's location. The distribution's radial profile is log-normal with
  | a mean of 4700m and a mode of 3877m in every direction.
  | A satellite offers further information:
  | with 95 per cent probability she is located within 2400 m distance of the satellite's path
  | (assuming a normal probability distribution)
  
'''
__version__ = ""0.1.0""


import numpy as np
import matplotlib.pyplot as plt
import sys
from pylab import *
from scipy.special import erf
from scipy.special import erfinv
from mpl_toolkits.basemap import Basemap
from matplotlib.colors import LogNorm


# global variables
# points on great circle
la_sat=np.array([52.590117,52.437385])
lo_sat=np.array([13.39915,13.553989])
# coords brandenburg gate
la_bgate=52.516288
lo_bgate=13.377689
# spree dat from file in top-level script at EOF



def main():
  """"""Finds location of maximum probability in a 2D space defined by three seperate PDFs.""""""
  
  # plotting
  fig=plt.figure(11,figsize=(6,6))
  plt.clf()
  plt.subplots_adjust(left=0.01, bottom=0.01, right=0.99, top=0.99, wspace=0., hspace=0.)
  ax = plt.subplot(1,1,1)
  xticklabels = getp(gca(), 'xticklabels')
  yticklabels = getp(gca(), 'yticklabels')
  
  # create map and set size
  boxsize=6000
  lat, lon=52.52437, 13.41053 # centre berlin map
  m = Basemap(projection='ortho',lon_0=lon,lat_0=lat,resolution='l',area_thresh='l',llcrnrx=-boxsize,llcrnry=-boxsize,urcrnrx=+boxsize,urcrnry=+boxsize)
  
  # read and plot street map / water areas from shapefile
  m.readshapefile('berlin_germany/berlin_germany_osm_roads_gen1', 'Streets',color='#DDDDDD')
  m.readshapefile('berlin_germany/berlin_germany_osm_waterareas', 'Water',color='#add8e6')
  
  # compute and plot points on great circle
  x,y=m.gcpoints(lo_sat[0],la_sat[0], lo_sat[1],la_sat[1], 100)
  m.plot(x,y,'r-',zorder=7)
  
  # plot spree list of coords
  x,y=m(lo_spree,la_spree)
  m.plot(x,y,'b.',markersize=2)
  
  # plot brandenburg gate
  gx,gy=m(lo_bgate,la_bgate) # coord transform
  m.plot(gx,gy,'bx')
  
  # create search grid
  bestY,bestX=m(13.4497839044, 52.5142566528 ) # grid centre
  minX=bestX-8000
  maxX=bestX+8000
  minY=bestY-8000
  maxY=bestY+8000
  
  rangeX=np.linspace(minX,maxX,nbins)
  rangeY=np.linspace(minY,maxY,nbins)
  npoints=None
  
  meshX, meshY = meshgrid(rangeX, rangeY)
  mzz=np.zeros(shape=(len(rangeY),len(rangeX)))
  maxmzz=0
  
  # compute probabilities for grid and location with max. prob.
  for ii in range(0, len(rangeY)):
    s = str(int(float(ii+1)/nbins*100.)) + '% computed'
    print s,
    sys.stdout.flush()
    backspace(len(s))
    for jj in range(0, len(rangeX)):
      mY,mX=m(rangeY[ii],rangeX[jj],inverse=True)
      prob_bgate,prob_spree,prob_sat,x_bgate,x_spree,x_sat=z_prob(m,mX,mY,npoints)
      mzz[ii][jj]=prob_bgate*prob_spree*prob_sat
      if mzz[ii][jj]>maxmzz:
        maxmzz=mzz[ii][jj]
        maxmX=mX
        maxmY=mY
        best_x_bgate=x_bgate
        best_x_spree=x_spree
        best_x_sat=x_sat
  
  # plot location with max. prob.
  loc_x,loc_y= m(maxmY,maxmX)
  m.plot(loc_x,loc_y,'r.',zorder=8)
  
  # output
  print ""\nDone!""
  print ""+++ search log +++""
  print ""max prob.:"", maxmzz, "" found at: (lat,lon)=("",maxmX,"","",maxmY,"")""
  print ""with dist. to spree: "", best_x_spree
  print ""with dist. to bgate: "", best_x_bgate
  print ""with dist. to sat: "", best_x_sat
  print ""resolution in m: (dx,dy)="",(maxX-minX)/nbins,(maxY-minY)/nbins #
  
  # plotting: color map and contours
  logcrange=arange(-5,-2.1,0.1)
  crange=10**logcrange
  cs = m.contourf(meshY,meshX,mzz,levels=crange,cmap=cm.Blues,zorder=5, alpha=0.3)
  
  # plotting legend
  prop = matplotlib.font_manager.FontProperties(size=10)
  b, = plot([10000,20000],'bx', linewidth=1)
  c, = plot([10000,20000],'b.', linewidth=1)
  d, = plot([10000,20000],'r-', linewidth=1)
  e, = plot([10000,20000],'r.', linewidth=1)
  legend([b,c,d,e,], ['Brandenburg Gate','Spree (file)','projected satellite path','coordinates with max. probability'],loc='upper left', ncol=1, shadow=False, fancybox=True, numpoints=1, prop=prop,labelspacing=-0.0)
  
  # plotting scale
  shift=6700
  sx=6368777.0109+shift
  sy=6370097.04832+shift
  m.plot([sx,sx+1000],[sy,sy],'k-',linewidth=5)
  s1lo,s1la=m(sx+1000,sy,inverse=True)
  s2lo,s2la=m(sx,sy,inverse=True)
  text(sx,sy-400,r' $1$ $km$')
  
  #plotting colorbar
  cax = axes([0.05,0.05,0.9,0.04]) # cbar location and dimension
  cbar = plt.colorbar(cs,boundaries=logcrange,cax=cax,orientation=""horizontal"",norm=LogNorm(vmin=1E-6, vmax=7E-3),ticks=[5e-5,1e-4,5e-4,1e-3,5e-3])
  cbar.ax.tick_params(labelsize=9)
  cbar.set_label(r'$\Pi_i P_i$                                                                                           ',labelpad=-25)

  savefig('zmap.png')
  
  return


def log_norm(x,mu,sigma):
  return exp(-(log(x)-mu)**2./(2*sigma**2))/(x*sigma*sqrt(2*pi))

def gauss(x,mu,sigma):
  return exp(-0.5*(x-mu)**2./sigma**2.)/sigma/sqrt(2*pi)

def shortest_distance_spree(m,la, lo):
  """"""Shortest distance between point and list of points
    m - basemap;
    calculates the shortest distance between coords la,lo
    and the connecting lines of two consecutive coordinates in the list
    of coordinates la_spree,lo_spree
  """"""
  min_dist=1E12
  for ii in range(0,len(la_spree)-1):
    dist_tmp= distance_line_point_la_lo(m,la,lo,la_spree[ii],lo_spree[ii],la_spree[ii+1],lo_spree[ii+1])
    segment,xl,yl=projection_in_segment(m,la,lo,la_spree[ii],lo_spree[ii],la_spree[ii+1],lo_spree[ii+1])
    if (segment== True and dist_tmp<min_dist):
      min_dist=dist_tmp
  dist_tmp=dist_closest_support_point(m,la,lo,la_spree,lo_spree)
  if dist_tmp<min_dist:
    min_dist=dist_tmp
  return min_dist

def dist_closest_support_point(m,la,lo,la_list,lo_list):
  """"""used within shortest_distance_spree
    m - basemap;
    calculates the shortest distance between point la,lo
    and points in the list of coordinates la_spree,lo_spree
  """"""
  min_dist=1E12
  for i in range(0,len(lo_spree)):
    dist_tmp= distance_on_earth(la_list[i], lo_list[i], la, lo)
    if dist_tmp<min_dist:
      min_dist=dist_tmp
  return min_dist

def projection_in_segment(m,la0,lo0,la1,lo1,la2,lo2):
  """"""used within shortest_distance_spree
    m - basemap;
    returns boolean value and coordinates of orthogonal projection point of la0,lo0
    onto the line going through la1,lo1 and la2,lo2
    boolean value True: orthogonal projection lies between the two points 
    boolean value False: orthogonal projection does not lie between the two points
  """"""
  x0,y0=m(lo0,la0)
  x1,y1=m(lo1,la1)
  x2,y2=m(lo2,la2)
  r=((x1-x2)*(x0-x2)+(y1-y2)*(y0-y2))/(((y2-y1)**2.+(x2-x1)**2.))
  xl=r*(x1-x2)+x2
  yl=r*(y1-y2)+y2
  dist_1=sqrt((x1-xl)**2.+(y1-yl)**2.)
  dist_2=sqrt((x2-xl)**2.+(y2-yl)**2.)
  dist_12=sqrt((x2-x1)**2.+(y2-y1)**2.)
  if (dist_1 <= dist_12+0.01 and dist_2 <= dist_12+0.01):
    return True, xl,yl
  else:
    return False, xl,yl

def shortest_distance_sat(m,la, lo, npoints=None):
  """"""Shortest distance between point and great circle
    m - basemap;
    npoints is optional:
    if npoints is not specified shortest_distance_sat1 is called.
    if npoints is specified shortest_distance_sat2 is called.
  """"""
  if npoints==None:
    min_dist=shortest_distance_sat1(m,la, lo)
  if npoints!=None:
    min_dist=shortest_distance_sat2(m,la, lo,npoints)
  return min_dist

def shortest_distance_sat1(m,la, lo):
  """"""Shortest distance between point and line
    m - basemap;
    calculates the shortest distance between coords la,lo
    and a line through two points defining the satellite path
  """"""
  min_dist=distance_line_point_la_lo(m,la,lo,la_sat[0],lo_sat[0],la_sat[1],lo_sat[1])
  return min_dist

def shortest_distance_sat2(m,la, lo, npoints):
  """"""Substitute function for shortest_distance_sat1
    m - basemap;
    calculates the shortest distance between coords la,lo and a great circle
    with number of equidistant points npoints.
  """"""
  x,y=m.gcpoints(lo_sat[0],la_sat[0], lo_sat[1],la_sat[1], npoints)
  lo_list,la_list=m(x,y,inverse=True)
  min_dist=1E12
  for i in range(0,len(la_list)):
    dist=distance_on_earth(la_list[i], lo_list[i], la, lo)
    if dist<min_dist:
      min_dist=dist
  return min_dist


def z_prob(m,la,lo,npoints=None):
  """"""Computes distances to point (la,lo) and probabilities according to the three PDFs
    m - basemap;
    npoints - optional, if
  """"""
  #part1 distance from river and prob
  sigma=2.730/sqrt(2)/erfinv(2*0.95-1.)
  x_spree=shortest_distance_spree(m,la, lo)
  prob_spree=gauss(x_spree,0,sigma)
  #part2 distance from brandenburg gate and prob
  mean=4.700
  mode=3.877
  x_bgate=distance_on_earth(la_bgate, lo_bgate, la, lo)
  mu=(2.*log(mean)+log(mode))/3.
  sigma=sqrt((log(mean)-log(mode))*2./3.)
  if x_bgate!=0:
    prob_bgate=log_norm(x_bgate,mu,sigma)
  else:
    prob_bgate=0
  #part3 distance from great circle and prob
  sigma=2.400/sqrt(2)/erfinv(2*0.95-1.)
  x_sat=shortest_distance_sat(m,la,lo,npoints)
  prob_sat=gauss(x_sat,0,sigma)
  return prob_bgate,prob_spree,prob_sat,x_bgate,x_spree,x_sat


def distance_line_point_la_lo(m,la0,lo0,la1,lo1,la2,lo2):
  """"""Computes distance between point (la0,lo0)
    and line through (la1,lo1) and (la2,lo2) given in longitude and latitude in km
    m - basemap;
  """"""
  x0,y0=m(lo0,la0)
  x1,y1=m(lo1,la1)
  x2,y2=m(lo2,la2)
  return distance_line_point(x0,y0,x1,y1,x2,y2)*0.001


def distance_line_point(x0,y0,x1,y1,x2,y2):
  """"""Computes distance between point (x0,y0) and line through (x1,y1) and (x2,y2)""""""
  return (abs((y2-y1)*x0-(x2-x1)*y0+x2*y1-y2*x1))/(sqrt((y2-y1)**2.+(x2-x1)**2.))

def distance_on_earth(lat1, long1, lat2, long2):
  """""" distance between two points given in long and lat on a sphere with radius 6371""""""
  radius=6371.
  degrees_to_radians = math.pi/180.0
  
  # phi = 90 - latitude
  phi1 = (90.0 - lat1)*degrees_to_radians
  phi2 = (90.0 - lat2)*degrees_to_radians
  
  # theta = longitude
  theta1 = long1*degrees_to_radians
  theta2 = long2*degrees_to_radians
  
  cos = (math.sin(phi1)*math.sin(phi2)*math.cos(theta1 - theta2) + math.cos(phi1)*math.cos(phi2))
  arc = math.acos( cos )
  
  return arc*radius



############## for calculating uncertainties ###########
def bgate_error():
  return distance_on_earth(la_bgate,lo_bgate,52.516272, 13.377722) #latter coords taken from GeoHack

def sat_error():
  boxsize=6000
  lat,lon=52.52437,13.41053 # centre Berlin map
  m = Basemap(projection='ortho',lon_0=lon,lat_0=lat,resolution='l',area_thresh='l',llcrnrx=-boxsize,llcrnry=-boxsize,urcrnrx=+boxsize,urcrnry=+boxsize)
  
  x0,y0=m(lo_sat[0],la_sat[0])
  x1,y1=m(lo_sat[1],la_sat[1])
  dog=sqrt((x1-x0)**2.+(y1-y0)**2.)
  print x0+0.5*(x1-x0),y0+0.5*(y1-y0)
  print distance_on_earth(la_sat[0],lo_sat[0],la_sat[1],lo_sat[1])
  
  x,y=m.gcpoints(lo_sat[0],la_sat[0], lo_sat[1],la_sat[1], 10000)
  
  log,lag=m(x[5000],y[5000],inverse=True)
  lop,lap=m(x0+0.5*(x1-x0),y0+0.5*(y1-y0),inverse=True)
  print lag,log,lap,lop
  return distance_on_earth(lag,log,lap,lop)

def backspace(n):
  print '\r' * n,
  return





if __name__ == ""__main__"":

  if len(sys.argv) < 3 or len(sys.argv) > 3:
    print ""type: python z.py <file> <resolution>""
    print ""e.g. python z.py spree.dat 100""
    print ""continue with default: spree.dat 100""
    z_data=np.loadtxt('spree.dat',delimiter="","")
    nbins=100
  
  else:
    file=sys.argv[1]
    nbins=int(sys.argv[2])
    z_data=np.loadtxt(file,delimiter="","")

  la_spree=z_data[:,0]
  lo_spree=z_data[:,1]

  main()

else:

  file=""spree.dat""
  nbins=100
  z_data=np.loadtxt(file,delimiter="","")
  la_spree=z_data[:,0]
  lo_spree=z_data[:,1]

","True","True","True","True","True","True","False","False"
"timnirmal/OpneCv-Learn","2.py","import cv2
import numpy as np

# 1. Wrap Perspective
from matplotlib import pyplot as plt

""""""
img = cv2.imread(""Materials/cards.jpg"")

width, height = 250, 350

pts1 = np.float32([[510, 125], [820, 205], [690, 630], [363, 532]])  # Source image coordinates to be cropped
pts2 = np.float32([[0, 0], [width, 0], [width, height], [0, height]])  # Resulting cropped image coordinates
# By changing the values of pts1 and pts2, the image can be cropped in different ways and perspectives

# Get the perspective transform matrix and apply it to the image
matrix = cv2.getPerspectiveTransform(pts1, pts2)
img_wrapped = cv2.warpPerspective(img, matrix, (width, height))  # Set Result Image Size
# (If exceeded, part of original image will be shown)

# Display the resulting image
cv2.imshow(""Original"", img)
cv2.imshow(""Result"", img_wrapped)
cv2.waitKey(0)
cv2.destroyAllWindows()

# Save the resulting image
cv2.imwrite(""2.jpg"", img)
""""""

# 2. Color Filtering
""""""
img = cv2.imread('Materials/flower.jpg') # Read the image
imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) # Convert to HSV
# HSV is a color space that is more sensitive to color differences
# Hue is the color, Saturation is the intensity, Value is the brightness
# Hue is the angle of the color, Saturation is the distance from the center of the color, Value is the brightness
# H means Hue, S means Saturation, V means Value

img_filtered = cv2.inRange(imgHSV, (0, 0, 0), (180, 255, 255)) # Filter the image
# Filter Red Color
# img_filtered = cv2.inRange(imgHSV, (0, 100, 100), (10, 255, 255))

# Display the resulting image
cv2.imshow(""Original"", img)
cv2.imshow(""Result"", imgHSV) # imshow use BGR, not HSV (So not useful for this case)
# View HSV image
cv2.imshow(""Result"", img_filtered)

cv2.waitKey(0)
cv2.destroyAllWindows()
""""""

# 3. Color Filtering with Trackbar
""""""
img = cv2.imread('Materials/flower.jpg') # Read the image
imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) # Convert to HSV

def empty(i):
    pass


height = 400
width = 500

# Trackbar name, window name, start value, max value, when we change value which fuction to call
cv2.namedWindow(""Trackbars"")
cv2.resizeWindow(""Trackbars"", width, height)

cv2.createTrackbar('Hue Min', 'Trackbars', 0, 179, empty)
cv2.createTrackbar('Hue Max', 'Trackbars', 179, 179, empty)
cv2.createTrackbar('Sat Min', 'Trackbars', 0, 255, empty)
cv2.createTrackbar('Sat Max', 'Trackbars', 255, 255, empty)
cv2.createTrackbar('Val Min', 'Trackbars', 0, 255, empty)
cv2.createTrackbar('Val Max', 'Trackbars', 255, 255, empty)



while True:
    hl = cv2.getTrackbarPos('Hue Min', 'Trackbars') # hue lower
    hu = cv2.getTrackbarPos('Hue Max', 'Trackbars') # hue upper
    sl = cv2.getTrackbarPos('Sat Min', 'Trackbars')
    su = cv2.getTrackbarPos('Sat Max', 'Trackbars')
    vl = cv2.getTrackbarPos('Val Min', 'Trackbars')
    vu = cv2.getTrackbarPos('Val Max', 'Trackbars')

    # Filter the image
    img_filtered = cv2.inRange(imgHSV, (hl, sl, vl), (hu, su, vu)) # Mask

    # Display the resulting image
    cv2.imshow(""Original"", img)
    cv2.imshow(""Result2"", img_filtered)

    imgResult = cv2.bitwise_and(img, img, mask=img_filtered)
    cv2.imshow(""Result"", imgResult)


    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cv2.waitKey(0)
cv2.destroyAllWindows()


# 74, 179, 33, 255, 20, 255
""""""

# 5. Bluing and Smoothing
""""""
img = cv2.imread('Materials/flower.jpg') # Read the image
imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) # Convert to HSV


img_filtered = cv2.inRange(imgHSV, (175, 108, 0), (179, 255, 255)) # Filter the image
# Filter Red Color

imgResult = cv2.bitwise_and(img, img, mask=img_filtered)
cv2.imshow(""Result"", imgResult)

# Display the resulting image
cv2.imshow(""Original"", img)
cv2.imshow(""Result HSV"", imgHSV) # imshow use BGR, not HSV (So not useful for this case)
# View HSV image
cv2.imshow(""Img Filtered"", img_filtered)

kernel = np.ones((15, 15), np.float32)/225
#average = cv2.blur(img_filtered, (15, 15))

# Averaging Blur
averaging_filter = cv2.filter2D(img_filtered, -1, kernel)
averaging = cv2.filter2D(imgResult, -1, kernel)
cv2.imshow(""Averaging"", averaging)
cv2.imshow(""Averaging_filter"", averaging_filter)

# Gaussian Blur
gaussian_filter = cv2.GaussianBlur(img_filtered, (15, 15), 0)
gaussian = cv2.GaussianBlur(imgResult, (15, 15), 0)
cv2.imshow(""Gaussian"", gaussian)

# Median Blur
median_filter = cv2.medianBlur(img_filtered, 15)
median = cv2.medianBlur(imgResult, 15)
cv2.imshow(""Median"", median)

# Bilateral Blur
bilateral_filter = cv2.bilateralFilter(img_filtered, 15, 75, 75)
bilateral = cv2.bilateralFilter(imgResult, 15, 75, 75)
cv2.imshow(""Bilateral"", bilateral)


cv2.waitKey(0)
cv2.destroyAllWindows()

# Kernel is a matrix that is used to filter the image

# Kernel is a matrix that is used to perform some operation on the image
# Kernel size must be odd
# Kernel size must be positive
# Kernel size must be greater than 1
# Kernel size must be less than image size
# It is like moving a window over the image


""""""


# 6. Morphological Operations (Morphological Transformation)

""""""
img = cv2.imread('Materials/flower.jpg') # Read the image
imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) # Convert to HSV

mask = cv2.inRange(imgHSV, (175, 108, 0), (179, 255, 255)) # Filter Red Color

imgResult = cv2.bitwise_and(img, img, mask=mask)

# Display the resulting image
cv2.imshow(""Original"", img)
cv2.imshow(""Img Filtered"", mask)
cv2.imshow(""Result"", imgResult)

kernel = np.ones((15, 15), np.float32)/225

# Erosion
erosion = cv2.erode(mask, kernel, iterations=4)
erosion_4times = cv2.erode(mask, kernel)
cv2.imshow(""Erosion"", erosion)
cv2.imshow(""Erosion2"", erosion_4times)

# Dilation
# Dilation is the opposite of erosion
dilation = cv2.dilate(mask, kernel)
dilation_4times = cv2.dilate(mask, kernel, iterations=4)
cv2.imshow(""Dilation"", dilation)
cv2.imshow(""Dilation2"", dilation_4times)

# Opening
# Errosion followed by Dilation (Like a blackhat filter)
opening = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
cv2.imshow(""Opening"", opening)

# Closing
# Dilation followed by Erosion (Like a whitehat filter)
closing = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
cv2.imshow(""Closing"", closing)

# Gradient
# Difference between dilation and erosion
gradient = cv2.morphologyEx(mask, cv2.MORPH_GRADIENT, kernel)
cv2.imshow(""Gradient"", gradient)

# Top Hat
# Difference between input image and Opening
top_hat = cv2.morphologyEx(mask, cv2.MORPH_TOPHAT, kernel)
cv2.imshow(""Top Hat"", top_hat)

# Black Hat
# Difference between closing and input image
black_hat = cv2.morphologyEx(mask, cv2.MORPH_BLACKHAT, kernel)
cv2.imshow(""Black Hat"", black_hat)


cv2.waitKey(0)
cv2.destroyAllWindows()
""""""

# 7. Contours

""""""
# contours is a list of all the contours in the image
# Each contour is a list of points (x, y)
# Each contour is a closed shape with an area, a perimeter and a list of points (like borders)

img = cv2.imread('Materials/contour.png') # Read the image
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Convert to grayscale

ret, thresh = cv2.threshold(gray, 230, 255, cv2.THRESH_BINARY_INV) # Threshold the image

contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) # Find contours
# RETR_TREE: Retrieves all of the contours and reconstructs a full hierarchy of nested contours
# CHAIN_APPROX_SIMPLE: Compresses horizontal, vertical, and diagonal segments and leaves only their end points
# cv2.CHAIN_APPROX_NONE: Retrieves all of the contours without establishing any hierarchical relationships
# cv2.CHAIN_APPROX_SIMPLE: Combines all nearby points on contour boundaries
# cv2.CHAIN_APPROX_TC89_L1: Modifies Teh-Chin chain approximation algorithm by Tom Foehl
# cv2.CHAIN_APPROX_TC89_KCOS: Combines all nearby points on contour boundaries with the Teh-Chin chain approximation algorithm
# cv2.CHAIN_APPROX_TC89_L1: Combines all nearby points on contour boundaries with the Teh-Chin chain approximation algorithm
# cv2.CHAIN_APPROX_TC89_KCOS: Combines all nearby points on contour boundaries with the Teh-Chin chain approximation algorithm

cv2.drawContours(img, contours, -1, (0, 0, 0), 3) # Draw all the contours on the image
# Change -1 to draw a specific contour

# -1: Draw all the contours
# 0, 0, 0: Color of the contours
# 3: Thickness of the contours
print(len(contours)) # Number of contours


cv2.imshow(""Original"", img)
#cv2.imshow(""Gray"", gray)
#cv2.imshow(""Threshold"", thresh)


# Plotting the contours

print(contours[2]) # First contour
c = contours[2].reshape(558, 2) # Reshape the contour to a 4x2 matrix


# Plot the contour
plt.plot(c[:, 0], c[:, 1], 'r')
plt.imshow(img)
plt.show()

# Plot each point of the contour
for i in range(len(c)):
    plt.plot(c[i, 0], c[i, 1], 'ro')
plt.imshow(img)
plt.show()

c2 = contours[0].reshape(-1, 2)

# Plot each point in img
for x,y in c2:
    cv2.circle(img, (x, y), 10, (255, 255, 0), -1)




# Plotting the contours



cv2.waitKey(0)
cv2.destroyAllWindows()

""""""

# 8. Contour approximation

""""""
# Approximate contours to polygons
# cv2.approxPolyDP(contour, epsilon, closed)
# contour: Input contour
# epsilon: Accuracy of the approximation
# closed: Whether or not to approximate a closed contour
# cv2.arcLength(contour, closed)
# contour: Input contour
# closed: Whether or not to approximate a closed contour
# cv2.contourArea(contour)
# contour: Input contour
# cv2.contourArea(contour, oriented)
# contour: Input contour
# oriented: Whether or not to compute the oriented contour area
# cv2.contourArea(contour, oriented)
# contour: Input contour
# oriented: Whether or not to compute the oriented contour area
# cv2.arcLength(contour, closed)
# contour: Input contour
# closed: Whether or not to approximate a closed contour
# cv2.contourArea(contour, oriented)
# contour: Input contour
# oriented: Whether or not to compute the oriented contour area
# cv2.contourArea(contour, oriented)
# contour: Input contour
# oriented: Whether or not to compute the oriented contour area

img = cv2.imread('Materials/contour.png') # Read the image
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Convert to grayscale
ret, thresh = cv2.threshold(gray, 230, 255, cv2.THRESH_BINARY_INV) # Threshold the image

contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) # Find contours

cv2.drawContours(img, contours, -1, (0, 0, 0), 3) # Draw all the contours on the image

print(len(contours)) # Number of contours




# print(contours[2]) # First contour
print(""Number of Points before approximation: "", len(contours[2]))
c = contours[2].reshape(-1, 2) # Reshape the contour to a 4x2 matrix


for x,y in c:
    cv2.circle(img, (x, y), 5, (0, 0, 255), -1)


peri = cv2.arcLength(c, True) # Calculate the perimeter of the contour  # True: Approximate the contour
approx = cv2.approxPolyDP(c, 0.04 * peri, True) # Approximate the contour
# epislon: Accuracy of the approximation
# True: Approximate the contour
# peri: Calculate the perimeter of the contour
# arcLength: for a closed contour, this function calculates the arc length, that is the sum of all the segment lengths

approx = approx.reshape(-1, 2) # Reshape the contour to a 4x2 matrix

print(""Number of Points after approximation: "", len(approx))

for x,y in approx:
    cv2.circle(img, (x, y), 10, (255, 0, 0), -1)


cv2.imshow(""Original"", img)


cv2.waitKey(0)
cv2.destroyAllWindows()

""""""","True","True","True","True","False","True","True","False"
"Mallikarjun19/Machine-Learning","5.py","import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

def create_convolutional_layer(filters, kernel_size, activation='relu', input_shape=None):
    if input_shape:
        return layers.Conv2D(filters, kernel_size, activation=activation, input_shape=input_shape)
    else:
        return layers.Conv2D(filters, kernel_size, activation=activation)

def create_maxpooling_layer(pool_size=(2, 2)):
    return layers.MaxPooling2D(pool_size)

def create_dense_layer(units, activation='relu'):
    return layers.Dense(units, activation=activation)

def build_convnet(input_shape, num_classes):
    model = models.Sequential()
    model.add(create_convolutional_layer(32, (3, 3), input_shape=input_shape))
    model.add(create_maxpooling_layer())
    model.add(create_convolutional_layer(64, (3, 3)))
    model.add(create_maxpooling_layer())
    model.add(layers.Flatten())
    model.add(create_dense_layer(128))
    model.add(create_dense_layer(num_classes, activation='softmax'))
    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
    return model

(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()
train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255
test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255

input_shape = (28, 28, 1)
num_classes = 10
model = build_convnet(input_shape, num_classes)
model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))
predictions2 = [test_images[:5],model.predict(test_images[:+5])]
from matplotlib import pyplot as plt

plt.figure(figsize=(25, 4))
for i in range(5):
    plt.subplot(1, 5, i+1)
    img_data = predictions2[0][i].reshape(28, 28)
    plt.imshow(img_data, cmap='gray')
    plt.title(f'Predicted: {tf.argmax(predictions2[1][i])}')
    plt.axis('off')
plt.show()
","False","True","True","True","False","True","False","False"
"dezzer158/SubbotinGU","6.py","import sys
import pandas as pd
from PyQt5.QtWidgets import (
    QApplication, QMainWindow, QVBoxLayout, QWidget, QPushButton,
    QLabel, QComboBox, QFileDialog, QTableWidget, QTableWidgetItem
)
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.figure import Figure
import matplotlib.pyplot as plt
import seaborn as sns

class DataVisualizer(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle(""Анализ данных и визуализация"")
        self.data = None

        self.init_ui()

    def init_ui(self):

        main_widget = QWidget()
        layout = QVBoxLayout()


        self.load_button = QPushButton(""Загрузить данные"")
        self.load_button.clicked.connect(self.load_data)
        layout.addWidget(self.load_button)

        self.stats_label = QLabel(""Статистика:"")
        layout.addWidget(self.stats_label)


        self.chart_selector = QComboBox()
        self.chart_selector.addItems([""Линейный график"", ""Гистограмма"", ""Круговая диаграмма""])
        self.chart_selector.currentIndexChanged.connect(self.update_chart)
        layout.addWidget(self.chart_selector)


        self.canvas = FigureCanvas(Figure(figsize=(5, 3)))
        layout.addWidget(self.canvas)
        self.ax = self.canvas.figure.add_subplot(111)

        main_widget.setLayout(layout)
        self.setCentralWidget(main_widget)

    def load_data(self):

        file_path, _ = QFileDialog.getOpenFileName(self, ""Выберите CSV-файл"", """", ""CSV Files (*.csv)"")
        if file_path:
            self.data = pd.read_csv(file_path)
            self.update_stats()
            self.update_chart()

    def update_stats(self):
        if self.data is not None:
            stats = f""Количество строк: {self.data.shape[0]}\n""
            stats += f""Количество столбцов: {self.data.shape[1]}\n""
            stats += f""Минимальные значения:\n{self.data.min()}\n""
            stats += f""Максимальные значения:\n{self.data.max()}\n""
            self.stats_label.setText(stats)

    def update_chart(self):
        if self.data is not None:
            chart_type = self.chart_selector.currentText()
            self.ax.clear()

            if chart_type == ""Линейный график"":
                self.data.plot(x=""Date"", y=""Value1"", ax=self.ax, kind=""line"")
            elif chart_type == ""Гистограмма"":
                self.data.plot(x=""Date"", y=""Value2"", ax=self.ax, kind=""bar"")
            elif chart_type == ""Круговая диаграмма"":
                if ""Category"" in self.data.columns:
                    self.data[""Category""].value_counts().plot.pie(ax=self.ax, autopct=""%1.1f%%"")

            self.canvas.draw()

if __name__ == ""__main__"":
    app = QApplication(sys.argv)
    window = DataVisualizer()
    window.show()
    sys.exit(app.exec_())
","True","True","True","True","False","True","False","False"
"ritt1/rit","9.py","from sklearn.datasets import fetch_olivetti_faces 
from sklearn.model_selection import train_test_split 
from sklearn.naive_bayes import GaussianNB 
from sklearn.metrics import accuracy_score 
import matplotlib.pyplot as plt 

X,y=fetch_olivetti_faces(return_X_y=True) 
Xt,Xs,yt,ys=train_test_split(X,y,test_size=0.3) 
p=GaussianNB().fit(Xt,yt).predict(Xs) 

print(""Acc:"",accuracy_score(ys,p)) 
plt.imshow(Xs[0].reshape(64,64),cmap='gray'); 
plt.title(f""T:{ys[0]} P:{p[0]}"");

plt.axis('off'); 
plt.show() 
","False","True","True","True","False","True","True","False"
"ishita6735/Trafficvision","3.py","import os
import cv2
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt

# Directory containing the images
leftimg_dir = 'C:/Users/ASUS/Downloads/idd-lite/idd20k_lite/leftImg'  # Adjust this path as needed
output_dir = 'preprocessed_leftimg'  # Directory to save preprocessed images
os.makedirs(output_dir, exist_ok=True)

# Image size to resize to
IMG_SIZE = (224, 224)


# Function to enhance image contrast using CLAHE
def enhance_image(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    cl1 = clahe.apply(gray)
    enhanced_image = cv2.merge([cl1, cl1, cl1])
    return enhanced_image


# Initialize an ImageDataGenerator for data augmentation
datagen = ImageDataGenerator(
    rescale=1.0 / 255.0,  # Normalize pixel values to [0, 1]
    rotation_range=20,  # Randomly rotate images
    width_shift_range=0.2,  # Randomly shift images horizontally
    height_shift_range=0.2,  # Randomly shift images vertically
    shear_range=0.2,  # Randomly shear images
    zoom_range=0.2,  # Randomly zoom images
    horizontal_flip=True,  # Randomly flip images horizontally
    fill_mode='nearest'  # Fill in new pixels with the nearest pixel values
)

# Iterate through the 'leftImg' directory to preprocess images
for subdir, dirs, files in os.walk(leftimg_dir):
    for file in files:
        # Check if the file is an image
        if file.endswith('.jpg') or file.endswith('.jpeg') or file.endswith('.png'):
            # Construct the full path to the image file
            image_path = os.path.join(subdir, file)

            # Load the image using OpenCV
            image = cv2.imread(image_path)

            # Check if the image was successfully loaded
            if image is not None:
                # Enhance the image contrast
                enhanced_image = enhance_image(image)

                # Resize the image
                resized_image = cv2.resize(enhanced_image, IMG_SIZE)

                # Save the preprocessed image
                save_path = os.path.join(output_dir, os.path.relpath(image_path, leftimg_dir))
                save_dir = os.path.dirname(save_path)
                os.makedirs(save_dir, exist_ok=True)
                cv2.imwrite(save_path, resized_image)

                # For demonstration purposes: Apply data augmentation and display images
                x = np.expand_dims(resized_image, axis=0)
                it = datagen.flow(x, batch_size=1)
                augmented_images = [next(it)[0] for i in range(3)]

                # Display the original, enhanced, and augmented images
                plt.figure(figsize=(12, 6))
                plt.subplot(1, 4, 1)
                plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
                plt.title(""Original Image"")
                plt.axis('off')

                plt.subplot(1, 4, 2)
                plt.imshow(cv2.cvtColor(enhanced_image, cv2.COLOR_BGR2RGB))
                plt.title(""Enhanced Image"")
                plt.axis('off')

                plt.subplot(1, 4, 3)
                plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))
                plt.title(""Resized Image"")
                plt.axis('off')

                plt.subplot(1, 4, 4)
                plt.imshow(augmented_images[0])
                plt.title(""Augmented Image"")
                plt.axis('off')

                plt.show()
                plt.close('all')  # Close all open figures to free memory
            else:
                print(""Error: Unable to read image file:"", image_path)

print(""Image preprocessing complete."")
","False","True","True","True","False","True","False","False"
"Harikrishnan23gitian/EBPL-DS-DETSASMC","color_extractor_and_quantization.py","# -*- coding: utf-8 -*-
""""""color_extractor_and_quantization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/DesignStripe/colab_notebooks/blob/main/color_extractor_and_quantization.ipynb

# Color Extractor
## This is a Google Colab Notebook showcasing how to efficiently and easily extract the colors of an image using python. We also show how to perform color quantization using OpenCV.

*Demo [picture](https://unsplash.com/photos/Dl39g6QhOIM) by [Cyrus Chew](https://unsplash.com/@cyrus_c) on Unsplash.*

## Step 1: Imports
""""""

# Imports
from google.colab import files
import cv2 as cv
from PIL import Image
import time
import numpy as np
import matplotlib.pyplot as plt

""""""## Step 2: Upload your image(s)""""""

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file ""{name}"" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

""""""## Step 3: Extract the n most present colors in the image""""""

n_colors = 5
img_max_dim = 256
final_palettes = []
quanticized_images = []
resized_imgs = []
accuracy_aimed = 1.0  # Stop iterating if the specified accuracy is reached.
max_iter = 45  # Stop the algorithm after the specified number of iterations.

for fn in uploaded.keys():
  start_time = time.time()
  img = Image.open(fn).convert('RGB')

  # Resize image for computation while keeping aspect ratio
  resized_width = int((img_max_dim * min(img.size)) / max(img.size))
  resized_img = img.copy()
  resized_img.thumbnail((img_max_dim, img_max_dim), Image.ANTIALIAS)

  # Reshape for k-means
  resized_img = np.array(resized_img, np.float32)

  # Extract the n most present colors
  # The algorithm stops iterating when any of the above conditions are met.
  criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, max_iter,
              accuracy_aimed)

  # KMEANS_PP_CENTERS: initializing centers that are far away from one another,
  # helps to reduce the randomness in the results.
  _, labels, centers = cv.kmeans(resized_img.reshape(-1,3), n_colors, None,
                                 criteria, 10, flags=cv.KMEANS_PP_CENTERS)

  # Find the biggest centers in descending order
  unique, counts = np.unique(labels, return_counts=True)
  labels_ordered = unique[counts.argsort()[::-1]]

  # Reorder colors by presence in the image - in descending order
  ordered_palette = centers[labels_ordered].astype(int)
  end_time = time.time() - start_time

  # Save color palette and elements for future visualizations
  final_palettes.append(ordered_palette)

  # Quanticize the image by reconstructing it with only the extracted colors
  quanticized_images.append(np.uint8(centers)[labels.flatten()]
                            .reshape((resized_img.shape)))
  resized_imgs.append(resized_img)

  # Display the initial image and extracted palette
  plt.figure(figsize=(12,12))
  plt.title(""Initial image"")
  plt.imshow(img)
  plt.grid()
  plt.axis('off')
  plt.show()

  plt.figure(figsize=(12,12))
  plt.imshow(ordered_palette[:n_colors][np.concatenate([[i] * 100 for i in range(len(ordered_palette[:n_colors]))]).reshape((-1,10)).T])
  plt.title(f""Final Palette with {n_colors} colors. Generated in %.2f seconds."" % end_time)
  plt.grid()
  plt.axis('off')
  plt.show()

  print(""\n"")

""""""## Color quantization

Having this new palette, you can play with the original image to reconstruct it using only these colors, which is called color quantization as seen in the article https://en.wikipedia.org/wiki/Color_quantization.

I invite you to play with the algorithm using different color combinations to better understand how it works!
""""""

# Plot the quanticized images
for index, quanticized_image in enumerate(quanticized_images):
  plt.figure(figsize=(12,12))
  plt.subplot(1,2,1)
  plt.title(""Here is what the original image looks like"")
  plt.grid()
  plt.axis('off')
  plt.imshow(resized_imgs[index]/255.0)
  plt.subplot(1,2,2)
  plt.title(f""Here is what the quanticized image looks like with {n_colors} colors!"")
  plt.grid()
  plt.axis('off')
  plt.imshow(quanticized_image)
  print(""\n"")

""""""## Next step: Build a better palette!
Use color harmony expert knowledge to understand the relation of the color in the extracted colors and build the best looking palette possible out of this image!

Here are a few tips:
- Try changing the color space! You can look at colors in different ways such as RGB, HSV, LAB, etc. HSV is a great space to better understand color relations.
- There is no general consensus on how and why a palette looks beautiful. Your best shot is to take a look at basic color harmonies and build a code that will try to follow them.
- Base yourself on the current trends, look at color palettes websites and try to orient your results towards them!
- It doesn't have to be perfect! Have fun, play with it, and build something great! Colors are a subjective thing. Some people will love them while some will hate them. You just want to allow customization to your tools as you can't force them to like a color schema.

### This is where you play with your results!
Take your palettes in the `final_palettes` list and build a code to make them look better!

*Send us your code updates and results at louis@designstripe.com. We would love to see what you achieve and give our feedback!*
""""""

# Add your code here and start testing right away!

","False","True","True","True","False","True","False","False"
"MoQuant/PyTorchForecaster","n.py","import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# Neural network class for machine learning process made with PyTorch
class Model(nn.Module):

    def __init__(self, window, output):
        super(Model, self).__init__()

        # Define layers of the neural network with appropriate dimensions
        self.layer1 = nn.Linear(window, 250)
        self.layer2 = nn.Linear(250, 100)
        self.layer3 = nn.Linear(100, output)
        self.relu = nn.ReLU()
    
    # Forward propigation
    def forward(self, x):
        x = self.layer1(x)
        x = self.relu(x)
        x = self.layer2(x)
        x = self.relu(x)
        x = self.layer3(x)
        return x

# Compute the dataframe with the technical indicators for the stock loaded in it
def AddMetrics(x, days=50):
    dataset = []
    n = len(x)
    for i in range(days, n):
        # Breaks the dataset into a window
        box = x[i-days:i]

        # Compute the mean and standard deviation of the stock data
        ma = np.mean(box)
        sd = np.std(box)

        # Compute the bollinger bands
        low = ma - 2*sd
        up = ma + 2*sd

        # Store each iteration in dataset
        dataset.append([x[i], ma, low, up])

    # Return pandas dataframe
    return pd.DataFrame(dataset, columns=['Price','MA','LB','UB'])

# Splits the data into training and testing data
def TrainTest(x, prop=0.85):
    I = int(prop*len(x))
    train = x[:I]
    test = x[I:]
    return train, test

# Build PyTorch tensors containing the stock data 
def Inputs(dataset, window=100, output=30):
    n = len(dataset)
    training_data = []
    for w in range(window, n-output+1):
        # Pulls all columns from dataset within the input bounds
        a1, a2, a3, a4 = np.array(dataset[w-window:w]).T.tolist()

        # Pulls b1 which are the close prices within the output bounds
        b1, b2, b3, b4 = np.array(dataset[w:w+output]).T.tolist()

        # Add data to training set
        training_data.append([a1 + a2 + a3 + a4, b1])

    # Convert the lists storing inputs and outputs into PyTorch tensors
    IN = [torch.tensor(item[0], dtype=torch.float32) for item in training_data]
    OUT = [torch.tensor(item[1], dtype=torch.float32) for item in training_data]
    return torch.stack(IN), torch.stack(OUT)

# Build the PyTorch tensor for the values to be predicted based on input
def Outputs(dataset, window):
    a1, a2, a3, a4 = np.array(dataset[-window:]).T.tolist()
    X = torch.tensor(a1 + a2 + a3 + a4, dtype=torch.float32)
    return torch.stack((X,)), a1

# Load the stock data, in this case Apple
data = pd.read_csv('AAPL.csv')

# Reverse the stock data to get the oldest prices first and the newest prices last
close = data['adjClose'].values.tolist()[::-1]

# Declare parameters to train the model
epochs = 2000
window = 100
output = 30
learning_rate = 0.0001

# Load the dataframe containing technical indicators
df = AddMetrics(close)

# Initialize the neural network class into the model object
model = Model(int(window*4), int(output))

# Split the data for training and testing
train, test = TrainTest(df)

# Extract the PyTorch tensors based on the trainng data
X, Y = Inputs(train, window=window, output=output)

# Declare a loss function and Adam optimizer for the Neural Network
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Run training with the set number of epochs
for epoch in range(epochs):
    # Compute outputs from model and compare them to the provided output
    outputs = model(X)
    loss = criterion(outputs, Y)

    # Set gradient and backpropigate the Neural Network
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Prints out how many epochs are left in training
    if (epoch + 1) % 100 == 0:
        print('Epochs left: ', epochs-epoch-1)

# Retrieve the testing torch tensor
XX, history = Outputs(test, window)

# Generate predictions
with torch.no_grad():
    test_outputs = model(XX)

predictions = test_outputs[-1].numpy().tolist()

# Declare plot figure and subplot
fig = plt.figure(figsize=(9, 5))
ax = fig.add_subplot(111)

# Graph the historical stock data
xa = list(range(len(history)))
ax.plot(xa, history, color='red', label='Historical')

# Graph the predicted future stock prices
xb = list(range(len(history), len(history)+len(predictions)))
ax.plot(xb, predictions, color='green', label='Predictions')

plt.show()








    
","True","True","True","True","True","True","False","False"
"Kathange/skeleton_detection_similarity","m.py","""""""
    用 opencv 讀取兩個影片
    分別生成骨架形狀
    並分析骨架相似度
    在分析完之後合併到同一視窗以供查看
    相似度部分除了看角度外，也看絕對位置 xyz都看

    使用注意：影片大小如果不一樣，要先在314行進行resize，把兩部影片變成一樣大。
    目前使用B1.pm4，他是16:9的寬高，如果寬高比例不一樣，.......再跟我說，我再改
    
    測試不同pixel的股價偵測出來的結果是否有差異
""""""



import cv2
import numpy as np
import mediapipe as mp
import csv
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec


# 設定原影片與要比較相似度的影片
input_video = 'B1.mp4'
compare_video = 'B1_trans.mp4'


class skeleton_detection_similarity:
    def __init__(self, video1, video2, adjust1, adjust2):
        # 打開兩個影片
        self.cap1 = cv2.VideoCapture(video1)
        self.cap2 = cv2.VideoCapture(video2)

        # 獲取第一個視頻的寬度和高度
        width1 = int(self.cap1.get(cv2.CAP_PROP_FRAME_WIDTH))
        height1 = int(self.cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))
        # 獲取第二個視頻的寬度和高度
        width2 = int(self.cap2.get(cv2.CAP_PROP_FRAME_WIDTH))
        height2 = int(self.cap2.get(cv2.CAP_PROP_FRAME_HEIGHT))

        # 設置調整後的寬度和高度（例如縮小一半）
        self.resized_width1 = int(width1 / adjust1)
        self.resized_height1 = int(height1 / adjust1)
        self.resized_width2 = int(width2 / adjust2)
        self.resized_height2 = int(height2 / adjust2)

        # 將每一幀得相似度放進 list 中
        self.angle_similarity_list = []
        self.position_similarity_list = []
        self.average_similarity_list = []

        # mediapipe 初始設定
        self.mp_drawing = mp.solutions.drawing_utils         # mediapipe 繪圖方法
        self.mp_drawing_styles = mp.solutions.drawing_styles # mediapipe 繪圖樣式
        self.mp_pose = mp.solutions.pose                     # mediapipe 姿勢偵測方法

        # 定義需要比較的關鍵點組（如肩、肘、腕等）
        self.key_points = [
            (11, 13, 15),   # 左肩-左肘-左腕
            (12, 14, 16),   # 右肩-右肘-右腕          
            (13, 11, 23),   # 左肘-左肩-左腰
            (14, 12, 24),   # 右肘-右肩-右腰
            (23, 25, 27),   # 左臀-左膝-左踝
            (24, 26, 28),   # 右臀-右膝-右踝
            (11, 23, 25),   # 左肩-左腰-左膝
            (12, 24, 26),   # 右肩-右腰-右膝
            (13, 15, 21),   # 左肘-左腕-左大拇指
            (14, 16, 22),   # 右肘-右腕-右大拇指
            (19, 15, 17),   # 左手-左腕-左小拇指
            (20, 16, 18),   # 右手-右腕-右小拇指
            (15, 17, 19),   # 左腕-左小拇指-左手
            (16, 18, 20),   # 右腕-右小拇指-右手
            (15, 19, 17),   # 左腕-左手-左小拇指
            (16, 20, 18),   # 右腕-右手-右小拇指
            (25, 27, 29),   # 左膝-左角踝-左腳跟
            (26, 28, 30),   # 右膝-右角踝-右腳跟
            (27, 29, 31),   # 左腳踝-左腳跟-左腳大拇指
            (28, 30, 32),   # 右腳踝-右腳跟-右腳大拇指
            (27, 31, 29),   # 左腳踝-左腳大拇指-左腳跟
            (28, 32, 30),   # 右腳踝-右腳大拇指-右腳跟
            (7,  3,  2 ),   # 左耳-左外眼角-左眼
            (8,  6,  5 ),   # 右耳-右外眼角-右眼
            (2,  1,  0 ),   # 左眼-左內眼角-鼻子
            (5,  4,  0 ),   # 右眼-右內眼角-鼻子
            (1,  0,  4 ),   # 左內眼角-鼻子-右內眼角
            (10, 0,  9 ),   # 右嘴-鼻子-左嘴
        ]

        # 初始化CSV文件
        self.csv_angle = open(f'skeleton_angle_similarity_{video1}.csv', mode='w', newline='', encoding='utf-8')
        self.csv_angle_writer = csv.writer(self.csv_angle)
        # 寫入標題行
        self.csv_angle_writer.writerow(['point', 'angle1', 'angle2'])

        # 初始化CSV文件
        self.csv_position = open(f'skeleton_position_similarity_{video1}.csv', mode='w', newline='', encoding='utf-8')
        self.csv_position_writer = csv.writer(self.csv_position)
        # 寫入標題行
        self.csv_position_writer.writerow(['point', 'position1.x', 'position1.y', 'position1.z', 'position2.x', 'position2.y', 'position2.z'])

        # 初始化CSV文件
        self.csv_similarity = open(f'skeleton_eachframe_similarity_{video1}.csv', mode='w', newline='', encoding='utf-8')
        self.csv_similarity_writer = csv.writer(self.csv_similarity)
        # 寫入標題行
        self.csv_similarity_writer.writerow(['angle_similarity_percentage', 'position_similarity_percentage', 'average_similarity'])

        # 取相似度圖表的名字
        self.chart = f'skeleton_similarity_{video1}.png'


    # 計算三點之間的角度
    def get_angle(self, p1, p2, p3):
        a = np.array(p1) # First
        b = np.array(p2) # Mid
        c = np.array(p3) # End

        # 使用arctan2()函式來進行運算，其得到之值為兩點之弧度，
        radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])
        # 而求出兩個弧度後再作相減，再乘以pi，即可得到三點連線之角度。
        angle = np.abs(radians * 180.0 / np.pi)
        
        if angle > 180.0:
            angle = 360 - angle
            
        return angle


    def calculate_angle_similarity(self, landmarks1, landmarks2):
        angles1 = []
        angles2 = []

        for points in self.key_points:
            p1_1 = [landmarks1[points[0]].x, landmarks1[points[0]].y]
            p2_1 = [landmarks1[points[1]].x, landmarks1[points[1]].y]
            p3_1 = [landmarks1[points[2]].x, landmarks1[points[2]].y]
            
            p1_2 = [landmarks2[points[0]].x, landmarks2[points[0]].y]
            p2_2 = [landmarks2[points[1]].x, landmarks2[points[1]].y]
            p3_2 = [landmarks2[points[2]].x, landmarks2[points[2]].y]
            
            # 呼叫函式計算角度
            angle1 = self.get_angle(p1_1, p2_1, p3_1)
            angle2 = self.get_angle(p1_2, p2_2, p3_2)
            
            angles1.append(angle1)
            angles2.append(angle2)

            # 寫入 CSV 檔
            self.csv_angle_writer.writerow([points, angle1, angle2])
        
        # 計算角度相似度
        angles1 = np.array(angles1)
        angles2 = np.array(angles2)
        
        cosine_similarity = np.dot(angles1, angles2) / (np.linalg.norm(angles1) * np.linalg.norm(angles2))
        angle_similarity_percentage = cosine_similarity * 100

        # 寫入 CSV 檔
        # self.csv_angle_writer.writerow([""angle_similarity_percentage"", angle_similarity_percentage, """"])
 
        return angle_similarity_percentage


    def calculate_position_similarity(self, landmarks1, landmarks2):
        distances = []

        # 提取所有關節點的座標(共33點)
        all_positions1 = {idx: (landmark.x, landmark.y, landmark.z) for idx, landmark in enumerate(landmarks1)}
        all_positions2 = {idx: (landmark.x, landmark.y, landmark.z) for idx, landmark in enumerate(landmarks2)}

        # 對兩部影片的同一個點計算歐幾里得距離
        for idx in all_positions1.keys():
            coord1 = all_positions1[idx]
            coord2 = all_positions2[idx]

            distance = np.sqrt(
                (coord1[0] - coord2[0]) ** 2 +
                (coord1[1] - coord2[1]) ** 2 +
                (coord1[2] - coord2[2]) ** 2
            )
            distances.append(distance)

            # 寫入 CSV 檔
            self.csv_position_writer.writerow([idx, coord1[0], coord1[1], coord1[2], coord2[0], coord2[1], coord2[2]])

        # 計算位置相似度 (平均距離的倒數)
        avg_distance = np.mean(distances)
        position_similarity_percentage = (1 - avg_distance) * 100

        # 寫入 CSV 檔
        # self.csv_position_writer.writerow([""position_similarity_percentage"", position_similarity_percentage, '', '', '', '', ''])

        return position_similarity_percentage


    # 比較兩個骨架的角度並計算相似度
    def calculate_similarity(self, landmarks1, landmarks2):
        # 得出角度相似度
        angle_similarity_percentage = self.calculate_angle_similarity(landmarks1, landmarks2)
        # 得出位置相似度
        position_similarity_percentage = self.calculate_position_similarity(landmarks1, landmarks2)
        # 計算兩者的平均相似度
        average_similarity = (angle_similarity_percentage + position_similarity_percentage) / 2
        
        return angle_similarity_percentage, position_similarity_percentage, average_similarity


    def run(self):
        # 啟用姿勢偵測
        with self.mp_pose.Pose(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5) as pose:

            # 確認是否成功打開
            if not self.cap1.isOpened():
                print(""Cannot open camera1"")
                exit()
            if not self.cap2.isOpened():
                print(""Cannot open camera2"")
                exit()

            while True:
                ret1, frame1 = self.cap1.read()
                ret2, frame2 = self.cap2.read()
                if not ret1 or not ret2:
                    print(""Cannot receive frame"")
                    break
                
                # 調整長寬
                after1 = cv2.resize(frame1, (self.resized_width1, self.resized_height1))
                after2 = cv2.resize(frame2, (self.resized_width2, self.resized_height2))

                resized_frame1 = cv2.cvtColor(after1, cv2.COLOR_BGR2RGB)   # 將 BGR 轉換成 RGB
                results1 = pose.process(resized_frame1)                  # 取得姿勢偵測結果
                # 根據姿勢偵測結果，標記身體節點和骨架
                self.mp_drawing.draw_landmarks(
                    after1,
                    results1.pose_landmarks,
                    self.mp_pose.POSE_CONNECTIONS,
                    landmark_drawing_spec=self.mp_drawing_styles.get_default_pose_landmarks_style())
                
                resized_frame2 = cv2.cvtColor(after2, cv2.COLOR_BGR2RGB)   # 將 BGR 轉換成 RGB
                results2 = pose.process(resized_frame2)                  # 取得姿勢偵測結果
                # 根據姿勢偵測結果，標記身體節點和骨架
                self.mp_drawing.draw_landmarks(
                    after2,
                    results2.pose_landmarks,
                    self.mp_pose.POSE_CONNECTIONS,
                    landmark_drawing_spec=self.mp_drawing_styles.get_default_pose_landmarks_style())

                # 如果兩個視頻都有檢測到骨架，計算相似度
                if results1.pose_landmarks and results2.pose_landmarks:
                    angle_similarity, position_similarity, average_similarity = self.calculate_similarity(results1.pose_landmarks.landmark, results2.pose_landmarks.landmark)
                    self.angle_similarity_list.append(angle_similarity)
                    self.position_similarity_list.append(position_similarity)
                    self.average_similarity_list.append(average_similarity)
                    self.csv_similarity_writer.writerow([angle_similarity, position_similarity, average_similarity])

                # 創建分隔線
                separator = np.zeros((self.resized_height1, 10, 3), dtype=np.uint8)
                separator[:] = (0, 0, 255)  # 將分隔線設置為紅色

                # 合併兩個視頻到一個窗口中
                combined_frame = np.hstack((after1, separator, after2))
                
                # 顯示調整後的視頻
                cv2.imshow('Combined Video', combined_frame)

                # 按 'q' 鍵退出
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
        
        # 釋放視頻對象並關閉所有窗口
        self.cap1.release()
        self.cap2.release()
        self.csv_angle.close()
        self.csv_position.close()
        self.csv_similarity.close()
        cv2.destroyAllWindows()
    

    # 繪製相似度折線圖
    def plot_similarity_graphs(self):
        # 創建圖表和 GridSpec 佈局
        fig = plt.figure(figsize=(12, 6))
        gs = GridSpec(2, 3, height_ratios=[3, 1], figure=fig)

        # 角度相似度圖
        ax1 = fig.add_subplot(gs[0, 0])
        ax1.plot(self.angle_similarity_list, label=""Angle Similarity"", color=""blue"")
        ax1.set_xlabel(""Frame"")
        ax1.set_ylabel(""Similarity (%)"")
        ax1.set_title(""Angle Similarity Percentage"")
        ax1.grid(True)
        ax1.legend()

        # 位置相似度圖
        ax2 = fig.add_subplot(gs[0, 1])
        ax2.plot(self.position_similarity_list, label=""Position Similarity"", color=""green"")
        ax2.set_xlabel(""Frame"")
        ax2.set_ylabel(""Similarity (%)"")
        ax2.set_title(""Position Similarity Percentage"")
        ax2.grid(True)
        ax2.legend()

        # 平均相似度圖
        ax3 = fig.add_subplot(gs[0, 2])
        ax3.plot(self.average_similarity_list, label=""Average Similarity"", color=""red"")
        ax3.set_xlabel(""Frame"")
        ax3.set_ylabel(""Similarity (%)"")
        ax3.set_title(""Average Similarity Percentage"")
        ax3.grid(True)
        ax3.legend()

        # 文本信息
        ax_text = fig.add_subplot(gs[1, :])
        ax_text.axis('off')  # 隱藏坐標軸

        # 構建信息文本
        info_text = (
            f""Keypoint Length: {len(self.key_points)}\n""
            f""List Length: {len(self.angle_similarity_list)}\n""
            f""Average Angle Similarity: {np.mean(self.angle_similarity_list):.2f}%\n""
            f""Average Position Similarity: {np.mean(self.position_similarity_list):.2f}%\n""
            f""Overall Average Similarity: {np.mean(self.average_similarity_list):.2f}%""
        )

        # 添加文本到圖表
        ax_text.text(0.5, 0.5, info_text, ha=""center"", va=""center"", fontsize=14, bbox={""facecolor"":""orange"", ""alpha"":0.5, ""pad"":10})

        # 調整佈局
        plt.tight_layout()
        plt.savefig(self.chart, dpi=300)
        plt.close()


if __name__ == '__main__':
    sds = skeleton_detection_similarity(input_video, compare_video, 6, 1.6)
    sds.run()

    print(""keypoint len:"", len(sds.key_points))
    print(""list len:"", len(sds.angle_similarity_list))
    # 印出相似度
    print(f""Average Angle Similarity: {np.mean(sds.angle_similarity_list):.2f}%"")
    print(f""Average Position Similarity: {np.mean(sds.position_similarity_list):.2f}%"")
    print(f""Overall Average Similarity: {np.mean(sds.average_similarity_list):.2f}%"")

    sds.plot_similarity_graphs()

","True","True","True","False","True","False","False","False"
"MarceloAnsilago/Mls","p.py","import pandas as pd
import streamlit as st
from streamlit_option_menu import option_menu
from st_aggrid import AgGrid, GridOptionsBuilder
import yfinance as yf
import base64
import os
from datetime import datetime, timedelta
from PIL import Image
from statsmodels.tsa.stattools import coint
import numpy as np
import statsmodels.api as sm
from hurst import compute_Hc
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
from sklearn.linear_model import LinearRegression
import time
import mplfinance as mpf               
from io import BytesIO
import base64                

# Inicializando o estado global para as cotações
if ""global_cotacoes"" not in st.session_state:
    st.session_state[""global_cotacoes""] = pd.DataFrame()


st.set_page_config(page_title=""Gerenciamento de Ações"", page_icon="":chart_with_upwards_trend:"", layout=""wide"")
logo_image = Image.open(""logos/LogoApp.png"")

# Função para carregar o ícone
def carregar_icone(ticker):
    # Verifica o formato do ticker e ajusta o nome do arquivo
    if len(ticker) >= 5 and ticker[4].isdigit():
        ticker_base = ticker[:4]
    else:
        ticker_base = ticker.replace("".SA"", """")

    # Caminho do arquivo do ícone
    icon_path = f""logos/{ticker_base}.jpg""

    if os.path.exists(icon_path):
        try:
            with open(icon_path, ""rb"") as image_file:
                encoded_string = base64.b64encode(image_file.read()).decode()
                return f""data:image/jpg;base64,{encoded_string}""
        except Exception as e:
            return None
    else:
        return None


# Função para calcular Half-Life
def half_life_calc(ts):
    lagged = ts.shift(1).fillna(method=""bfill"")
    delta = ts - lagged
    X = sm.add_constant(lagged.values)
    ar_res = sm.OLS(delta, X).fit()
    half_life = -1 * np.log(2) / ar_res.params[1]
    return half_life

# Função para calcular Hurst exponent
def hurst_exponent(ts):
    H, c, data = compute_Hc(ts, kind='price', simplified=True)
    return H

# Função para calcular Beta Rotation (ang. cof)
def beta_rotation(series_x, series_y, window=40):
    beta_list = []
    try:
        for i in range(0, len(series_x) - window):
            slice_x = series_x[i:i + window]
            slice_y = series_y[i:i + window]
            X = sm.add_constant(slice_x.values)
            mod = sm.OLS(slice_y, X)
            results = mod.fit()
            beta = results.params[1]
            beta_list.append(beta)
    except Exception as e:
        st.error(f""Erro ao calcular beta rotation: {e}"")
        raise

    return beta_list[-1]  # Return the most recent beta value

# Função para calcular o beta móvel em uma janela deslizante
def calcular_beta_movel(S1, S2, window=40):
    returns_S1 = np.log(S1 / S1.shift(1)).dropna()
    returns_S2 = np.log(S2 / S2.shift(1)).dropna()

    betas = []
    index_values = returns_S1.index[window-1:]  # Ajustar para a janela

    for i in range(window, len(returns_S1) + 1):
        reg = LinearRegression().fit(returns_S2[i-window:i].values.reshape(-1, 1), returns_S1[i-window:i].values)
        betas.append(reg.coef_[0])

    return pd.Series(betas, index=index_values)

# Exibir o gráfico de beta móvel
def plotar_beta_movel(S1, S2, window=40):
    try:
        returns_S1 = np.log(S1 / S1.shift(1)).dropna()
        returns_S2 = np.log(S2 / S2.shift(1)).dropna()

        betas = []
        index_values = returns_S1.index[window - 1:]  # Ajustar para a janela

        for i in range(window, len(returns_S1) + 1):
            reg = LinearRegression().fit(
                returns_S2[i - window:i].values.reshape(-1, 1),
                returns_S1[i - window:i].values
            )
            betas.append(reg.coef_[0])

        beta_movel = pd.Series(betas, index=index_values)

        # Plotar o gráfico de beta móvel
        plt.figure(figsize=(10, 5))
        plt.plot(beta_movel, label=f'Beta Móvel ({window} períodos)')
        plt.axhline(0, color='black', linestyle='--')
        plt.title(f'Beta Móvel ({window} períodos)')
        plt.xlabel('Data')
        plt.ylabel('Beta')
        plt.legend()
        plt.xticks(rotation=45, fontsize=6)
        plt.grid(True)

        # Reduzir a quantidade de rótulos no eixo X
        ax = plt.gca()             # Pega o axis atual
        ticks = ax.get_xticks()    # Pega os ticks atuais
        ax.set_xticks(ticks[::5])  # Exibe somente 1 a cada 5

        st.pyplot(plt)
    except Exception as e:
        st.error(f""Erro ao calcular ou plotar o beta móvel: {e}"")


# Exibir o gráfico de dispersão entre os dois ativos
def plotar_grafico_dispersao(S1, S2):
    plt.figure(figsize=(10, 5))
    plt.scatter(S1, S2)
    plt.title(f'Dispersão entre {S1.name} e {S2.name}')
    plt.xlabel(f'{S1.name}')
    plt.ylabel(f'{S2.name}')
    plt.grid(True)
    st.pyplot(plt)


def obter_preco_atual(ticker):
    dados = yf.download(ticker, period=""1d"")  # Baixar o dado mais recente
    if not dados.empty:
        return dados['Close'].iloc[-1]  # Retornar o preço de fechamento mais recente
    else:
        return None
# Função para plotar o gráfico do Z-Score
def plotar_grafico_zscore(S1, S2):
    ratios = S1 / S2
    zscore_series = (ratios - ratios.mean()) / ratios.std()

    plt.figure(figsize=(10, 5))
    plt.plot(zscore_series, label='Z-Score')
    plt.axhline(0, color='black', linestyle='--')
    plt.axhline(2, color='red', linestyle='--')
    plt.axhline(-2, color='green', linestyle='--')
    plt.legend(loc='best')
    plt.xlabel('Data')
    plt.ylabel('Z-Score')
    plt.xticks(rotation=45, fontsize=6)
    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True, prune='both'))
    st.pyplot(plt)

    # Função para plotar o gráfico dos preços das ações
def plotar_grafico_precos(S1, S2, ticker1, ticker2):
    plt.figure(figsize=(10, 5))
    plt.plot(S1, label=ticker1)
    plt.plot(S2, label=ticker2)
    plt.legend(loc='best')
    plt.xlabel('Data')
    plt.ylabel('Preço de Fechamento')
    plt.xticks(rotation=45, fontsize=6)
    st.pyplot(plt)


# Função para encontrar pares cointegrados e calcular z-score, half-life, Hurst, ang. cof
def find_cointegrated_pairs(data, zscore_threshold_upper, zscore_threshold_lower):
    n = data.shape[1]
    keys = data.keys()
    pairs = []
    pvalues = []
    zscores = []
    half_lives = []
    hursts = []
    beta_rotations = []

    for i in range(n):
        for j in range(i + 1, n):
            S1 = data[keys[i]].dropna()  # Remover NaNs de S1
            S2 = data[keys[j]].dropna()  # Remover NaNs de S2

            # Garantir que ambas as séries tenham o mesmo comprimento após a remoção dos NaNs
            combined = pd.concat([S1, S2], axis=1).dropna()
            if len(combined) < 2:  # Verificar se ainda há dados suficientes
                continue

            S1 = combined.iloc[:, 0]
            S2 = combined.iloc[:, 1]

            try:
                score, pvalue, _ = coint(S1, S2)
                if pvalue < 0.05:
                    ratios = S1 / S2
                    zscore = (ratios - ratios.mean()) / ratios.std()

                    if zscore.iloc[-1] > zscore_threshold_upper or zscore.iloc[-1] < zscore_threshold_lower:
                        pairs.append((keys[i], keys[j]))
                        pvalues.append(pvalue)
                        zscores.append(zscore.iloc[-1])
                        half_lives.append(half_life_calc(ratios))
                        hursts.append(hurst_exponent(ratios))
                        beta_rotations.append(beta_rotation(S1, S2))

            except Exception as e:
                print(f""Erro ao calcular cointegração para {keys[i]} e {keys[j]}: {e}"")
                continue

    return pairs, pvalues, zscores, half_lives, hursts, beta_rotations
# Função para criar cards explicativos das métricas
def criar_card_metrica(nome_metrica, valor_metrica, descricao):
    st.markdown(
        f""""""
        <div style=""border: 1px solid #ddd; border-radius: 10px; padding: 10px; text-align: center; background-color: #f9f9f9; height: 250px; margin-bottom: 15px; display: flex; flex-direction: column; justify-content: space-between;"">
            <div>
                <h4 style=""margin: 0;"">{nome_metrica}</h4>
                <hr style=""border: none; border-top: 2px solid red; margin: 5px 0 10px 0;"">
            </div>
            <div style=""flex-grow: 1; display: flex; justify-content: center; align-items: center;"">
                <h2 style=""margin: 0; font-size: 24px;"">{valor_metrica}</h2>
            </div>
            <div style=""margin-top: 10px; text-align: center;"">
                <p style=""font-size: 14px; color: #888; margin-bottom: 4px;"">{descricao}</p>
            </div>
        </div>
        """""",
        unsafe_allow_html=True
    )


def exibir_metrica_cartao(ticker, ultimo_preco, ultima_data, icone=None):
    icone_html = (
        f'<img src=""{icone}"" style=""max-width: 100px; max-height: 100px; object-fit: contain;"">'
        if icone else '<p style=""color: red;"">Sem Ícone</p>'
    )

    st.markdown(
        f""""""
        <div style=""border: 1px solid #ddd; border-radius: 10px; padding: 10px; text-align: center; background-color: #f9f9f9; height: 300px; margin-bottom: 15px; display: flex; flex-direction: column; justify-content: space-between;"">
            <div>
                <h2 style=""margin: 0;"">{ticker}</h2> <!-- Aumentei a fonte do título -->
                <hr style=""border: none; border-top: 2px solid red; margin: 5px 0 10px 0;"">
            </div>
            <div style=""flex-grow: 1; display: flex; justify-content: center; align-items: center;"">
                {icone_html} <!-- Ícone com altura ajustada -->
            </div>
            <div style=""margin-top: 10px; text-align: center;"">
                <h6 style=""font-size: 14px; color: #888; margin-bottom: 4px;"">Última Cotação ({ultima_data})</h6>
                <h3 style=""margin: 0; font-size: 24px;"">R$ {ultimo_preco:.2f}</h3>
            </div>
        </div>
        """""",
        unsafe_allow_html=True
    )


# Menu Lateral
with st.sidebar:
    st.image(logo_image, use_column_width=True)  # Exibir a imagem no menu lateral
    selected = option_menu(
        menu_title=""Menu Principal"",  # required
        options=[""Página Inicial"", ""Cotações"", ""Análise"", ""Operações"", ""Backtesting""],  # required
        icons=[""house"", ""currency-exchange"", ""graph-up-arrow"", ""briefcase"", ""clock-history""],  # ícones para cada página
        menu_icon=""cast"",  # ícone do menu
        default_index=0,  # seleciona a aba 'Página Inicial'
    )

# Aba ""Ações Acompanhadas""
if selected == ""Página Inicial"":
    # st.title(""Ações Acompanhadas"")
    
    def get_base64(file_path):
        with open(file_path, ""rb"") as f:
            data = f.read()
        return base64.b64encode(data).decode()

    image_base64 = get_base64(""logos/Ações_Acompanhadas.png"")

    st.markdown(
        f""""""
        <div style=""display: flex; align-items: center;"">
            <img src=""data:image/png;base64,{image_base64}"" alt=""Logo"" style=""height: 130px; margin-right: 10px;"">
            <h1 style=""margin: 0;"">Ações Acompanhadas</h1>
        </div>
        """""",
        unsafe_allow_html=True
    )


    # Verificar se o DataFrame global tem dados
    if ""global_cotacoes"" in st.session_state and not st.session_state[""global_cotacoes""].empty:
        # DataFrame global com as cotações
        cotacoes_df = st.session_state[""global_cotacoes""].copy()

        # Verificar se o índice é chamado 'Date' e transformá-lo em coluna
        if ""Date"" not in cotacoes_df.columns:
            cotacoes_df.reset_index(inplace=True)

        # Garantir que a coluna ""Date"" exista
        if ""Date"" in cotacoes_df.columns:
            # Preparar os dados para exibição (última cotação de cada ticker)
            cotacoes_df = cotacoes_df.melt(id_vars=[""Date""], var_name=""ticker"", value_name=""fechamento"")
            cotacoes_df = cotacoes_df.dropna().sort_values(by=[""ticker"", ""Date""], ascending=[True, False])
            cotacoes_df = cotacoes_df.groupby(""ticker"").first().reset_index()

            # Exibir as métricas em 5 colunas com espaçamento de 5px
            cols = st.columns(5, gap=""small"")

            for index, row in cotacoes_df.iterrows():
                ticker = row['ticker']
                ultimo_preco = row['fechamento']
                ultima_data = row['Date']

                # **Carregar ícone do ticker**
                icone = carregar_icone(ticker)

                # Exibir a métrica no formato de cartão
                with cols[index % 5]:
                    exibir_metrica_cartao(ticker, ultimo_preco, ultima_data, icone)
        else:
            st.error(""A coluna de datas ('Date') não foi encontrada no DataFrame."")
    else:
        st.warning(""Nenhuma cotação carregada. Por favor, carregue as cotações na aba 'Cotações'."")





# Aba de Cotações
if selected == ""Cotações"":
    def get_base64(file_path):
     with open(os.path.abspath(file_path), ""rb"") as f:
        return base64.b64encode(f.read()).decode()

    image_base64 = get_base64(""logos/cotacoes.png"") 
    
    st.markdown(
        f""""""
        <div style=""display: flex; align-items: center;"">
            <img src=""data:image/png;base64,{image_base64}"" alt=""Cotação"" style=""height: 130px; margin-right: 10px;"">
            <h1 style=""margin: 0;"">Cotações de Ações</h1>
        </div>
        """""",
        unsafe_allow_html=True
    )

    # Upload do arquivo TXT com tickers
    st.markdown(""### Upload do Arquivo com Tickers"")
    uploaded_file = st.file_uploader(
        ""Envie um arquivo TXT com os tickers das ações (um por linha):"", 
        type=""txt"", 
        key=""file_uploader_cotacoes""
    )

    # Entrada do número de períodos para buscar cotações
    num_periodos = st.number_input(
        ""Número de Períodos (em dias) para Buscar Cotações"",
        min_value=1,
        max_value=365,
        value=200,
        step=1,
        key=""number_input_periodos""
    )

    # Botão para processar e exibir as cotações
    if st.button(""Carregar Cotações"", key=""botao_carregar_cotacoes""):
        if uploaded_file:
            # Ler os tickers do arquivo TXT
            tickers = uploaded_file.read().decode('utf-8').splitlines()
            tickers = [ticker.strip().upper() for ticker in tickers if ticker.strip()]
            
            if tickers:
                # Criar um DataFrame temporário para armazenar novas cotações
                new_cotacoes = pd.DataFrame()

                progress_bar = st.progress(0)  # Barra de progresso
                status_text = st.empty()  # Espaço para texto dinâmico

                for idx, ticker in enumerate(tickers):
                    try:
                        status_text.text(f""Baixando cotações para {ticker}..."")
                        dados = yf.download(ticker, period=f""{num_periodos}d"")['Close']
                        dados.name = ticker  # Renomear a série com o ticker
                        new_cotacoes = pd.concat([new_cotacoes, dados], axis=1)
                    except Exception as e:
                        st.error(f""Erro ao buscar cotações para {ticker}: {e}"")

                    # Atualizar barra de progresso
                    progress_bar.progress((idx + 1) / len(tickers))
                
                # Remover status e barra após conclusão
                status_text.empty()
                progress_bar.empty()

                # Atualizar o DataFrame global no session_state
                if not new_cotacoes.empty:
                    # Formatar o índice de datas antes de atualizar o DataFrame global
                    if isinstance(new_cotacoes.index, pd.DatetimeIndex):
                        new_cotacoes.index = new_cotacoes.index.strftime(""%Y-%m-%d"")
                    new_cotacoes.index.name = ""Date""  # Nomear o índice como 'Date'

                    # Concatenar os dados ao DataFrame global, removendo duplicatas
                    st.session_state[""global_cotacoes""] = pd.concat(
                        [st.session_state[""global_cotacoes""], new_cotacoes], axis=1
                    ).loc[:, ~pd.concat(
                        [st.session_state[""global_cotacoes""], new_cotacoes], axis=1
                    ).columns.duplicated()]
                    st.success(""Cotações carregadas com sucesso!"")
            else:
                st.error(""O arquivo está vazio ou não contém tickers válidos."")
        else:
            st.error(""Por favor, envie um arquivo TXT com os tickers das ações."")

    # Exibir o DataFrame global no Streamlit, se não estiver vazio
    if not st.session_state[""global_cotacoes""].empty:
        df_para_exibir = st.session_state[""global_cotacoes""].reset_index()
        st.markdown(""### Cotações Carregadas"")
        st.dataframe(df_para_exibir)
    else:
        st.warning(""Nenhuma cotação foi carregada ainda."")

def mostrar_fluxo_liquido(venda_total: float, compra_total: float) -> float:
    """"""
    Exibe o fluxo financeiro líquido da operação de long & short.

    Retorna o valor do fluxo líquido para uso posterior.
    """"""
    resultado_total = venda_total - compra_total

    mensagem = f""**Fluxo Líquido da Operação: R$ {resultado_total:.2f}**""
    explicacao = (
        ""Este valor representa o fluxo financeiro inicial da operação de long & short. ""
        ""Se for positivo, você recebe esse montante ao montar a estratégia. ""
        ""Se for negativo, você precisa investir esse valor para abrir as posições.""
    )

    if resultado_total >= 0:
        st.success(mensagem)
    else:
        st.error(mensagem)

    st.markdown(f""<p style='font-size: 14px; color: #666;'>{explicacao}</p>"", unsafe_allow_html=True)

    return resultado_total



if selected == ""Análise"":
   
    def get_base64(file_path):
     with open(os.path.abspath(file_path), ""rb"") as f:
        return base64.b64encode(f.read()).decode()

    image_base64 = get_base64(""logos/analise.png"") 
    
    st.markdown(
        f""""""
        <div style=""display: flex; align-items: center;"">
            <img src=""data:image/png;base64,{image_base64}"" alt=""Cotação"" style=""height: 130px; margin-right: 10px;"">
            <h1 style=""margin: 0;"">Análise de Cointegração de Ações</h1>
        </div>
        """""",
        unsafe_allow_html=True
    )

    # Seleção de parâmetros para análise
    with st.form(key='analysis_form'):
        numero_periodos = st.number_input(
            ""Número de Períodos para Análise"",
            min_value=1,
            value=120,
            help=""Número de períodos (mais recentes) para considerar na análise de cointegração.""
        )
        zscore_threshold_upper = st.number_input(""Limite Superior do Z-Score"", value=2.0)
        zscore_threshold_lower = st.number_input(""Limite Inferior do Z-Score"", value=-2.0)
        submit_button = st.form_submit_button(label=""Analisar Pares Cointegrados"")

    if submit_button or 'cotacoes_pivot' in st.session_state:
        if submit_button:
            if ""global_cotacoes"" not in st.session_state or st.session_state[""global_cotacoes""].empty:
                st.error(""Nenhuma cotação carregada. Por favor, carregue as cotações antes de realizar a análise."")
                st.stop()

            cotacoes_df = st.session_state[""global_cotacoes""]
            if ""Date"" in cotacoes_df.columns:
                cotacoes_df.set_index(""Date"", inplace=True)

            cotacoes_pivot = cotacoes_df.tail(numero_periodos)
            st.session_state['cotacoes_pivot'] = cotacoes_pivot

        cotacoes_pivot = st.session_state['cotacoes_pivot']
        st.write(f""Número de períodos selecionados para análise: {cotacoes_pivot.shape[0]}"")

        st.subheader(""Pares Encontrados"")

        # Encontrar pares cointegrados e calcular métricas
        # Spinner enquanto os pares são analisados
        with st.spinner(""🔍 Analisando cointegração entre os ativos...""):
            pairs, pvalues, zscores, half_lives, hursts, beta_rotations = find_cointegrated_pairs(
                cotacoes_pivot, zscore_threshold_upper, zscore_threshold_lower
            )

        if pairs:
            for idx, (pair, zscore, pvalue, hurst, beta, half_life) in enumerate(zip(pairs, zscores, pvalues, hursts, beta_rotations, half_lives)):
                par_str = f""{pair[0]} - {pair[1]}""
                metricas_str = f""Z-Score: {zscore:.2f} | P-Value: {pvalue:.4f} | Hurst: {hurst:.4f} | Beta: {beta:.4f} | Half-Life: {half_life:.2f}""
                
                if st.button(f""{par_str} | {metricas_str}"", key=f""btn_{idx}""):
                    st.session_state['par_selecionado'] = pair

            if 'par_selecionado' in st.session_state:
                pair_selected = st.session_state['par_selecionado']
                S1 = cotacoes_pivot[pair_selected[0]]
                S2 = cotacoes_pivot[pair_selected[1]]
                ratios = S1 / S2
                zscore_series = (ratios - ratios.mean()) / ratios.std()

                st.markdown(""---"")
                st.markdown(f""<h4 style='text-align: center;'>{pair_selected[0]} - {pair_selected[1]}</h4>"", unsafe_allow_html=True)

                col1, col2 = st.columns(2)

                with col1:
                    st.subheader(""Z-Score do Par"")
                    # Cria a figura e o eixo
                    fig, ax = plt.subplots(figsize=(10, 5))
                    ax.plot(zscore_series, label='Z-Score')
                    ticks = ax.get_xticks()  
                    ax.set_xticks(ticks[::5])
                    ax.axhline(0, color='black', linestyle='--')
                    ax.axhline(2, color='red', linestyle='--')
                    ax.axhline(-2, color='green', linestyle='--')
                    ax.axhline(3, color='orange', linestyle='--', label='+3 Desvio (Stop)')
                    ax.axhline(-3, color='orange', linestyle='--', label='-3 Desvio (Stop)')
                    ax.legend(loc='best')
                    ax.set_xlabel('Data')
                    ax.set_ylabel('Z-Score')

                    # Rotaciona e diminui o tamanho das labels do eixo X
                    plt.xticks(rotation=45, fontsize=6)

                    ax.grid(True)

                    # Ajusta automaticamente o layout para evitar sobreposições
                    fig.tight_layout()

                    # Exibe no Streamlit
                    st.pyplot(fig)

                with col2:
                    st.subheader(""Cotação Normalizada"")
                    fig, ax = plt.subplots(figsize=(10, 5))

                    # Plota os dois ativos normalizados
                    ax.plot(S1 / S1.iloc[0], label=f""{pair_selected[0]}"")
                    ax.plot(S2 / S2.iloc[0], label=f""{pair_selected[1]}"")

                    # Ajusta legendas e eixos
                    ax.legend(loc='best')
                    ax.set_xlabel('Data')
                    ax.set_ylabel('Cotação Normalizada')
                    ax.grid(True)

                    # Rotaciona e diminui a fonte das datas
                    plt.xticks(rotation=45, fontsize=6)

                    # Reduz a quantidade de rótulos no eixo X
                    ticks = ax.get_xticks()      # Pega os ticks atuais
                    ax.set_xticks(ticks[::5])    # Exibe somente 1 a cada 5

                    fig.tight_layout()
                    st.pyplot(fig)

                col3, col4 = st.columns(2)

                with col3:
                    st.subheader(f""Beta Móvel para {pair_selected[0]} e {pair_selected[1]}"")
                    plotar_beta_movel(S1, S2, window=40)

                with col4:
                    st.subheader(f""Dispersão entre {pair_selected[0]} e {pair_selected[1]}"")
                    plotar_grafico_dispersao(S1, S2)


               # === Tabs adicionais antes do Expander ===
               # Determina os ativos antes de usar nas abas
                current_zscore = zscores[pairs.index(pair_selected)]
                if current_zscore > 0:
                    stock_to_sell = pair_selected[0]
                    stock_to_buy = pair_selected[1]
                else:
                    stock_to_sell = pair_selected[1]
                    stock_to_buy = pair_selected[0]


      
                tab1, tab2 = st.tabs([""📐 Calcular Proporção"", ""📎 Correlação inversa""])

                with tab1:
                    with st.expander(""📐 Cálculo da Proporção entre os Ativos"", expanded=False):

                        # Dados atualizados com base nos papéis selecionados
                        preco_venda = cotacoes_pivot[stock_to_sell].iloc[-1]
                        preco_compra = cotacoes_pivot[stock_to_buy].iloc[-1]
                        ativo_venda = stock_to_sell
                        ativo_compra = stock_to_buy

                        # Colunas superiores com preços e entrada de capital
                        col_precos1, col_precos2 = st.columns(2)

                        with col_precos1:
                            st.markdown(f""### 🔻 Vender (Short): `{ativo_venda}`"")
                            st.write(f""Preço atual de **{ativo_venda}**: R$ {preco_venda:.2f}"")
                            capital_maximo = st.number_input(
                                ""Capital Total para Venda (R$)"",
                                min_value=100.0,
                                value=25000.0,
                                step=100.0,
                                help=""Limite de capital disponível para definir o máximo de lotes de venda""
                            )

                        with col_precos2:
                            st.markdown(f""### 🔺 Comprar (Long): `{ativo_compra}`"")
                            st.write(f""Preço atual de **{ativo_compra}**: R$ {preco_compra:.2f}"")

                        st.markdown(""---"")
                        st.subheader(""📊 Melhor Proporção com Base no Limite de Venda"")

                        melhor_resultado = None
                        max_lotes_venda = int(capital_maximo // (100 * preco_venda))

                        for lotes_venda in range(1, max_lotes_venda + 1):
                            total_venda = lotes_venda * 100 * preco_venda

                            for lotes_compra in range(1, 100):
                                total_compra = lotes_compra * 100 * preco_compra
                                residuo = abs(total_venda - total_compra)

                                if melhor_resultado is None or residuo < melhor_resultado[""residuo""]:
                                    melhor_resultado = {
                                        ""lotes_venda"": lotes_venda,
                                        ""lotes_compra"": lotes_compra,
                                        ""total_venda"": total_venda,
                                        ""total_compra"": total_compra,
                                        ""residuo"": residuo,
                                        ""fluxo_liquido"": total_venda - total_compra
                                    }

                        if melhor_resultado:
                            col_result1, col_result2 = st.columns(2)

                            with col_result1:
                                st.markdown(f""### 🔻 Vender (Short): `{ativo_venda}`"")
                                st.write(f""- Lotes de 100: **{melhor_resultado['lotes_venda']}**"")
                                st.write(f""- Quantidade: **{melhor_resultado['lotes_venda'] * 100} ações**"")
                                st.write(f""- Total Venda: R$ {melhor_resultado['total_venda']:.2f}"")

                            with col_result2:
                                st.markdown(f""### 🔺 Comprar (Long): `{ativo_compra}`"")
                                st.write(f""- Lotes de 100: **{melhor_resultado['lotes_compra']}**"")
                                st.write(f""- Quantidade: **{melhor_resultado['lotes_compra'] * 100} ações**"")
                                st.write(f""- Total Compra: R$ {melhor_resultado['total_compra']:.2f}"")

                            st.markdown(""---"")
                            fluxo = melhor_resultado['fluxo_liquido']

                            if fluxo >= 0:
                                st.success(f""💰 Fluxo Inicial da Operação: R$ {fluxo:.2f}"")
                            else:
                                st.error(f""📉 Fluxo Inicial da Operação: R$ {fluxo:.2f}"")

                            st.markdown(f""📎 Resíduo Absoluto entre os valores: R$ {melhor_resultado['residuo']:.2f}"")
                        else:
                            st.warning(""❗ Nenhuma combinação de lotes encontrada."")





                with tab2:
                    st.subheader(""📎 Correlação Inversa (Z-Score Invertido)"")
                    st.info(""📌 Este gráfico mostra o comportamento do par **invertido**, onde agora consideramos o ativo que estava sendo comprado como vendido e vice-versa."")

                    # Inversão dos ativos
                    S_inv_1 = S2  # Antes comprado
                    S_inv_2 = S1  # Antes vendido

                    stock_to_sell = pair_selected[1]  # Agora vendemos o que antes comprávamos
                    stock_to_buy = pair_selected[0]   # Compramos o que antes vendíamos

                    ratio_inv = S_inv_1 / S_inv_2
                    zscore_inv = (ratio_inv - ratio_inv.mean()) / ratio_inv.std()

                    fig, ax = plt.subplots(figsize=(10, 5))
                    ax.plot(zscore_inv, label=""Z-Score Inverso"")
                    ax.axhline(0, color='black', linestyle='--')
                    ax.axhline(2, color='red', linestyle='--', label='+2')
                    ax.axhline(-2, color='green', linestyle='--', label='-2')
                    ax.axhline(3, color='orange', linestyle='--', label='+3 (Stop)')
                    ax.axhline(-3, color='orange', linestyle='--', label='-3 (Stop)')
                    ax.set_title(f""Inversão: {stock_to_sell} / {stock_to_buy}"")
                    ax.set_ylabel(""Z-Score Inverso"")
                    ax.legend()
                    ax.grid(True)
                    st.pyplot(fig)

                    # Simulação com ativos invertidos
                    st.markdown(""---"")
                    st.info(""📌 Este gráfico mostra o comportamento do par **invertido**, onde agora consideramos o ativo que estava sendo comprado como vendido e vice-versa."")
                    st.markdown(""### Configurar Operação"")

                    current_zscore_inv = zscore_inv.iloc[-1]

                    if current_zscore_inv > 0:
                        st.markdown(f""**Legenda de Operação:** Com o Z-Score positivo ({current_zscore_inv:.2f}), recomenda-se **VENDER {stock_to_sell}** e **COMPRAR {stock_to_buy}**."")
                        sell_price = S_inv_1.iloc[-1]
                        buy_price = S_inv_2.iloc[-1]
                    else:
                        st.markdown(f""**Legenda de Operação:** Com o Z-Score negativo ({current_zscore_inv:.2f}), recomenda-se **VENDER {stock_to_buy}** e **COMPRAR {stock_to_sell}**."")
                        # inverte os papéis
                        stock_to_sell, stock_to_buy = stock_to_buy, stock_to_sell
                        sell_price = S_inv_2.iloc[-1]
                        buy_price = S_inv_1.iloc[-1]

                    # Aqui você pode continuar com os controles de simulação iguais ao da aba 1





                with st.expander(""Configurar Operação"", expanded=True):
                    current_zscore = zscores[pairs.index(pair_selected)]

                    if current_zscore > 0:
                        st.markdown(
                            f""**Legenda de Operação:** Com o Z-Score positivo ({current_zscore:.2f}), recomenda-se **VENDER {pair_selected[0]}** (ativo sobrevalorizado) e **COMPRAR {pair_selected[1]}** (ativo subvalorizado).""
                        )
                        stock_to_sell = pair_selected[0]
                        stock_to_buy = pair_selected[1]
                        sell_price = S1.iloc[-1]
                        buy_price = S2.iloc[-1]
                    else:
                        st.markdown(
                            f""**Legenda de Operação:** Com o Z-Score negativo ({current_zscore:.2f}), recomenda-se **VENDER {pair_selected[1]}** (ativo sobrevalorizado) e **COMPRAR {pair_selected[0]}** (ativo subvalorizado).""
                        )
                        stock_to_sell = pair_selected[1]
                        stock_to_buy = pair_selected[0]
                        sell_price = S2.iloc[-1]
                        buy_price = S1.iloc[-1]
                  
                    col1, col2 = st.columns(2)

                    # =========================
                    # Coluna 1 - Ação Vendida
                    # =========================
                    with col1:
                        st.subheader(f""Vender Ação: {stock_to_sell}"")
                        venda_quantidade = st.number_input(""Quantidade para Vender"", min_value=100, step=100, value=100, key=""venda_quantidade"")
                        venda_preco_atual = sell_price
                        venda_total = venda_quantidade * venda_preco_atual

                        st.write(f""Preço Atual: R$ {venda_preco_atual:.2f}"")
                        st.write(f""Total Venda: R$ {venda_total:.2f}"")
                        
                        slider_venda = st.slider(""Movimento (%)"", min_value=0, max_value=25, value=5, step=1, key=""slider_venda"")
                        st.write(f""Simulação de {slider_venda}%"")

                        novo_preco_caindo = venda_preco_atual * (1 - slider_venda / 100)
                        lucro_short = (venda_preco_atual - novo_preco_caindo) * venda_quantidade

                        novo_preco_subindo = venda_preco_atual * (1 + slider_venda / 100)
                        preju_short = (venda_preco_atual - novo_preco_subindo) * venda_quantidade

                        st.metric(f""Queda de {slider_venda}% (Lucro Short)"", f""R$ {novo_preco_caindo:.2f}"", delta=round(lucro_short, 2))
                        st.metric(f""Alta de {slider_venda}% (Prejuízo Short)"", f""R$ {novo_preco_subindo:.2f}"", delta=round(preju_short, 2))

                    # =========================
                    # Coluna 2 - Ação Comprada
                    # =========================
                    with col2:
                        st.subheader(f""Comprar Ação: {stock_to_buy}"")
                        compra_quantidade = st.number_input(""Quantidade para Comprar"", min_value=100, step=100, value=100, key=""compra_quantidade"")
                        compra_preco_atual = buy_price
                        compra_total = compra_quantidade * compra_preco_atual

                        st.write(f""Preço Atual: R$ {compra_preco_atual:.2f}"")
                        st.write(f""Total Compra: R$ {compra_total:.2f}"")

                        slider_compra = st.slider(""Movimento (%)"", min_value=0, max_value=25, value=5, step=1, key=""slider_compra"")
                        st.write(f""Simulação de {slider_compra}%"")

                        novo_preco_subindo_long = compra_preco_atual * (1 + slider_compra / 100)
                        lucro_long = (novo_preco_subindo_long - compra_preco_atual) * compra_quantidade

                        novo_preco_caindo_long = compra_preco_atual * (1 - slider_compra / 100)
                        preju_long = (novo_preco_caindo_long - compra_preco_atual) * compra_quantidade

                        st.metric(f""Alta de {slider_compra}% (Lucro Long)"", f""R$ {novo_preco_subindo_long:.2f}"", delta=round(lucro_long, 2))
                        st.metric(f""Queda de {slider_compra}% (Prejuízo Long)"", f""R$ {novo_preco_caindo_long:.2f}"", delta=round(preju_long, 2))
                    resultado_total = mostrar_fluxo_liquido(venda_total, compra_total)
                    st.markdown(""---"")
                    preju_total = preju_short + preju_long
                    st.metric(""STOP (Soma dos Prejuízos)"", f""R$ {preju_total:.2f}"", delta=round(preju_total, 2))

         

 
                st.markdown(""---"")
                if st.button(""Salvar Operação como Excel""):
                    operacao_data = {
                        ""Ativo Vendido"": [stock_to_sell],
                        ""Ativo Comprado"": [stock_to_buy],
                        ""Preço Venda"": [sell_price],
                        ""Preço Compra"": [buy_price],
                        ""Quantidade Vendida"": [venda_quantidade],
                        ""Quantidade Comprada"": [compra_quantidade],
                        ""Resultado Total"": [resultado_total],
                        ""Data Operação"": [datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")]
                    }

                    df_operacao = pd.DataFrame(operacao_data)
                    output = BytesIO()
                    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                        df_operacao.to_excel(writer, index=False, sheet_name=""Operacao"")
                    output.seek(0)

                    st.download_button(""Baixar Operação em Excel"", data=output, file_name=f""operacao_{pair_selected[0]}_{pair_selected[1]}.xlsx"", mime=""application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"")

                # Exibir métricas no rodapé
                st.markdown(""---"")
                st.markdown(""### 📊 Métricas do Par Selecionado e Recomendações"")

                col1, col2, col3, col4, col5 = st.columns(5)

                # Z-Score
                col1.metric(""Z-Score"", f""{zscores[pairs.index(pair_selected)]:.2f}"")
                col1.caption(""📌 Mede o desvio da média. Valores acima de ±2 indicam oportunidade de reversão."")

                # P-Value
                col2.metric(""P-Value"", f""{pvalues[pairs.index(pair_selected)]:.4f}"")
                col2.caption(""📌 Probabilidade de cointegração ser aleatória. Valores abaixo de 0.05 são desejáveis."")

                # Hurst Exponent
                col3.metric(""Hurst"", f""{hursts[pairs.index(pair_selected)]:.4f}"")
                col3.caption(""📌 Mede a tendência de reversão ou persistência. Valores próximos de 0.5 são ideais."")

                # Beta Rotation
                col4.metric(""Beta"", f""{beta_rotations[pairs.index(pair_selected)]:.4f}"")
                col4.caption(""📌 Sensibilidade do ativo em relação ao outro. Quanto menor, melhor para pares estáveis."")

                # Half-Life
                col5.metric(""Half-Life"", f""{half_lives[pairs.index(pair_selected)]:.2f}"")
                col5.caption(""📌 Tempo esperado para metade da reversão ao valor médio. Quanto menor, melhor."")




if selected == ""Operações"":
    st.title(""Operações"")

    # Upload do arquivo Excel de operações
    uploaded_file = st.file_uploader(""Carregar Arquivo de Operação (Excel)"", type=[""xlsx""])

    if uploaded_file is not None:
        import pandas as pd
        import matplotlib.pyplot as plt
        from sklearn.linear_model import LinearRegression
        import numpy as np

        # Função para plotar Beta Móvel
        def plotar_beta_movel(S1, S2, window=40):
            try:
                returns_S1 = np.log(S1 / S1.shift(1)).dropna()
                returns_S2 = np.log(S2 / S2.shift(1)).dropna()

                betas = []
                index_values = returns_S1.index[window - 1:]  # Ajustar para a janela

                for i in range(window, len(returns_S1) + 1):
                    reg = LinearRegression().fit(
                        returns_S2[i - window:i].values.reshape(-1, 1),
                        returns_S1[i - window:i].values
                    )
                    betas.append(reg.coef_[0])

                beta_movel = pd.Series(betas, index=index_values)

                # Plotar o gráfico de beta móvel
                plt.figure(figsize=(10, 5))
                plt.plot(beta_movel, label=f'Beta Móvel ({window} períodos)')
                plt.axhline(0, color='black', linestyle='--')
                plt.title(f'Beta Móvel ({window} períodos)')
                plt.xlabel('Data')
                plt.ylabel('Beta')
                plt.legend()
                plt.grid(True)
                st.pyplot(plt)
            except Exception as e:
                st.error(f""Erro ao calcular ou plotar o beta móvel: {e}"")

        try:
            # Ler o arquivo Excel
            df_operacoes = pd.read_excel(uploaded_file)

            # 1) Converter a coluna de datas para datetime
            df_operacoes[""Data Operação""] = pd.to_datetime(df_operacoes[""Data Operação""], errors=""coerce"")

            # Avisar caso tenha datas inválidas
            if df_operacoes[""Data Operação""].isnull().any():
                st.warning(""Há datas inválidas em 'Data Operação'. Verifique o arquivo Excel."")

            # Verificar se ""global_cotacoes"" está carregado
            if ""global_cotacoes"" not in st.session_state or st.session_state[""global_cotacoes""].empty:
                st.error(""Nenhuma cotação carregada no sistema. Por favor, carregue as cotações na aba 'Cotações'."")
                st.stop()

            # 2) Converter o índice do DataFrame de cotações para DateTimeIndex
            cotacoes_df = st.session_state[""global_cotacoes""]
            if not pd.api.types.is_datetime64_any_dtype(cotacoes_df.index):
                # Tenta converter o índice atual para datetime
                cotacoes_df.index = pd.to_datetime(cotacoes_df.index, errors='coerce')
                # Remover possíveis linhas com índice inválido (NaT)
                cotacoes_df = cotacoes_df[~cotacoes_df.index.isnull()]
                # Sobrescreve no session_state, se desejar reutilizar o DF corrigido
                st.session_state[""global_cotacoes""] = cotacoes_df

            # Preparar a tabela final
            tabela_final = []

            for index, row in df_operacoes.iterrows():
                # Verifica se a data é válida (não é NaT)
                if pd.isnull(row[""Data Operação""]):
                    st.error(f""Linha {index}: Data da Operação inválida. Operação ignorada."")
                    continue

                ativo_venda = row[""Ativo Vendido""]
                ativo_compra = row[""Ativo Comprado""]
                data_operacao = row[""Data Operação""]
                quantidade_venda = row[""Quantidade Vendida""]
                quantidade_compra = row[""Quantidade Comprada""]
                valor_inicial_venda = row[""Preço Venda""]
                valor_inicial_compra = row[""Preço Compra""]
                numero_periodos = 120  # Número de períodos padrão

                # Verificar se os ativos estão disponíveis nas cotações
                if ativo_venda not in cotacoes_df.columns or ativo_compra not in cotacoes_df.columns:
                    st.error(f""Os ativos {ativo_venda} ou {ativo_compra} não estão disponíveis nas cotações globais."")
                    continue

                # Obter as séries históricas dos ativos, limitadas a 120 períodos
                series_venda = cotacoes_df[ativo_venda].tail(numero_periodos)
                series_compra = cotacoes_df[ativo_compra].tail(numero_periodos)

                # Calcular o Z-Score
                ratios = series_venda / series_compra
                zscore_series = (ratios - ratios.mean()) / ratios.std()

                # Calcular valor total e saldo
                valor_total_venda = quantidade_venda * series_venda.iloc[-1]
                valor_total_compra = quantidade_compra * series_compra.iloc[-1]
                saldo = valor_total_venda - valor_total_compra

                # Adicionar dados à tabela final
                tabela_final.append({
                    ""Data Operação"": data_operacao,
                    ""Ativo"": ativo_venda,
                    ""Quantidade"": quantidade_venda,
                    ""Tipo"": ""Venda"",
                    ""Valor Inicial"": f""R$ {valor_inicial_venda:.2f}"",
                    ""Valor Total"": f""R$ {valor_total_venda:.2f}"",
                    ""Saldo"": f""R$ {saldo:.2f}""
                })

                tabela_final.append({
                    ""Data Operação"": data_operacao,
                    ""Ativo"": ativo_compra,
                    ""Quantidade"": quantidade_compra,
                    ""Tipo"": ""Compra"",
                    ""Valor Inicial"": f""R$ {valor_inicial_compra:.2f}"",
                    ""Valor Total"": f""R$ {valor_total_compra:.2f}"",
                    ""Saldo"": f""R$ {saldo:.2f}""
                })

                # Exibir informações e gráficos
                st.markdown(""---"")  # Separador
                st.write(f""**Ativo Vendido:** {ativo_venda}"")
                st.write(f""**Ativo Comprado:** {ativo_compra}"")
                st.write(f""**Data da Operação:** {data_operacao}"")
                st.write(f""**Média das Razões:** {ratios.mean():.4f}"")
                st.write(f""**Desvio Padrão das Razões:** {ratios.std():.4f}"")

                # ======= Gráfico do Z-Score =======
                plt.figure(figsize=(12, 3))
                plt.plot(zscore_series, label=""Z-Score"")
                plt.axhline(0, color='black', linestyle='--')
                plt.axhline(2, color='red', linestyle='--', label=""+2 Desvio"")
                plt.axhline(-2, color='green', linestyle='--', label=""-2 Desvio"")
                plt.axhline(3, color='orange', linestyle='--', label=""+3 Desvio (Stop)"")
                plt.axhline(-3, color='orange', linestyle='--', label=""-3 Desvio (Stop)"")
                plt.legend(loc='best')
                plt.xlabel(""Períodos"")
                plt.ylabel(""Z-Score"")
                plt.title(f""Z-Score: {ativo_venda} vs {ativo_compra}"")
                plt.grid(True)
                st.pyplot(plt)

                # ======= Gráfico de Paridade Normalizada =======
                plt.figure(figsize=(12, 3))
                plt.plot(series_venda / series_venda.iloc[0], label=f""{ativo_venda}"")
                plt.plot(series_compra / series_compra.iloc[0], label=f""{ativo_compra}"")
                plt.legend(loc='best')
                plt.xlabel(""Períodos"")
                plt.ylabel(""Cotação Normalizada"")
                plt.title(f""Paridade Normalizada: {ativo_venda} vs {ativo_compra}"")
                plt.grid(True)
                st.pyplot(plt)

                # ======= Gráficos de Beta Móvel e Dispersão =======
                col3, col4 = st.columns(2)

                with col3:
                    plotar_beta_movel(series_venda, series_compra, window=40)

                with col4:
                    plt.figure(figsize=(10, 5))
                    plt.scatter(series_venda, series_compra, alpha=0.7)
                    plt.xlabel(f""{ativo_venda}"")
                    plt.ylabel(f""{ativo_compra}"")
                    plt.title(f""Dispersão: {ativo_venda} vs {ativo_compra}"")
                    plt.grid(True)
                    st.pyplot(plt)

                # ======= Novos Gráficos de Desempenho (em duas colunas) =======
                # ======= Novos Gráficos de Desempenho (em duas colunas) =======
                data_inicial = data_operacao - pd.Timedelta(days=5)

                # Pegar série completa sem limitar a 120 períodos
                serie_completa_vendida = cotacoes_df[ativo_venda]
                serie_completa_comprada = cotacoes_df[ativo_compra]

                # Filtrar a partir de data_inicial até a última cotação
                desempenho_vendido = serie_completa_vendida.loc[data_inicial:]
                desempenho_comprado = serie_completa_comprada.loc[data_inicial:]

                col5, col6 = st.columns(2)

                # ======= Novos Gráficos de Desempenho (em duas colunas) =======
                data_inicial = data_operacao - pd.Timedelta(days=5)

                # Pegar série completa sem limitar a 120 períodos
                serie_completa_vendida = cotacoes_df[ativo_venda]
                serie_completa_comprada = cotacoes_df[ativo_compra]

                # Filtrar a partir de data_inicial até a última cotação
                desempenho_vendido = serie_completa_vendida.loc[data_inicial:]
                desempenho_comprado = serie_completa_comprada.loc[data_inicial:]

                col5, col6 = st.columns(2)

                # Gráfico 1 (Ativo Vendido)
                with col5:
                    plt.figure(figsize=(10, 4))
                    plt.plot(desempenho_vendido.index, desempenho_vendido.values, color='blue')
                    plt.title(
                        f""Desempenho do Ativo Vendido ({ativo_venda})\n""
                        f""(5 dias antes do início da operação até a última cotação) – Par: {ativo_compra}""
                    )
                    plt.xlabel(""Data"")
                    plt.ylabel(""Preço"")
                    plt.grid(False)  # remove a grade de fundo
                    st.pyplot(plt)

                # Gráfico 2 (Ativo Comprado)
                with col6:
                    plt.figure(figsize=(10, 4))
                    plt.plot(desempenho_comprado.index, desempenho_comprado.values, color='red')
                    plt.title(
                        f""Desempenho do Ativo Comprado ({ativo_compra})\n""
                        f""(5 dias antes do início da operação até a última cotação) – Par: {ativo_venda}""
                    )
                    plt.xlabel(""Data"")
                    plt.ylabel(""Preço"")
                    plt.grid(False)  # remove a grade de fundo
                    st.pyplot(plt)



            # Exibir tabela consolidada
            st.markdown(""---"")
            st.markdown(""### Tabela Consolidada de Operações"")
            tabela_df = pd.DataFrame(tabela_final)
            st.dataframe(tabela_df)

            # Separador com Posição Atual
            st.markdown(""---"")
            st.markdown(""### Posição Atual"")

            # DataFrame consolidado para posição atual
            posicao_atual = []

            for index, row in df_operacoes.iterrows():
                ativo_venda = row[""Ativo Vendido""]
                ativo_compra = row[""Ativo Comprado""]
                quantidade_venda = row[""Quantidade Vendida""]
                quantidade_compra = row[""Quantidade Comprada""]
                preco_inicial_venda = row[""Preço Venda""]
                preco_inicial_compra = row[""Preço Compra""]

                # Obter preços atuais dos ativos
                preco_atual_venda = cotacoes_df[ativo_venda].iloc[-1] if ativo_venda in cotacoes_df.columns else None
                preco_atual_compra = cotacoes_df[ativo_compra].iloc[-1] if ativo_compra in cotacoes_df.columns else None

                if preco_atual_venda is not None:
                    lucro_venda = (preco_inicial_venda - preco_atual_venda) * quantidade_venda
                    posicao_atual.append({
                        ""Ativo"": ativo_venda,
                        ""Tipo"": ""Venda"",
                        ""Quantidade"": quantidade_venda,
                        ""Preço Inicial"": f""R$ {preco_inicial_venda:.2f}"",
                        ""Preço Atual"": f""R$ {preco_atual_venda:.2f}"",
                        ""Lucro/Prejuízo"": f""R$ {lucro_venda:.2f}""
                    })

                if preco_atual_compra is not None:
                    lucro_compra = (preco_atual_compra - preco_inicial_compra) * quantidade_compra
                    posicao_atual.append({
                        ""Ativo"": ativo_compra,
                        ""Tipo"": ""Compra"",
                        ""Quantidade"": quantidade_compra,
                        ""Preço Inicial"": f""R$ {preco_inicial_compra:.2f}"",
                        ""Preço Atual"": f""R$ {preco_atual_compra:.2f}"",
                        ""Lucro/Prejuízo"": f""R$ {lucro_compra:.2f}""
                    })

            # Exibir a tabela de posição atual
            if posicao_atual:
                posicao_atual_df = pd.DataFrame(posicao_atual)
                st.dataframe(posicao_atual_df)
            else:
                st.warning(""Nenhum dado disponível para a posição atual."")

            # Calcular o saldo final
            saldo_final = sum([
                (float(row[""Lucro/Prejuízo""].replace('R$', '').replace(',', '').strip()))
                for row in posicao_atual
            ])

            # Exibir o saldo final
            st.markdown(""### Saldo Final Consolidado"")
            st.markdown(f""<h3 style='text-align: center; color: blue;'>R$ {saldo_final:.2f}</h3>"", unsafe_allow_html=True)

        except Exception as e:
            st.error(f""Erro ao processar o arquivo: {e}"")","False","False","False","False","True","False","False","False"
"AlanRigden/Final-college-project","produce.py","# importing all packages used with the program

import requests
import json

import matplotlib.pyplot as plt
import numpy as np


def weather_api(api_url, location, graph_it):
    # changing the settings as the college network produces errors if not
    requests.adapters.HTTPAdapter(max_retries=5)
    '''verify=False does produce a security issue
     but due to the college security the program will not work without that change'''
    response_api = requests.get(
        api_url,
        verify=False)
    # strips the json to just cleaner text so it can easily be read
    data = response_api.text
    parse_json = json.loads(data)
    # breaks down the json further into just the needed bit
    hourly = parse_json['hourly']
    time = hourly['time']
    temp = hourly['temperature_2m']
    rain = hourly['rain']

    # removes un-needed text from the time list
    time = ([s[11:] for s in time])
    # finds the average temp for the day
    avg_temp = round(sum(temp) / len(temp), 2)

    if graph_it:
        # creation of graph
        title = f'average temperature of {location}'
        fig, ax = plt.subplots()
        ax.bar(time, temp)
        ax.set_ylabel('temperature (°c)')
        ax.set_title(title)
        plt.xticks(rotation=50)
        # stores it in the static folder so it can be accessed without having to pass a large piece of data
        plt.savefig('static/images/temp.png')

        # creation of graph
        title = f'predicted rainfall of {location}'
        fig, ax = plt.subplots()
        ax.bar(time, rain)
        ax.set_ylabel('rain (mm)')
        ax.set_title(title)
        plt.xticks(rotation=50)
        # stores it in the static folder so it can be accessed without having to pass a large piece of data
        plt.savefig('static/images/rain.png')

    return avg_temp


def air_quality_api(api_url, location):
    # changing the settings as the college network produces errors if not
    requests.adapters.HTTPAdapter(max_retries=5)
    '''verify=False does produce a security issue
     but due to the college security the program will not work without that change'''
    response_api = requests.get(
        api_url,
        verify=False)
    # print(response_API.status_code)
    data = response_api.text
    parse_json = json.loads(data)
    hourly = parse_json['hourly']

    time = hourly['time']
    time = ([s[11:] for s in time])

    # breaks down the json further into just the needed bit
    pm10 = hourly['pm10']
    pm2_5 = hourly['pm2_5']
    alder_pollen = hourly['alder_pollen']
    birch_pollen = hourly['birch_pollen']
    grass_pollen = hourly['grass_pollen']
    olive_pollen = hourly['olive_pollen']

    # removes none values which would create errors in list manipulation later
    alder_pollen = [x for x in alder_pollen if x is not None]
    birch_pollen = [x for x in birch_pollen if x is not None]
    grass_pollen = [x for x in grass_pollen if x is not None]
    olive_pollen = [x for x in olive_pollen if x is not None]

    # if errors are still occurring these try statements will fix it
    try:
        alder_average = round(sum(alder_pollen) / len(alder_pollen), 2)
    except ZeroDivisionError:
        alder_average = 0
    try:
        birch_average = round(sum(birch_pollen) / len(birch_pollen), 2)
    except ZeroDivisionError:
        birch_average = 0
    try:
        grass_average = round(sum(grass_pollen) / len(grass_pollen), 2)
    except ZeroDivisionError:
        grass_average = 0
    try:
        olive_average = round(sum(olive_pollen) / len(olive_pollen), 2)
    except ZeroDivisionError:
        olive_average = 0

    # replaces none values with a 0 and then converts them to float
    pm10 = [str(i or 0) for i in pm10]
    pm10 = [float(i) for i in pm10]

    pm2_5 = [str(i or 0) for i in pm2_5]
    pm2_5 = [float(i) for i in pm2_5]
    # puts them into two lists so they can be passes more efficiently through the program
    pollen_names = ['alder_pollen', 'birch_pollen', 'grass_pollen', 'olive_pollen']
    pollen_averages = [alder_average, birch_average, grass_average, olive_average]
    # creation of graph
    x_axis = np.arange(len(time))
    plt.figure(figsize=(17, 6))
    plt.bar(x_axis - 0.2, pm10, 0.4, label='pm10')
    plt.bar(x_axis + 0.2, pm2_5, 0.4, label='pm2.5')
    plt.xticks(rotation=90)
    plt.xticks(x_axis, time)
    plt.xlabel(""times"")
    plt.ylabel(""amount of particles in the air (μg/m³)"")
    plt.title(f""amount of particles in the air of different type in {location}"")
    plt.tick_params(axis='x', which='major', labelsize=8)
    plt.legend()
    # stores it in the static folder so it can be accessed without having to pass a large piece of data
    plt.savefig('static/images/air_quality.png')

    return pollen_names, pollen_averages


def convert_lat(location):
    # changing the settings as the college network produces errors if not
    requests.adapters.HTTPAdapter(max_retries=5)
    '''verify=False does produce a security issue
     but due to the college security the program will not work without that change'''
    response_api = requests.get(
        f'https://geocoding-api.open-meteo.com/v1/search?name={location}',
        verify=False)
    # strips the json to just cleaner text so it can easily be read
    data = response_api.text
    parse_json = json.loads(data)
    # breaks down the json further into just the needed bit
    results = parse_json['results']
    loc = results[0]
    lat = loc['latitude']
    long = loc['longitude']
    return lat, long


def password_checker(password):
    SpecialSym = ['$', '@', '#', '%']
    val = True

    if len(password) < 6:
        val = False

    if not any(char.isdigit() for char in password):
        val = False

    if not any(char.isupper() for char in password):
        val = False

    if not any(char.islower() for char in password):
        val = False

    if not any(char in SpecialSym for char in password):
        val = False

    return val
","False","False","False","False","False","True","False","False"
"YYYYXL1004/IMDB","b.py","# -*- coding: utf-8 -*-
import os
import re
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import TextVectorization, Embedding, Bidirectional, LSTM, Dense, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split

# 超参数配置[1,4](@ref)
MAX_LEN = 500
VOCAB_SIZE = 20000
EMBED_DIM = 128
BATCH_SIZE = 64
EPOCHS = 10


def load_imdb_data(data_dir):
    """"""加载IMDB数据集[5](@ref)""""""

    def load_texts_labels(folder):
        texts, labels = [], []
        for label in ['pos', 'neg']:
            path = os.path.join(folder, label)
            for file in os.listdir(path):
                with open(os.path.join(path, file), 'r', encoding='utf-8') as f:
                    texts.append(f.read())
                    labels.append(1 if label == 'pos' else 0)
        return texts, labels

    train_dir = os.path.join(data_dir, 'train')
    test_dir = os.path.join(data_dir, 'test')
    return (load_texts_labels(train_dir),
            load_texts_labels(test_dir))


def preprocess_text(text):
    """"""文本预处理[1,5](@ref)""""""
    text = re.sub(r'<[^>]+>', ' ', text)  # 去除HTML标签
    text = re.sub(r""([.!?,'/()])"", r' \1 ', text)  # 保留标点
    text = re.sub(r'[^a-zA-Z.!?,\']', ' ', text)  # 去除非字母字符
    text = re.sub(r'\s+', ' ', text).strip().lower()  # 合并空格并转小写
    return text


# 数据加载与预处理
(train_texts, train_labels), (test_texts, test_labels) = load_imdb_data(""aclImdb"")
train_texts = [preprocess_text(t) for t in train_texts]
test_texts = [preprocess_text(t) for t in test_texts]

# 数据集分割[5](@ref)
train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_texts, train_labels,
    test_size=0.2,
    stratify=train_labels
)

# 文本向量化层[4](@ref)
vectorizer = TextVectorization(
    max_tokens=VOCAB_SIZE,
    output_sequence_length=MAX_LEN,
    output_mode='int'
)
vectorizer.adapt(train_texts)

# 构建模型[1,4](@ref)
model = Sequential([
    tf.keras.Input(shape=(1,), dtype=tf.string),
    vectorizer,
    Embedding(VOCAB_SIZE + 1, EMBED_DIM, mask_zero=True),
    Bidirectional(LSTM(64, return_sequences=True)),
    Dropout(0.5),
    Bidirectional(LSTM(32)),
    Dense(64, activation='relu', kernel_regularizer='l2'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

# 模型编译[4](@ref)
model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-4),
    loss='binary_crossentropy',
    metrics=['accuracy',
             tf.keras.metrics.Precision(name='precision'),
             tf.keras.metrics.Recall(name='recall')]
)

# 训练配置[4](@ref)
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

# 模型训练[4](@ref)
history = model.fit(
    tf.convert_to_tensor(train_texts, dtype=tf.string),
    tf.convert_to_tensor(train_labels, dtype=tf.float32),
    validation_data=(
        tf.convert_to_tensor(val_texts, dtype=tf.string),
        tf.convert_to_tensor(val_labels, dtype=tf.float32)
    ),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=[early_stop]
)

# 模型评估[4](@ref)
test_loss, test_acc, test_precision, test_recall = model.evaluate(
    tf.convert_to_tensor(test_texts, dtype=tf.string),
    tf.convert_to_tensor(test_labels, dtype=tf.float32),
    batch_size=BATCH_SIZE
)

print(f""\n测试准确率：{test_acc:.4f}"")
print(f""F1 Score：{2 * (test_precision * test_recall) / (test_precision + test_recall):.4f}"")

# 可视化训练过程
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Accuracy Trend')
plt.legend()
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Loss Trend')
plt.legend()
plt.tight_layout()
plt.show()
plt.savefig('b.train.png')  # 保存训练过程图像","False","True","True","False","False","True","False","False"
"pyhope/myscripts","D.py","#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np
import argparse
from matplotlib import pyplot as plt
from scipy.stats import linregress

# Argument parser for command line options
parser = argparse.ArgumentParser()
parser.add_argument(""--file"", ""-i"", type=str, default='msd_fft.txt', help=""input file"")
parser.add_argument(""--lim1"", ""-l1"", type=float, default=1e5, help=""time threshold 1"")
parser.add_argument(""--lim2"", ""-l2"", type=float, default=5e5, help=""time threshold 2"")
parser.add_argument(""--timestep"", ""-ts"", type=float, default=50, help=""time steps"")
parser.add_argument(""--output"", ""-o"", type=str, default='D_H', help=""output filename"")
parser.add_argument(""--error_output"", ""-eo"", type=str, default='D_err', help=""error output filename"")
parser.add_argument(""--multiply"", ""-m"", type=float, default=1, help=""multiply factor for D"")

args = parser.parse_args()
timestep = args.timestep
factor = args.multiply

# Function for linear fitting
def fit(x, y):
    slope, intercept, r_value, p_value, std_err = linregress(x, y)
    return slope, intercept, r_value**2, std_err

# Function to calculate MSD
def CalcMSD(msd):
    t = np.array(list(range(len(msd)))) * timestep
    index1 = np.argmax(t >= args.lim1)
    index2 = np.argmax(t >= args.lim2)
    
    slope, intercept, r_squared, std_err = fit(t[index1:index2], msd[index1:index2])

    D = slope / 6  # unit: A^2/fs
    D = D * 1e-5   # unit: m^2/s
    D = D * factor
    D_error = std_err / 6 * 1e-5  # unit: * m^2/s
    D_error = D_error * factor

    print(f""D = %.2e ± %.2e m^2/s"" % (D, D_error))
    print(f""R^2 = {r_squared}"")
    return D, D_error

# Plotting and analysis
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111)
msd = np.loadtxt(args.file, skiprows=1)

with open(args.file, ""r"") as file:
    first_line = file.readline()
    numbers = first_line.lstrip(""# "").split()
    number_list = [int(number) for number in numbers]

dataout = open(args.output, ""w"")
errorout = open(args.error_output, ""w"")

if msd.ndim == 1:
    ax.plot(np.array(list(range(len(msd)))) * timestep, msd)
    D, D_error = CalcMSD(msd)
    dataout.write(str(D) + ' ')
    errorout.write(str(D_error) + ' ')
elif msd.ndim == 2:
    Data = dict()
    for index, row in enumerate(msd.T):
        ele_num = number_list[index]
        print('Element:', ele_num)
        ax.plot(np.array(list(range(len(row)))) * timestep, row, label='Element: ' + str(ele_num))
        D, D_error = CalcMSD(row)
        Data[ele_num] = [D, D_error]

    for i in range(1, len(Data) + 1):
        dataout.write('%.3e\n' % Data[i][0])
        errorout.write('%.2e\n' % Data[i][1])
    ax.legend()

dataout.close()
errorout.close()

ax.set_xlabel('Time (fs)')
ax.set_ylabel('MSD (A^2)')
plt.savefig('msd.jpg', dpi=300)
","True","True","True","False","False","True","False","False"
